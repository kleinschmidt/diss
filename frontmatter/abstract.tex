\begin{abstract}

  Perceptual systems have to make sense out of a world that is not only noisy
  and ambiguous, but that also varies from situation to situation. Human speech
  perception is a perceptual domain where this problem has long been
  acknowledged: individual talkers vary substantially in how they produce
  linguistic units using acoustic cues.  Yet, how the speech system solves this
  problem of talker variability remains poorly understood. This thesis presents
  a computational framework---the \emph{ideal adapter}---for understanding this
  problem and how the speech perception system solves it. The basic insight of
  this framework is that variability in speech is not arbitrary but rather
  \emph{structured}: talkers are reasonably consistent in the way they produce
  cues, and individual talkers tend to cluster into groups by gender,
  regional background, etc.  This structure means that listeners can use their
  previous experience with other talkers to guide perception of unfamiliar
  talkers, as well as familiar talkers that they encounter again. This framework
  unifies a large and messy literature on how listeners cope with talker
  variability, leads to quantitative models that provide good fits to human
  behavior in a variety of situations, and makes specific, testable predictions
  that open up new frontiers in understanding speech perception. This framework
  also applies to perception in general, and highlights how speech perception
  can serve as a model organism for understanding how perceptual systems cope
  with a variable but structured world.
  
\end{abstract}