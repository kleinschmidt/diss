

```{r preamble}

```

# Introduction

* Have to cope with talker variability/lack of invariance.
* Can think of this as a statistical inference problem.
* Prediction: listeners benefit from anything that is informative about cue distributions.
* Question: _how_ informative are socio-indexical variables like age, sex, dialect, etc.?
    * ...about underlying cue distributions?
    * ...about phonetic categories?
    * ...about socio-indexical variables themselves?
* Use ideal observer approach to quantify these.
* Tell us something about what sorts of conditional distributions listeners should be tracking (if they're optimal), and lay the groundwork for future empirical work.

## Background

* Know that there are different levels of talker variability.
    * Vowels/fricatives have a lot, much of it conditioned on sex [@Peterson1952; @Hillenbrand1995; @Jongman2000; @McMurray2011a; @Newman2001]. For vowels this can be largely [but not entirely; @Bladon1984; @Johnson2005; @Johnson2006] attributed to differences in vocal tract length, but not so for fricatives.
    * Stop voicing, less talker variation, and little systematic effect of sex [@Allen2003; @Lisker1964; @Chodroff2015]
    * Stylistic variation in both though. Dialect for vowels [@Clopper2005; @LabovDDDD], and dialect/native language background for voicing [French-English bilinguals, @Caramazza1973; @Flege1987; @Pineda2010; @Sumner2011; Scottish English, @Docherty2011].
* We also know that listeners are sensitive to (real or believed) socio-indexical groups:
    * @Niedzielski1999: say that talker is canadian, hear more canadian raising
    * gender stereotypicality and visual cues affect perception of vowels and fricatives [@Strand1999;@Johnson1999;@Strand1996]

## Goals

So the _existence_ of group differences is established, as have listeners' _sensitivity_ to them. So, qualitatively, the predictions of the ideal adapter hold up. But we don't have the _quantiative_ measurements we need to test these predictions in more detail. There are three major goals of this paper:

1. Quantify how _informative_ socio-indexical groups are about phonetic cue distributions.
2. Quantify the _utility_ of socio-indexical groups for accurate speech recognition.
3. Show how listeners can make inferences about socio-indexical variables in the same way as linguistic variables, based on cue distributions.

We address these goals at different levels of socio-indexical grouping, and for two different sets of phonetic cues/contrasts. Comparing multiple contrasts is a first step in refining the predictions of the ideal adapter framework about what distributions listeners should track.

# Methods

## Datasets

### Vowels

* Nationwide speech project (Vowel F1/F2)
    * 48 talkers: 4 each of male/female, from 6 dialect regions
    * F1 and F2 for 5 repetitions of isolated word vowels
    * For normalized formants, used Lobanov normalization (z-score F1 and F2 for each talker).

### Stop voicing

* Buckeye corpus (word initial stop VOT)
    * 24 (out of 40) talkers, balanced male/female, young/old (40 y.o.)
    * VOT automatically extracted for word-initial voiced and voiceless stops in conversational speech
    * about 30 voiced, 50 voiceless per place per talker (range: 5-150).

## Modeling

* Model each category as a Bi- or uni-variate normal distribution (for vowels and stops, respectively)
* Use maximum likelihood estimate (sample mean and covariance) based on all tokens from category at the relevant grouping level.
* Phonetic categories:
    * Vowels: 11 monopthongs of English + ey.
    * Stops: voicing (voiced/voiceless) at labial, coronal, and dorsal places of articulation. analyzed separately by place and then results averaged.
* Grouping levels:
    * Marginal (all tokens)
    * Sex (male/female)
    * Age (young/old, VOT only)
    * Dialect (North/New England/Mid-Atlantic/South/Midland/West, vowels only)
    * Diaelct+Sex (12 levels, vowels only)
    * Talker

### Comparing cue distributions

In order to evalute the _informativity_ of socio-indexical variables with respect to cue distributions themselves, we use Kullback-Leibler (KL) divergence to measure how much the group-specific cue distributions differ from the overall (marginal) cue distributions. This measures the expected cost (in bits of extra message length) of encoding data from each group using a code that's optimized for the marginal distribution.
We do this separately for each linguistic category and group, and then average the results, calculating bootstrapped confidence intervals over groups.

The KL divergence of $Q$ from $P$ is $DL(Q||P) = \int p(x) \log \frac{p(x)}{q(x)} \mathrm{d}x$ (with density functions $q$ and $p$ respectively). In our case, $P=\mathcal{N}_G$ is a multivariate[^multivariate] normal cue distribution for the group, with mean $\mu_G$ and covariance $\Sigma_G$, while $Q=\mathcal{N}_M$ is the marginal multivariate normal cue distribution with mean $\mu_M$ and covariance $\Sigma_M$. With some simplification[^gaus-kl], the KL divergence of the marginal from the group distribution works out to be
$$
DL(\mathcal{N}_M || \mathcal{N}_G) = \frac{1}{2} \left( \mathrm{tr}(\Sigma_M^{-1} \Sigma_G) + (\mu_M - \mu_G) \Sigma_M^{-1} (\mu_M - \mu_G) - d + \log\frac{|\Sigma_M|}{|\Sigma_G|} \right)
$$
where $d$ is the dimensionality of the distribution.

[^multivariate]: The math is the same for the univariate special case, as with VOT.
[^gaus-kl]: See, for instance, [http://stanford.edu/~jduchi/projects/general_notes.pdf](), p. 13.

### Speech recognition

Next, to address the _utility_ of socio-indexical grouping for speech recognition, we calculate, for each level of socio-indexical grouping, the probability of correct recognition of phonetic categories. We do this using an "ideal listener" model [@Clayards2008; @Kleinschmidt2015] that compute the posterior probability of a category given an observed cue value based on the likelihood of that cue being generated by each category's cue distribution.

* For talker $t=i$, classify each observation's $x_i$ phonetic category $v_i$.[^notation]
* If we assume that talker $t$'s group is _known_, this is straightforward: $p(v_i | x_i, g) \propto p(x_i | v_i, g) p(v_i)$
* If the group must also be inferred, there are two steps:
    1. Get the posterior probability of each group based on _all_ observations from the talker $p(g | x)$, as below.
    2. Get marginal probability of category $v_i$ by taking average of each group-specific probability, weighted by posterior of group, $p(v_i | x_i) = \sum_j p(v_i | x_i, g=j) p(g=j | x)$.


### Indexical group recognition

Finally, to address much cue distributions tell listeners about socio-indexical variables themselves, we classify each talker's socio-indexical group (at each level). We do this using a similar ideal observer model. That is, we compute the posterior probability of each socio-indexical group, given all of the talker's observed cue values.

* Compute posterior probability of group given talker's observations $p(g | x)$:
    * Because the category $v_i$ of each observation $x_i$ is _unknown_, each observation $x_i$'s likelihood under group $g$ is the average of the likelihood under group $g$'s model for each category $v_i=j$:
    * $p(x_i | g) = p(x_i | g) = \sum_j p(x_i | v_i=j, g)p(v_i=j)$
    * These likelihoods combine in the usual way to give the posterior:
    * $p(g | x) \propto p(x | g)p(g) = \prod_i p(x_i | g) p(g)$

[^notation]: $x_i$ refers to a single observed cue value (possibly multidimensional, in the case of vowel formants), and $x$ (without subscript) refers to a _vector_ of multiple observations (from a single talker, unless otherwise specified). $v_i$ refers to observation $i$'s category, and $g$ to a talker's group. $\sum_j$ refer to a sum over all possible values of $j$.

### Controls

* For classification, if test data is included in the training set, this artificially inflates accuracy at test.  Cross-validation controls for this by splitting data into training and test sets.  For group-level models (sex, age, dialect, and dialect+sex), we use leave-one-talker-out cross-validation: train each group's models with test talker's observations held out. For the talker-specific models, we use 6-fold cross-validation (or leave-one-out when there were fewer than 6 tokens in a category for a talker), where each phonetic category is split into 6 approximately equal subsets. Then, one subset of each category is selected for test, the models are trained on the remaining five, and the test data is classified as above.
* For the vowel data, the different levels of grouping have very different group sizes. The broadest (sex) has 24 talkers per group (23 after holdout), while the most specific (dialect+sex) has only 4 (3 after holdout).  This introduces a systematic bias in favor of broader groupings, because small sample sizes lead to noisier estimates of the underlying model, and hence lower accuracy (on average) at test. To correct for this, in addition to leave-one-out validation, we randomly selecting subsampled each group to be the same size when training models. We use a different random subsample for each talker's training set, with two group sizes: 3 talkers per group (corresponding to the Dialect+Sex grouping) and 7 talkers per group (corresponding to Dialect). These provide a useful lower bound on the true group-level accuracy, and allows accuracy to be compared across grouping levels.

# Results

# Discussion

## A lower bound

* Model each category as a single gaussian distribution.
* Really, each category is a _mixture_ of talker-specific distributions.
* For the purposes of _adaptation_, need to explicitly model this. But it's harder.
* And really what we're trying to get at here is how far the group-level prior _alone_ gets you. This is a __lower bound__ on the utility of group-level knowledge for dealing with an unfamiliar talker.
* Modeling each category as a single distribution of tokens provides an approximation of the predictive distribution you'd get for an _unfamiliar_ talker, where you marginalize out the talker-level models 

## What to track?

* Major question that ideal adapter raises is _what_ linguistic, socio-indexical, and acoustic/phonetic variables listeners are learning distributions for.
    * Doesn't directly answer this, per se, but rather provides a set of tools for thinking about it.
    * When combined with data like we have here, formulate hypotheses about what listeners _should_ do.
* Generally: don't want to track _everything_. Listeners need only track distributions of variables that are informative.
    * Applies at level of phonetic categories: don't track VOT given vowel place.
    * But also applies at the level of socio-indexical groupings.
    * Can actually _hurt_ you to track cue distributions at a level that's not informative (bias-variance tradeoff).
* However, informativity goes beyond better _speech_ recognition.
    * Listeners get a lot of non-linguistic information from speech signal.
    * Have to consider the _goals_ of speech perception, which go beyond just recognizing phonetic categories.
    * Sociolinguistics recognizes that the communication of social information is equally or more important in many cases.
    * Groupings that are socially meaningful can thus be informative and justify being tracked, even if ignoring them has a negligible effect on speech recognition, as long as the corresponding cue distributions carry some information about relevant social variables. 
    * Dialect is a good case: overall, ignoring dialect doesn't have huge consequences on recognition accuracy. But it can be inferred (at least above chance) based on vowel F1 and F2.
    * (( MAYBE )) And the vowels that are most informative about dialec group are not ones whose recognition suffers greatly when dialect is ignored.
    
## Consequences for adapting to unfamiliar talkers

* Adaptation/recalibration varies substantially across contrasts and cues, especially when it comes to generalization across talkers. The ideal adapter provides, in principle, and explanation for these differences: <explain>
* (( OR: ...provides a quantitative framework for explaining? for addressing these differences? ))
* The analysis here provides some evidence that this explanation is correct. When there's no differences across talkers, listeners benefit from assuming that talkers' distributions are the same, until they get evidence to the contrary.

TODO: work this in:
    > The structure of how talkers actually vary in their use of VOT and formants can thus serve as a _prior_ on whether to group observed cue values from one talker with those from another talker. There's a tradeoff in adapting to an unfamiliar talker: On the one hand, the cues produced by a talker are the most informative observations about the cues that talker will produce in the future. On the other hand, listeners often have very few tokens from an unfamiliar talker themselves, and estimates of an underlying distribution from a small number of observations are either highly uncertain or noisy and more likely to be wrong. Thus, in cases where cue distributions are similar across talkers, listeners can benefit by treating those observations as if they came from the same underlying distribution. But in cases where talkers differ a lot, lumping observations from different talkers or groups together will lead to reduced comprehension accuracy and/or slowed adaptation to a talker's own cue distributions. The analyses reported here show that _both_ types of situations actually occur in variation across talkers.
    > 
    > Given this, it's hardly surprising that there are disagreements in the literature about whether listeners generalize what they learn from one talker to another, unfamiliar talker. When adapting to a voicing contrast, listeners are slower (which in the ideal adapter corresponds to stronger prior beliefs, based on more prior observations) and tend to lump different talkers together, requiring more evidence that two talkers produce _different_ cue distributions before they start to classify those talkers' VOT differently. For instance, @Kraljic2007 found that listeners did not recalibrate to a /d/-/t/ contrast in a talker-specific way, but listeners only heard 10 critical trials from each talker. Using a distributional learning paradigm, where listeners heard 300 trials from each talker, @Munson2011 found that listeners _did_ (eventually) learn talker-specific classification function, but that this took hundreds of trials to emerge. For contrasts that show more talker variability, there's (to my knowledge) no work on vowels themselves, but recalibration of fricative contrasts (which _do_ show similar levels of talker and gender variability; @Jongman2000; @McMurray2011a) consistently shows that listeners tend to recalibration separately to male and female talkers [among others: @Eisner2005; @Kraljic2005; @Kraljic2007; @Reinisch2014].

## Distributions vs. recognition accuracy

