---
bibliography: /Users/dkleinschmidt/Documents/papers/library-clean.bib
output:
    html_document:
        code_folding: hide
        dev: png
        keep_md: true
        md_extensions: +implicit_figures
        pandoc_args:
        - --filter
        - pandoc-fignos
        - --filter
        - pandoc-tablenos
    pdf_document:
        md_extensions: +implicit_figures
        pandoc_args:
        - --filter
        - pandoc-fignos
        - --filter
        - pandoc-tablenos
---

```{r preamble, message=FALSE, warning=FALSE, error=FALSE}

library(knitr)
opts_chunk$set(message=FALSE,
               warning=FALSE,
               error=FALSE)

## Produce markdown-formatted figures so that pandoc knows what to do with
## the captions. requires pandoc-fignos to parse the IDs. refer to figures
## in text with {@fig:label} or just @fig:label
## 
## (see https://github.com/tomduck/pandoc-fignos)
knit_hooks$set(plot = function(x, options) {
  paste0('![', options$fig.cap, ']',
         '(', opts_knit$get('base.url'), paste(x, collapse='.'), ')',
         '{#fig:', options$label, '}')
})
## Produce markdown-formatted table captions with anchors for cross-refs.
## Requires pandoc-tablenos to parse the IDs. Refer to tables
## in text with {@tbl:label} or @tbl:label.
## Based partly on http://stackoverflow.com/a/18672268
##
## (see https://github.com/tomduck/pandoc-tablenos)
knit_hooks$set(tbl.cap = function(before, options, envir) {
  if(!before){
    paste0('\n\nTable: ', options$tbl.cap,
           ' {#tbl:', options$label, '}', sep = '')
  }
})

library(magrittr)
library(dplyr)
library(purrr)
library(tidyr)
library(stringr)

library(svglite)
library(ggplot2)
library(cowplot)
## theme_set(theme_bw())

## devtools::install_github('kleinschmidt/daver')
library(daver)

## devtools::install_bitbucket('hlplab/nspvowels')
library(nspvowels)

## devtools::install_bitbucket('hlplab/votcorpora')
library(votcorpora)

apply_groupings <- function(d, groupings) {
  groupings %>% 
    map(~ d %>% mutate(data = map(data, . %>% group_by_(.x)),
                       grouping = .x)) %>%
    reduce(bind_rows)
}

grouping_levels <- c('Marginal', 'Dialect', 'Age', 'Sex', 'Dialect_Sex', 'Talker')

```

# Introduction

* Two traditions in speech perception: phonetics + sociolinguistics, very different approaches to _variation_.
* For phonetics is a __problem__:
    * Have to cope with talker variability/lack of invariance.
* For sociolinguistics, its a __rich source of social information__.
    * The particular variety of the language that you speak says a lot about who you are as a person.
* In recent work, these two approaches have converged.
    * Both _speech recognition_ and _socio-indexical perception_ can be thought of as processes of statistical inference. 
    * Basic idea: how well does each hypothesis explain observed data?
    * For speech recognition: hypotheses are linguistic units (e.g., phonetic categories), observations are acoustic cue values. Inference depends on knowing the _cue distribution_ for each category.
    * In this way of thinking, talker variability means that speech recognition is a _hierarchical_ inference process: talker variability means that the cue distributions change from situation to situation, and so successful speech recognition, requires that listeners infer what the appropriate cue distributions are for each linguistic category in the current situation.
    * For speech recognition, one prediction: listeners benefit from anything that is informative about cue distributions.
    * One thing we know from sociolinguistics is that socio-indexical variables---like age, gender, ethnicity, native language background, social class, region of origin, etc.---provide information about the cue distributions for many different phonetic categories. Hence, ideal adapter predicts that listeners should track group-level cue distributions, for categories where that grouping variable is informative about those distributions.
    * For social perception: knowledge of such group-level distributions is precisely what listeners need (in this way of considering the problem) in order to _infer_ socio-indexical variables. Recall that inference relies on evaluating how well each possible hypothesis explains observed data.
* Raises a critical question: _how_ informative are socio-indexical variables like age, sex, dialect, etc.?
    * ...about underlying cue distributions?
    * ...about phonetic categories?
    * ...about socio-indexical variables themselves?
* Most of the sociolinguistic work has been _descriptive_, and to the extent that it addresses any of these goals, doesn't quantify these factors.
* Use an ideal observer approach to quantitatively assess.
* ((meh)) Tell us something about what sorts of conditional distributions listeners should be tracking (if they're optimal), and lay the groundwork for future empirical work.

## Background

* Know that the amount and structure of talker variability differs between cues and phonetic categories.
    * Vowels/fricatives have a lot, much of it conditioned on sex [@Peterson1952; @Hillenbrand1995; @Jongman2000; @McMurray2011a; @Newman2001]. For vowels this can be largely [but not entirely; @Bladon1984; @Johnson2005; @Johnson2006] attributed to differences in vocal tract length, but not so for fricatives.
    * Stop voicing, less talker variation, and little systematic effect of sex [@Allen2003; @Lisker1964; @Chodroff2015]
    * Stylistic variation in both though. Dialect for vowels [@Clopper2005; @LabovDDDD], and dialect/native language background for voicing [French-English bilinguals, @Caramazza1973; @Flege1987; @Pineda2010; @Sumner2011; Scottish English, @Docherty2011].
* We also know that listeners are sensitive to (real or believed) socio-indexical groups:
    * @Niedzielski1999: say that talker is canadian, hear more canadian raising
    * gender stereotypicality and visual cues affect perception of vowels and fricatives [@Strand1999;@Johnson1999;@Strand1996]
    * indirect evidence comes from recalibration studies that show different patterns of generalization from male to female talkers (and vice-versa) for different cues/contrasts [@Eisner2005; @Kraljic2005; @Kraljic2007; @Reinisch2014]
* Listeners can infer socio-indexical variables but it's not clear what linguistic variables they use.
    * Dialect classification [@Clopper2006,@Clopper2007]
    * [cf. @Thomas2002 for more references]

## Goals

So the _existence_ of group differences is established, as is listeners' _sensitivity_ to them. So, qualitatively, the predictions of the ideal adapter hold up. But we don't have the _quantiative_ measurements we need to test these predictions in more detail. There are three major goals of this paper:

1. Quantitatively assess how _informative_ different socio-indexical variables are about phonetic cue distributions.
2. Determine the _utility_ of socio-indexical variables for accurate speech recognition.
3. Show that knowledge of cue distributions allows listeners to make inferences about socio-indexical variables in the same way as linguistic variables.

We address these goals at different levels of socio-indexical grouping, and for two different sets of phonetic cues/contrasts. The overarching goal is to quantify the importance of socio-indexical variables in a common currency that allows comparison _across_ variables and cues/contrasts.  Comparing multiple contrasts is a first step in refining the predictions of the ideal adapter framework about what distributions listeners should track.

# Methods

## Datasets

We analyze speech from two corpora, one focusing on vowels and the other on stop consonant voicing.

### Vowels

```{r nsp-data, cache=TRUE, results='hide'}

nsp_vows <- nspvowels::nsp_vows %>%
  ungroup() %>%
  mutate(Marginal='all', Dialect_Sex = paste(Sex, Dialect, sep='_'))

nsp_vows_lob <- nsp_vows %>%
  group_by(Talker) %>%
  mutate_each(funs(. %>% scale() %>% as.numeric()), F1:F2) %>%
  ungroup()

## check normalization
nsp_vows_lob %>%
  gather(formant, value, F1:F2) %>%
  group_by(Talker, formant) %>%
  summarise_each(funs(mean, sd), value) %$%
  assert_that(all.equal(mean, rep(0, length(mean))),
              all.equal(sd, rep(1, length(sd))))

vowel_data <- data_frame(cues = c('Un-normalized F1xF2', 'Lobanov Normalized F1xF2'),
                         data = list(nsp_vows, nsp_vows_lob))
vowel_groupings <- c('Marginal', 'Sex', 'Dialect', 'Dialect_Sex', 'Talker')
vowel_data_grouped <- apply_groupings(vowel_data, vowel_groupings)

token_per_vow <- nsp_vows %>% group_by(Talker, Vowel) %>% tally() %$% mean(n)
n_talkers <- nsp_vows %>% group_by(Talker) %>% summarise() %>% tally()

n_per_dialect_sex <- nsp_vows %>% group_by(Dialect, Sex, Talker) %>% summarise() %>% tally() %$% unique(n)
n_dialect <- nsp_vows %$% Dialect %>% unique() %>% length()

```

For vowels, we used data from the Nationwide Speech Project [@Clopper2005]. Specifically, we analyzed first and second formant frequencies (F1xF2, measured in Hertz) recorded at vowel midpoints in isolated, read "hVd" words. This corpus contains `r n_talkers` talkers, `r n_per_dialect_sex` male and female from each of `r n_dialect` regional varieties of English: North, New England, Midland, Mid-Atlantic, South, and West [see map in @Clopper2005; regions based on @Labov2005].  Each talker provided approximately `r round(token_per_vow, 1)` repetitions of each of 11 English monophthong vowels (plus "ey"), for a total of `r nrow(nsp_vows)` observations.

Because much of the variability in talkers is due to overall differences in formant frequencies, we also used Lobanov-normalized formant frequencies as input, in addition to the un-normalized formant frequencies in Hertz. Lobanov normalization z-scores F1 and F2 separately for each talker [@Lobanov1971].

### Stop voicing

```{r vot-data, cache=TRUE}

vot <-
  votcorpora::vot %>%
  filter(source == 'buckeye') %>%
  rename(Talker = subject,
         Sex = sex,
         Age = age_group) %>%
  group_by(phoneme, Talker) %>%
  mutate(Token = row_number(),
         cues = 'VOT') %>%
  ungroup() %>%
  mutate(Marginal = 'all')

vot_by_place <-
  vot %>%
  group_by(place, cues) %>%
  nest()

vot_groupings <- c('Marginal', 'Sex', 'Age', 'Talker')
vot_by_place_grouped <- apply_groupings(vot_by_place, vot_groupings)

n_vot_talkers <- vot %>% group_by(Talker) %>% summarise() %>% nrow()
n_vot_per_talker <- vot %>% group_by(phoneme, voicing, place, Talker) %>% tally()

```

We analyzed data on word-initial stop consonant voicing in conversational speech from the Buckeye corpus [@Pitt2007]. Voice onset time (VOT) was automatically extracted for `r nrow(vot)` word initial stops, `r vot %>% filter(voicing=='voiced') %>% nrow()` voiced and `r vot %>% filter(voicing=='voiceless') %>% nrow()` voiceless, for labial, coronal, and dorsal places of articulation (Wedel, _in prep_). Data came from `r n_vot_talkers` talkers, who were balanced male and female and younger/older than 40 years. On average, each talker produced `r round(mean(n_vot_per_talker$n))` tokens for each phoneme (range of `r min(n_vot_per_talker$n)` -- `r max(n_vot_per_talker$n)`).

## Modeling

Each phonetic category was modeled as a normal distribution: stop voicing as univariate distributions of VOT, and vowels as bivariate distributions of F1 and F2. We used the maximum likelihood estimators for the model parameters, which are the sample mean and covariance matrix.

For each socio-indexical grouping level, we trained separate models for each phonetic category based on all tokens from that category and grouping level (holding out test data when necessary, see below). The grouping levels we considered were

* Marginal (all tokens)
* Sex (male/female)
* Age (younger/older than 40, VOT only)
* Dialect (six regions, vowels only)
* Diaelct+Sex (12 levels, vowels only)
* Talker

### Comparing cue distributions

In order to evalute the _informativity_ of socio-indexical variables with respect to cue distributions themselves, we use Kullback-Leibler (KL) divergence to measure how much the group-specific cue distributions differ from the overall (marginal) cue distributions. This measures the expected cost (in bits of extra message length) of encoding data from each group using a code that's optimized for the marginal distribution.
We do this separately for each linguistic category and group, and then average the results, calculating bootstrapped confidence intervals over groups.

The KL divergence of $Q$ from $P$ is $DL(Q||P) = \int p(x) \log \frac{p(x)}{q(x)} \mathrm{d}x$ (with density functions $q$ and $p$ respectively). In our case, $P=\mathcal{N}_G$ is a multivariate[^multivariate] normal cue distribution for the group, with mean $\mu_G$ and covariance $\Sigma_G$, while $Q=\mathcal{N}_M$ is the marginal multivariate normal cue distribution with mean $\mu_M$ and covariance $\Sigma_M$. With some simplification[^gaus-kl], the KL divergence of the marginal from the group distribution works out to be
$$
DL(\mathcal{N}_M || \mathcal{N}_G) = \frac{1}{2} \left( \mathrm{tr}(\Sigma_M^{-1} \Sigma_G) + (\mu_M - \mu_G) \Sigma_M^{-1} (\mu_M - \mu_G) - d + \log\frac{|\Sigma_M|}{|\Sigma_G|} \right)
$$
where $d$ is the dimensionality of the distribution, and logs are natural logarithms (we report KL divergence in bits, which is the above quantity divided by $log(2)$).

[^multivariate]: The math is the same for the univariate special case, as with VOT.
[^gaus-kl]: See, for instance, [http://stanford.edu/~jduchi/projects/general_notes.pdf](), p. 13.

### Speech recognition

Next, to address the _utility_ of socio-indexical grouping for speech recognition, we calculate, for each level of socio-indexical grouping, the probability of correct recognition of phonetic categories. We do this using an "ideal listener" model [@Clayards2008; @Kleinschmidt2015] that compute the posterior probability of a category given an observed cue value based on the likelihood of that cue being generated by each category's cue distribution. By doing this using, for instance, the cue distributions of each category produced by female talkers provides an estimate of how well a listener would be able to recognize speech from an unfamiliar female talker if all they knew was the talker's sex.

We want to determine the phonetic category $v_i$[^notation] of each of the cues $x_i$ produced by a talker. If we assume that the listener knows that this talker belongs to group $g=j$, this inference is a straightforward application of Bayes Rule:
$$
p(v_i | x_i, g=j) \propto p(x_i | v_i, g=j) p(v_i)
$$
If, on the other hand, the listener does not know which group the talker belongs to, they have to marginalize out group. This ammounts to taking a weighted average of the posterior probabilities under each group, weighted by the probability that the talker belongs to that group, $p(g | x)$ (which we compute below):
$$
p(v_i | x_i) = \sum_j p(v_i | x_i, g=j) p(g=j | x)
$$
(where $x$ refers to all the tokens produced by this talker).

For vowels, we classified vowel categories directly. For voicing, the only cue available is VOT, which does not (reliably) distinguish place of articulation. Thus, we classified voicing separately for each place of articulation, and then average the resulting accuracy.

### Indexical group recognition

Finally, to address much cue distributions tell listeners about socio-indexical variables themselves, we classify each talker's socio-indexical group (at each level). This provides a measure of how well a listener would be able to determine, for instance, whether a talker was male or female based only on the distributions of cues they produce (even without knowing the intended category of each production).

As for speech recognition, we use an ideal observer model. That is, we compute the posterior probability of each socio-indexical group $g=j$, given all of the talker's observed cue values $x$:
$$
p(g | x) \propto p(x | g) p(g) = \left(\prod_i p(x_i | g) \right) p(g)
$$

The only complication is that, without knowing the the phonetic category of each observation a priori, each observation may have been generated by any of the phonetic categories. Thus, to determine the _overall_ likelihood of observing a cue value $x_i$ under group $g$, we first have to marginalize over categories $v_i$:
$$
p(x_i | g) = \sum_k p(x_i | v_i=k, g) p(v_i=k | g)
$$

For all of these classifications, we assume a flat prior on categories/groups. We perform this analysis separately for each level of socio-indexical grouping. For instance, we compute both $p(\mathrm{sex} | x)$ and $p(\mathrm{dialect} | x) for each talker.

[^notation]: $x_i$ refers to a single observed cue value (possibly multidimensional, in the case of vowel formants), and $x$ (without subscript) refers to a _vector_ of multiple observations (from a single talker, unless otherwise specified). $v_i$ refers to observation $i$'s category, and $g$ to a talker's group. $\sum_j$ refer to a sum over all possible values of $j$.

### Controls

#### Cross-validation

For classification, if test data is included in the training set, this artificially inflates accuracy at test [@James2013, Section 5.1].  Cross-validation controls for this by splitting data into training and test sets.  For group-level models (sex, age, dialect, and dialect+sex), we use leave-one-talker-out cross-validation: train each group's models with test talker's observations held out. For the talker-specific models, we use 6-fold cross-validation (or leave-one-out when there were fewer than 6 tokens in a category for a talker), where each phonetic category is randomly split into 6 approximately equal subsets. Then, one subset of each category is selected for test, the models are trained on the remaining five, and the test data is classified as above.

Cross-validation is not only important because it provides an unbiased measure of classifier accuracy. It is also essential for testing the hypothesis that _group-level_ cue distributions are useful to listeners.  If the test talker is included in the training dataset, then the utility of that talker's own productions is confounded with any utility of the group itself.

#### Different group sizes

For the vowel data, the different levels of grouping have very different group sizes, and this requires some caution. The broadest (sex) has 24 talkers per group (23 after holdout), while the most specific (dialect+sex) has only 4 (3 after holdout).  This introduces a systematic bias in favor of broader groupings, because small sample sizes lead to noisier estimates of the underlying model, and hence lower accuracy (on average) at test [@James2013, Section 2.2.2]. To correct for this, after holding out the test talker, we randomly subsampled talkers (without replacement) within each group in the training set to be the same as the smallest group (3 talkers, based on Dialect+Sex[^seven-talkers]). We use a different random subsample for each talker's training set. Thus, any additional variance introduced is accounted for by bootstrapping talkers.  The estimates obtained in this way allow us to compare accuracy across groupings with different group sizes, but at the cost of underestimating the true group-level accuracy across the board. As such, they must be considered a useful lower bound on the utility of socio-indexical groupings.

[^seven-talkers]: We also ran the analyses resampling each group to 7 talkers, which corresponds to the Dialect-level group size after holdout (excluding the Dialect+Sex grouping, since there are only 4 talkers per group before holdout). Besides a small increase in overall accuracy (because of the reduced variance of the distribution estimates), this did not substantially change the results.


# Results

## Informativity of socio-indexical groupings about cue distributions

```{r kl-helpers, cache=TRUE}

run_kl <- function(data_grouped, reference_grouping,
                   category_col='Vowel', cue_cols=c('F1', 'F2'), ...) {
  ## check input format
  assert_that(has_name(data_grouped, 'data'),
              has_name(data_grouped, 'grouping'))

  train <- partial(train_models, grouping=category_col, formants=cue_cols,  ...)
  
  models <- data_grouped %>%
    mutate(models = map2(data, grouping, ~ train(.x) %>% rename_(group=.y)))

  models %>%
    filter(grouping == reference_grouping) %>%
    mutate(reference_models = map(models, rename_, reference_group = 'group')) %>%
    select(-grouping, -data, -models) %>%
    left_join(models %>% filter(grouping != reference_grouping)) %>%
    mutate(kl_from_reference = map2(models, reference_models,
                                    ~ left_join(.x, .y, by=category_col) %>%
                                      mutate(KL = map2_dbl(model.x, model.y,
                                                           KL_mods)) %>%
                                      select_('group', 'reference_group',
                                              category_col, 'KL')
                                    )
           ) %>%
    unnest(kl_from_reference)
    
}

```

```{r vowel-kl, cache=TRUE, dependson=c('kl-helpers', 'vowel-data')}

vowel_kl <-
  vowel_data_grouped %>%
  run_kl(reference_grouping = 'Marginal',
         category_col = 'Vowel',
         cue_cols = c('F1', 'F2')) %>%
  filter(!is.na(KL))                    # NAs come from one vowel ('uh') that
                                        # only has one token for one talker.


```

```{r vot-kl, cache=TRUE, dependson=c('kl-helpers', 'vot-data')}

vot_kl <-
  vot_by_place_grouped %>%
  run_kl(reference_grouping = 'Marginal',
         category_col = 'voicing',
         cue_cols = 'vot')

```

```{r vowel-vot-kl-plot, fig.width=8.3, fig.height=4.2, fig.cap='Socio-indexical variables are more informative about cue distributions for vowel (formants) than for stop voicing (vot). On top of this, more specific groupings (like Talker and Dialect+Sex) are more informative than broader groupings (Sex). This is indicated by higher KL divergence of each grouping level from marginal (showing mean and 95% bootstrapped CIs over groups).'}

bind_rows(vot_kl %>% mutate(contrast = 'Stop voicing'),
          vowel_kl %>% mutate(contrast = 'Vowels')) %>%
  mutate(grouping = factor(grouping, levels=grouping_levels)) %>%
  group_by(contrast, cues, grouping, group) %>%
  summarise(KL = mean(KL)) %>%
  ggplot(aes(x=grouping, y=KL, color=grouping)) +
  geom_point(position='jitter', alpha=0.2) + 
  geom_pointrange(stat='summary', fun.data=mean_cl_boot,
                  position = position_dodge(w=0.5)) +
  facet_grid(.~contrast+cues, scales='free', space='free') +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  labs(x = 'Grouping (broadest to most specific)',
       y = 'KL Divergence of cue distributions\nfrom marginal (bits)')

```

Figure {@fig:vowel-vot-kl-plot} plots the KL divergence of cue distributions at different levels of grouping from marginal distributions, across contrasts (vowels and stop voicing) and cues (vot, raw/Lobanov-normalized F1xF2). There are two clear patterns. 

First, socio-indexical variables are in general more informative for vowels than for stop voicing, even using normalized formants as input. Strikingly, the _most_ informative variable for VOT---talker identity---is roughly as informative as the _least_ informative variable for Lobanov-normalized F1xF2 (Sex). As Figure {@fig:vot-vs-vowel-dists} shows, this means that, on average, individual talkers' VOT distributions diverge from the marginal distribution (B) at roughly the same level that the male and female distributions of normalized F1xF2 diverge from the marginal normalized F1xF2 distributions (A).

```{r vot-vs-vowel-dists, fig.width=6.2, fig.height=6.0, fig.cap='Male and female distributions of Normalized F1xF2 diverge from the marginal distributions (A) only slightly less than talker-specific VOT distributions diverge from marginal (B).'}

gg_vowels_by_sex <-
  ggplot(nsp_vows_lob, aes(x=F2, y=F1, group=Vowel)) +
  ## stat_density2d(data = nsp_vows_lob %>% select(-Sex),
  ##                geom='contour', bins=4, color='#888888') +
  ## stat_density2d(geom='contour', bins=4, aes(color=Sex)) +
  stat_ellipse(data = nsp_vows_lob %>% select(-Sex),
               color='#888888', type='norm') +
  stat_ellipse(aes(color=Sex), type='norm') +
  scale_x_reverse() +
  scale_y_reverse() +
  facet_grid(.~Sex) +
  labs(color = 'Sex')

gg_vot_by_talker <-
  ggplot(vot, aes(x=vot, group=voicing)) +
  stat_density(aes(group=paste(Talker,voicing), color='Talker'),
               geom='line',
               position='identity',
               alpha = 0.5) +
  stat_density(aes(color='Marginal'),
               geom='line',
               position='identity') +
  scale_color_manual("", values = c('black', 'gray')) +
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank())

ggdraw() +
  draw_plot(gg_vowels_by_sex, 0, 0.5, 1, 0.5) +
  draw_plot(gg_vot_by_talker, 0.07, 0, 0.93, 0.5) +
  draw_plot_label(c("A", "B"), c(0, 0), c(1, 0.5), size = 15)

## plot_grid(gg_vowels_by_sex, gg_vot_by_talker, labels=c('A', 'B'), nrow=2)

```

Second, there are substantial differences in the informativity of the socio-indexical grouping variables we considered. The overall pattern is that more specific grouping factors are more informative than broader groupings. The noteable exception to this pattern is the Sex is the most informative variable for un-normalized F1xF2 distributions, which reflects the fact that overall sex differences explain much (but not all) of the talker variation in F1xF2 [@Hillenbrand1995;@Johnson2006].

As Figure {@fig:vowel-kl-by-category}, while the relative ordering of grouping variables' informativity is consistent across vowels, their actual degree of informativity varies quite a bit. Dialect (and Dialect+Sex) is particularly informative for `aa`, `ae`, `eh`, and `uw`, vowels with distinctive variants in at least one of the dialect regions from the NSP. `aa` is undergoing a merger with `ao` in some regions, `ae` and `eh` participate in the northern cities chain shift, and `uw` is fronted in some regions [and in others, but only by female talkers; @Clopper2005].

```{r vowel-kl-by-category, fig.cap='Individual vowels vary substantially in the informativity of grouping variables about their cue distributions. Only normalized F1xF2 is shown to emphasize dialect effects.'}

vowel_kl %>%
  mutate(grouping = factor(grouping, levels=grouping_levels)) %>%
  filter(str_detect(cues, 'Normalize')) %>%
  group_by(cues, Vowel, grouping, group) %>%
  summarise(KL = mean(KL)) %>%
  ggplot(aes(x=Vowel, y=KL, color=grouping)) +
  geom_pointrange(stat='summary', fun.data=mean_cl_boot,
                  position = position_dodge(w=0.5)) +
  facet_grid(cues ~ ., scales='free', space='free') +
  labs(x = 'Vowel',
       y = 'KL Divergence of cue distributions\nfrom marginal (bits)')

```

## Utility of socio-indexical groupings for speech recognition

```{r classification-helpers, cache=TRUE}

## 1. Likelihood of each token under each vowel for each dialect model
#' Compute posterior vowel category conditional on group
#'
#' Applies classify_vowels to test data for each group_model.
#'
#' @param data_test test data to calculate posteriors for
#' @param group_models named list of group models, each of which is a named list
#'   of vowel models
#' @return data frame with one row per data_test row x group x vowel model, with
#'   added columns group_model (name of group model), vowel_model (name of vowel
#'   model), lhood p(x | vowel_model, group_model), posterior (p(vowel_model |
#'   x, group_model)).
compute_category_post_given_group <- function(data_test, group_models) {
  group_models %>%
    map(~ unlist_models(., 'category')) %>%
    map(~ classify(data_test, ., 'category')) %>%
    data_frame(group_model=names(.),
               x=.) %>%
    unnest(x) %>%
    rename(category_model=model)
}

#' Combine category | group posteriors with group posteriors
#'
#' @param group_category_posteriors category posterior probabilities conditional
#'   on group, in the form of a data frame with at least columns category_model,
#'   group_model, and posterior (e.g., output of
#'   compute_category_post_given_group)
#' @param group_posterior marginal group posterior probabilities, in the form of
#'   a data frame with columns group_model and log_posterior (e.g., output of
#'   compute_group_marginal_posterior)
#' @return a data frame with the joint posterior of category category and group,
#'   in posterior and log_posterior.
#' 
compute_joint_category_group_post <- function(group_category_posteriors, group_posteriors) {
  group_posteriors %>%
    select(group_model, group_log_posterior=log_posterior) %>%
    inner_join(group_category_posteriors, by = 'group_model') %>%
    mutate(log_posterior = log(posterior) + group_log_posterior,
           posterior = exp(log_posterior))
}

#' Compute joint indexical-linguistic posterior
#'
#' @param trained data frame with \code{models} and \code{data_test} (as
#'   produced by \code{\link{train_models_indexical_with_holdout}}).
#' @param obs_vars quoted names of columns in test data that together indentify
#'   a single observation (e.g., \code{c('Vowel', 'Token')})
#' @return a data frame with one observation per combination of group (e.g.,
#'   Dialect), category (e.g. "ae"), and row in the ORIGINAL, un-nested data
#'   set, with new columns \code{group_model}, \code{category_model},
#'   \code{lhood}, \code{posterior}, and \code{log_posterior}. Posterior
#'   probabilities sum to 1 within each cross-validation fold (e.g., Talker) +
#'   observation (e.g., Vowel+Token) combination, over all values of category
#'   and group.
#' 
trained_to_joint_post <- function(trained, obs_vars) {

  trained %>%
    mutate(conditional_posteriors = map2(data_test, models,
                                         compute_category_post_given_group),
           group_posteriors = map(conditional_posteriors,
                                  . %>%
                                    group_by_(.dots=obs_vars) %>%
                                    marginalize('group_model') %>%
                                    aggregate_lhood('group_model')),
           joint_posteriors = map2(conditional_posteriors, group_posteriors,
                                   compute_joint_category_group_post)) %>%
    unnest(joint_posteriors)
}


#' @param d data frame
#' @param holdout Column defining cross validation folds
#' @param ... additional arguments passed to \code{\link{train_models}}.
classify_by_talker_cv <- function(d, holdout='Token', category='Vowel', ...) {
  train <- partial(train_models, grouping=category, ...)

  d %>%
    nspvowels::train_test_split(holdout=holdout) %>%
    mutate(models_trained = map(data_train,
                                . %>% group_by(Talker) %>% train()),
           models_tested = map2(data_test, models_trained, classify,
                                category=category)) %>%
    unnest(models_tested) %>%
    mutate(grouping = 'Talker',
           group_is = 'Known',
           group = Talker) %>%
    rename(category_model = model)
}


```


```{r vowel-classification-models, cache=TRUE, dependson=c('vowel-data', 'classification-helpers')}

min_talker_per_group <- function(d) {
  d %>%
    do(n = length(unique(.$Talker))) %>%
    select_('n') %>%
    unlist() %>%
    min()
}

vowel_models <-
  vowel_data_grouped %>%
  mutate(group_size = map_dbl(data, min_talker_per_group)) %>%
  right_join(cross_d(list(group_size = unique(.$group_size),
                          subsample_size = c(3,7))) %>%
               filter(group_size > subsample_size)) %>%
  mutate(trained = pmap(list(data, grouping, subsample_size),
                        train_models_indexical_subsample_holdout),
         joint_posteriors = map(trained, trained_to_joint_post))

```

```{r vowel-classification, cache=TRUE, dependson=c('vowel-classification-models', 'classification-helpers')}

vowel_joint_class <-
  vowel_models %>%
  unnest(map2(joint_posteriors, grouping, ~ rename_(.x, group=.y))) %>%
  group_by(cues, grouping, subsample_size)

vowel_marginal_class <-
  vowel_joint_class %>%
  group_by(Talker, Vowel, Token, group, category_model, add=TRUE) %>%
  marginalize_log('log_posterior') %>%
  normalize_log_probability('log_posterior') %>%
  mutate(group_is = 'Inferred')

vowel_true_group_class <-
  vowel_joint_class %>%
  filter(group == group_model) %>%
  group_by(Talker, Vowel, Token, add=TRUE) %>%
  normalize_log_probability('log_posterior') %>%
  mutate(group_is = 'Known')

vowel_talker_class <-
  vowel_data %>%
  unnest(map(data, classify_by_talker_cv, holdout='Token'))

vowel_class <-
  bind_rows(vowel_marginal_class,
            vowel_true_group_class,
            vowel_talker_class)

```

```{r vot-classification-models, cache=TRUE, dependson=c('vot-data', 'classification-helpers')}

vot_models <-
  vot_by_place_grouped %>%
  filter(grouping != 'Talker') %>%
  mutate(trained = map2(data, grouping,
                        ~ train_models_indexical_with_holdout(.x,
                                                              .y,
                                                              category = 'voicing',
                                                              formants = 'vot')),
         joint_posteriors = map(trained,
                                ~ trained_to_joint_post(., c('voicing', 'Token')))
         )

```

```{r vot-classification, cache=TRUE, dependson=c('vot-classification-models', 'classification-helpers')}

vot_joint_class <-
  vot_models %>%
  unnest(map2(joint_posteriors, grouping, ~ rename_(.x, group=.y))) %>%
  group_by(place, cues, grouping)

vot_marginal_class <-
  vot_joint_class %>%
  group_by(Talker, voicing, Token, group, category_model, add=TRUE) %>%
  marginalize_log('log_posterior') %>%
  normalize_log_probability('log_posterior') %>%
  mutate(group_is = 'Inferred')

vot_true_group_class <-
  vot_joint_class %>%
  filter(group == group_model) %>%
  group_by(Talker, voicing, Token, add=TRUE) %>%
  normalize_log_probability('log_posterior') %>%
  mutate(group_is = 'Known')

vot_talker_class <-
  vot_by_place %>%
  unnest(map(data, . %>%
                     group_by(Talker, voicing) %>%
                     mutate(split=ntile(runif(length(Talker)), 6)) %>%
                     classify_by_talker_cv(holdout='split',
                                           category='voicing',
                                           formants='vot')
             )
         )

vot_class <-
  bind_rows(vot_marginal_class,
            vot_true_group_class,
            vot_talker_class)

```

```{r check-classification, results='hide', cache=TRUE, dependson=c('vowel-classification', 'vot-classification')}

vowel_class %>%
  group_by(cues, group_is, grouping, subsample_size, Talker, Vowel, Token) %>%
  summarise(n_choice = sum(posterior_choice),
            sum_post = sum(posterior)) %$%
  assert_that(all(n_choice == 1),
              all.equal(sum_post, rep(1, length(sum_post))))

vot_class %>%
  group_by(cues, place, group_is, grouping, Talker, voicing, Token) %>%
  summarise(n_choice = sum(posterior_choice),
            sum_post = sum(posterior)) %$%
  assert_that(all(n_choice == 1),
              all.equal(sum_post, rep(1, length(sum_post))))

```

```{r vowel-class-subset, cache=TRUE, dependson=c('vowel-classification')}

## only use known group, subsample
vowel_class_all <- vowel_class

vowel_class <-
  vowel_class_all %>%
  filter(group_is == 'Known',
         subsample_size == 3 | grouping == 'Talker')

vot_class_all <- vot_class

vot_class <-
  vot_class_all %>%
  filter(group_is == 'Known')
```


```{r classification-accuracy}

#' Two methods to compute accuracy from classifier
#'
#' @param tbl classifier output
#' @param category_col name of column specifying true category
#' @param method if 'choice' (default), accuracy is 1 if choice is correct,
#'   0 if not. if 'posterior', accuracy is posterior probability of true
#'   category
#' @return tbl with accuracy in column \code{accuracy}
get_accuracy <- function(tbl, category_col, method='choice') {
  assert_that(method %in% c('choice', 'posterior'))
  assert_that(has_name(tbl, category_col))

  category_eq_mod <- lazyeval::interp(~var==category_model,
                                      var=as.name(category_col))

  if (method == 'choice') {
    tbl %>%
      filter(posterior_choice) %>%
      mutate_(accuracy = category_eq_mod)
  } else if (method == 'posterior') {
    tbl %>%
      filter_(category_eq_mod) %>%
      mutate_(accuracy = ~ posterior)
  }

}

acc_method <- 'choice'

vowel_accuracy <- 
  vowel_class %>% get_accuracy('Vowel', method=acc_method)

vot_accuracy <-
  vot_class %>% get_accuracy('voicing', method=acc_method)

accuracy <- 
  vowel_accuracy %>%
  select(-Age) %>%
  mutate(contrast = 'Vowels') %>%
  bind_rows(vot_accuracy %>% mutate(contrast = 'Stop voicing')) %>%
  ungroup() %>%
  mutate(grouping = factor(grouping, levels=grouping_levels))

accuracy_summary <-
  accuracy %>%
  group_by(contrast, cues, grouping, group_is, Talker) %>%
  summarise(accuracy = mean(accuracy)) %>%
  do({ mean_cl_boot(.$accuracy) }) %>%
  rename(accuracy = y,
         accuracy_lo = ymin,
         accuracy_hi = ymax)

## Talker advantage
## (TODO: incorporate)
accuracy_talker_advantage_summary <- 
  accuracy %>%
  group_by(cues, contrast, grouping, Talker) %>%
  summarise(accuracy=mean(accuracy)) %>%
  rename(Talker_=Talker) %>%
  spread(grouping, accuracy) %>%
  gather(comparison, accuracy, -cues, -contrast, -Talker, -Talker_) %>%
  mutate(talker_advantage = Talker - accuracy) %>%
  group_by(contrast, cues, comparison) %>%
  do({ boot_ci(.$talker_advantage, function(d,i) mean(d[i], na.rm=TRUE)) }) %>%
  filter(is.finite(observed))

```


```{r overall-accuracy-group-known, fig.width=8.3, fig.height=4.2, fig.cap='Speech recognition accuracy using for marginal, group-level, and talker-specific cue distributions. Small points show individual talkers, and large points and lines show mean and bootstrapped 95% CIs over talkers. Marginal and group-level accuracy is based on leave-one-talker out cross-validation, and talker-specific on 6-fold cross-validation (or leave-one-token-per-category out if there are fewer than 6 tokens per category).'}

accuracy_summary %>%
  ggplot(aes(x = grouping, y=accuracy,
             color = grouping)) +
  geom_point(data = accuracy %>%
               group_by(grouping, Talker, cues, contrast) %>%
               summarise(accuracy = mean(accuracy)),
             position='jitter', alpha=0.2) +
  geom_pointrange(aes(ymin=accuracy_lo, ymax=accuracy_hi), stat='identity') +
  facet_grid(.~contrast+cues, scales='free_x', space='free_x') +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  labs(x = 'Grouping (broadest to most specific)',
       y = 'Probability of correct recognition') +
  lims(y = c(NA, 1))

```


Figure {@fig:overall-accuracy-group-known} shows the probability of correct recognition for stop voicing/vowel, based on the cue distrbutions at each level of grouping.  As with informativity about the distributions themselves, there's an asymmetry between vowels and stop voicing in the utility of socio-indexical variables for speech recognition. Probability of correct recognition is overall higher for stop voicing than vowels.  Voicing recognition is also less sensitive to the particular grouping variable, which is consistent with the finding from the last section that VOT distributions do not differ substantially at different levels of socio-indexical grouping. There is a slight advantage for using talker-specific VOT distributions for recognition, but this is not statistically reliable.

Normalized input results in higher vowel recognition accuracy across the board, again paralleling the findings about the cue distributions themselves. The one exception is at the level of talker-specific distributions, where recognition accuracy is unchanged (since Lobanov normalization is a linear transformation of the input, which leaves the structure of the categories within each talker unchanged).

Also paralleling the cue distributions themselves, classifying according to sex-specific distributions provides the biggest boost in recognition for un-normalized formant accuracy. For normalized input, none of the socio-indexical grouping factors provide much of an advantage over the marginal distributions. In both cases, dialect provides minimal benefit for recognition, alone or in combination with sex.

```{r by-vowel-acc-group-known, fig.width=10, fig.height=5}

acc_by_vowel <- accuracy %>%
  filter(contrast == 'Vowels') %>%
  group_by(cues, group_is, grouping, subsample_size, Vowel, Talker) %>%
  summarise_each(funs(mean), accuracy)

acc_by_vowel_summary <-
  acc_by_vowel %>%
  do({ mean_cl_boot(.$accuracy) }) %>%
  rename(accuracy = y,
         accuracy_lo = ymin,
         accuracy_hi = ymax)

acc_by_vowel_summary %>%
  ggplot(aes(x=Vowel, y=accuracy, color=grouping)) +
  geom_pointrange(aes(ymin=accuracy_lo, ymax=accuracy_hi),
                  position = position_dodge(w=0.7)) +
  facet_grid(cues~.)

```

Figure {@fig:by-vowel-acc-group-known} shows probability of correct recognition for each vowel individually. The pattern of probability of correct recognition given each socio-indexical grouping variable for individual vowels largely mirrors the overall pattern. For normalized input, there is little (if any) utility of socio-indexical grouping variables for correct recognition, with the possible exception of `ae` and `eh`. For non-normalized input, the effect of sex dominates.

### Does dialect contribute anything?

Given the well-known dialect-specific variation in vowel production, it's somewhat surprising that dialect-level cue distributions do not, in our models, provide a substantial overall improvement in correct recognition.  But there are two reasons why these results can be misleading. 

First, not all vowels vary substantially from the norm in all dialects. To address this possibility, we examine how using dialect-level formant distributions affects the probability of making particular _errors_ that would result from ignoring dialect-specific shifts. For instance, in the Northern Citiies Shift, the `ae` vowel is raised, resulting in a distribution very much like the standard `eh` distribution. If dialect is ignored, then `ae` tokens from talkers from a dialect region with this shift will be mistaken for `eh`.

```{r dialect-error-table, results='asis', tbl.cap='Dialect-predicted confusions, based on the summary by @Clopper2005.'}

nspvowels::clopper_competitors %>% knitr::kable()

```

```{r socio-relevant-vowel-acc, fig.width=10, fig.height=6, fig.cap='Rates of dialect-specific errors under each level of socio-indexical grouping. Stars indicate errors that are significantly reduced by considering dialect (either over marginal, or over sex).'}

vowel_class_competitors <-
  vowel_class %>%
  ungroup() %>%
  mutate(grouping = factor(grouping, levels=grouping_levels)) %>%
  select(-Dialect) %>%
  left_join(nsp_vows %>% group_by(Talker, Dialect) %>% summarise(), by='Talker') %>%
  inner_join(nspvowels::clopper_competitors, by = c('Vowel', 'Dialect'))

vowel_competitor_errors <-
  vowel_class_competitors %>%
  filter(category_model == Competitor) %>%
  group_by(cues, grouping, Dialect, Vowel, category_model, Talker) %>%
  summarise(confusions = mean(posterior_choice)) %>%
  mutate(pair = paste(Vowel, category_model, sep=' -> '))

vowel_competitor_errors_summary <-
  vowel_competitor_errors %>%
  group_by(cues, grouping, Dialect, pair, Talker) %>%
  summarise(confusions = mean(confusions)) %>%
  do({ boot_ci(.$confusions, function(d,i) mean(d[i])) })

## dialect advantage for each confusion
vowel_competitor_dialect_summary <-
  vowel_competitor_errors %>%
  rename(Dialect_ = Dialect, Talker_ = Talker) %>%
  spread(grouping, confusions) %>%
  group_by(cues, Dialect_, Vowel, pair, Talker_) %>%
  transmute(Dialect_over_marginal = Dialect - Marginal,
            Dialect_over_sex = Dialect_Sex - Sex) %>%
  rename(Dialect = Dialect_, Talker = Talker_) %>%
  summarise_each(funs(mean), Dialect_over_marginal, Dialect_over_sex) %>%
  gather('comparison', 'value', Dialect_over_marginal, Dialect_over_sex) %>%
  group_by(cues, Dialect, Vowel, pair, comparison) %>%
  do({ boot_ci(.$value, function(d,i) mean(d[i]), h0=0) })
  
vowel_competitor_dialect_stars <-
  vowel_competitor_dialect_summary %>%
  filter(observed < -1e-10, boot_p < 0.10) %>%
  group_by(cues, Dialect, Vowel, pair) %>%
  summarise(boot_p = min(boot_p)) %>%
  mutate(boot_p_stars = daver::p_val_to_stars(boot_p, approaching=TRUE)) 

pd <- function() position_dodge(w=0.7)

vowel_competitor_errors %>%
  ggplot(aes(x=pair, y=confusions)) +
  ## geom_bar(stat='summary', fun.y = mean, position=pd()) +
  geom_pointrange(aes(color=grouping),
                  stat='summary', fun.data=mean_cl_boot, position=pd()) +
  geom_text(data = vowel_competitor_errors_summary %>%
              group_by(cues, Dialect, pair) %>%
              summarise(max_ci = max(ci_high)) %>%
              right_join(vowel_competitor_dialect_stars),
            aes(y=max_ci+0.02, label=boot_p_stars), color='black') +
  facet_grid(cues~Dialect, space='free', scales='free') +
  ggtitle('Socio-linguistically predicted confusions') +
  labs(y = 'Proportion error occurred', x = 'Error') +
  theme(axis.text.x = element_text(angle=45, hjust=1))

```

Based on the summary by @Clopper2005, we compiled a list of likely dialect-specific errors (Table {@tbl:dialect-error-table}). Note that not all dialect-specific changes in vowel _distributions_ will lead to _errors_. For instance, `uw` fronting occurs in many dialects, but this shift moves `uw` into otherwise unoccupied territory and likely will have no consequences for correct recognition (at least to the extent that it occurs in this dataset). Figure {@fig:socio-relevant-vowel-acc} shows the error rates for each of these predicted confusions, under each level of socio-indexical grouping. Despite the high level of variabilty, knowing a dialect does help reduce at least some dialect-specific errors (e.g., `ao` raising to `uh` in the Mid-Atlantic dialect and, in the North dialect, the `ao`-`aa` non-merger, and `ae` raising to `eh` (although this is variable across the eight talkers in the dialect group and does not reach significance).

Second, the errorbars in Figures {@fig:overall-accuracy-group-known} and {@fig:by-vowel-acc-group-known} plot variability across talkers, independently for each grouping level. But for assessing the utility of dialect, a more appropriate measure of uncertainty is the variability of the within-talker _difference_ between accuracy with and without considering dialect. If talkers vary in their baseline level of intelligibility, then this within-subject difference may be less variable.  Figure {@fig:dialect-over-marginal-sex} plots the average (and 95% bootstrapped CIs) within-talker increase in correct recognition of each vowel for knowing each talkers dialect. Two different baselines are used: the marginal cue distributions, and the sex-specific distributions (compared with dialect+sex).  This suggests that there may be weak effects for `ey`, `ae`, `eh`, and `uh`, but it is hard to draw firm conclusions for two reasons. First, correct recognition is at ceiling for many talkers/vowels. Second, the training data subsampling procedure that is necessary to even allow for comparison across different levels of grouping introduces additional variance that's not directly accounted for here (even though it is indirectly accounted for by bootstrapping over talkers, each of whose models was trained on a different random subsample of talkers).

```{r dialect-over-marginal-sex, fig.width=8.9, fig.height=3.7}

dialect_advantage <-
  vowel_accuracy %>%
  filter(grouping != 'Talker') %>%
  group_by(cues, subsample_size, group_is, Talker, Vowel, grouping) %>%
  summarise(proportion = mean(accuracy),
            logodds = log(sum(accuracy)+0.5) - log(sum(!accuracy)+0.5)) %>%
  gather('measure', 'accuracy', proportion, logodds) %>%
  group_by(measure, add=TRUE) %>%
  ## select(-Dialect, -Marginal, -Sex, -Dialect_Sex, -group) %>%
  spread(grouping, accuracy) %>%
  transmute(Dialect_over_marginal = Dialect - Marginal,
            Dialect_over_sex = Dialect_Sex - Sex) %>%
  left_join(nsp_vows %>% group_by(Talker, Dialect, Sex) %>% summarise())

dialect_advantage_boot_ci <- dialect_advantage %>%
  filter(subsample_size == 3, group_is == 'Known') %>%
  gather('comparison', 'value', Dialect_over_marginal, Dialect_over_sex) %>%
  group_by(cues, Vowel, measure, comparison) %>%
  do({ boot_ci(., function(d,i) mean(d$value[i]), h0=0) })

dialect_advantage %>%
  filter(subsample_size == 3, group_is == 'Known') %>%
  gather('comparison', 'value', Dialect_over_marginal, Dialect_over_sex) %>%
  ggplot(aes(x=Vowel, y=value, color=comparison)) +
  geom_pointrange(stat='summary', fun.data=mean_cl_boot, position=pd()) +
  geom_hline(yintercept=0) +
  facet_grid(measure ~ cues, scales='free_y') +
  labs(y = 'Change in accuracy when\nconditioning on dialect')

```

## Inferring socio-indexical variables from cue distributions

```{r indexical-classification, cache=TRUE}

## have to re-train models without subsampling
vowel_index_class <-
  vowel_data_grouped %>%
  filter(! grouping %in% c('Marginal', 'Talker')) %>%
  mutate(trained = map2(data, grouping, train_models_indexical_with_holdout),
         posteriors = map(trained, classify_indexical_with_holdout)) %>%
  unnest(map2(posteriors, grouping, ~ rename_(.x, group = .y)))

vot_index_class <-
  vot_by_place_grouped %>%
  filter(! grouping %in% c('Marginal', 'Talker')) %>%
  mutate(trained = map2(data, grouping,
                        ~ train_models_indexical_with_holdout(.x,
                                                              .y,
                                                              category='voicing',
                                                              formants='vot')),
         posteriors = map(trained, classify_indexical_with_holdout)) %>%
  unnest(map2(posteriors, grouping, ~ rename_(.x, group = .y))) %>%
  group_by(cues, grouping, Talker, group, model) %>%
  aggregate_log_lhood('log_posterior') %>%
  normalize_log_probability('log_posterior')

```

```{r indexical-accuracy, cache=TRUE, dependson=c('indexical-classification')}

vowel_index_acc <-
  vowel_index_class %>%
  filter(as.logical(posterior_choice)) %>%
  mutate(accuracy = group == model)

vot_index_acc <-
  vot_index_class %>%
  filter(posterior_choice) %>%
  mutate(accuracy = group == model)

index_acc <-
  bind_rows(vowel_index_acc %>% mutate(contrast = 'Vowel'),
            vot_index_acc %>% mutate(contrast = 'Stop voicing')) %>%
  mutate(grouping = factor(grouping, levels = grouping_levels))

## compute chance accuracy
index_chance_acc <-
  bind_rows(vowel_index_class, vot_index_class) %>%
  mutate(grouping = factor(grouping, levels = grouping_levels)) %>%
  group_by(cues, grouping, group) %>%
  summarise() %>%
  summarise(chance = 1/n())

## bootstrapped CIs
index_acc_summary <-
  index_acc %>%
  left_join(index_chance_acc) %>%
  group_by(cues, grouping) %>%
  do({ boot_ci(.$accuracy, function(d,i) mean(d[i]), h0=.$chance[1]) })

## exact binomial CIs (kosher because talker accs are single binary observations)
binomial_ci = function(p, x) qbeta(p, sum(x)+0.5, length(x)-sum(x)+0.5)
index_acc_summary_exact <-
  index_acc %>%
  left_join(index_chance_acc) %>%
  group_by(contrast, cues, grouping) %>%
  summarise_each(funs(mean,
                      ci_low=binomial_ci(0.025, .),
                      ci_high=binomial_ci(0.975, .)),
                 accuracy)

```

```{r indexical-acc-plot, fig.width=8.25, fig.height=3.5, fig.cap="Probability of correctly classifying a talker's socio-indexical group varies with the grouping variable, contrast, and cues. A talker is correctly classified if the overall posterior probability of their actual group given their unlabeled productions is the highest of all groups."}

index_acc_summary_exact %>%
  left_join(index_chance_acc) %>%
  ggplot(aes(x=grouping, y=mean, color=grouping)) +
  geom_point(aes(shape='Observed'), size=3) +
  geom_linerange(aes(ymin=ci_low, ymax=ci_high)) +
  geom_point(aes(y=chance, shape='Chance'), size=3) +
  facet_grid(.~contrast + cues, scales='free_x', space='free_x') +
  ylim(c(0,1)) +
  scale_shape_manual('', values = c(1, 16), breaks = c('Chance', 'Observed')) +
  labs(x = 'Grouping',
       y = 'Probability of correct classification\n(Socio-indexical group)') +
  theme(axis.text.x = element_text(angle=45, hjust=1))

```

For most groupings, it is possible to infer each talker's group with above chance accuracy, given only that talker's unlabeled observed cues (Figure {@fig:indexical-acc-plot}; all $p<0.01$, except for inferring a talker's sex based on their VOT distributions, and dialect from un-normalized F1xF2 distributions, both $p>0.15$).

In some respects, these results mirror the patterns of informativity about the cue distributions themselves (Figure {@fig:vowel-vot-kl-plot}). Vowel formant distributions vary more according to group than do stop VOT distributions, and likewise socio-indexical group can, on the whole, be inferred with higher accuracy based on vowel formants than on VOT. For vowels specifically, much of the variability across talkers is driven by sex differences, and this is the grouping variable that's easiest to infer of the three we tested.

However, in other respects, these results do _not_ simply mirror informativity. For instance, un-normalized F1xF2 distributions diverge from marginal more for dialect+sex than they do for sex alone, but accuracy at inferring a talker's sex is nearly at ceiling, while accuracy is barely above chance for inferring a talker's dialect+sex. Likewise, normalized F1xF2 distributions given dialect and dialect+sex diverge from marginal less than non-normalized, but accuracy at inferring these two grouping variables is higher for normalized than non-normalized.

Why the discrepancy? The informativity measure we used was the average divergence of _each category's_ cue distribution. For inferring indexical groups, we assumed that listeners do not know the intended category of each observation, and so the relevant distributions are the _marginal_ distribution, which is a _mixture_ of the category-specific ones. Even if there are some categories whose individual distributions diverge across groups, the overall mixture distribution of all categories may still be too similar to allow for the group to be reliably inferred.

## Joint inference of categories and groups

The fact that it's possible to infer many grouping variables from unlabeled observations suggests that these grouping variables can be helpful even when they're not known to listeners a priori. For instance, knowing a talker's sex substantially improves vowel recognition, and sex can be inferred from vowel productions even without knowing the category of each vowel. To evaluate this, we computed the joint posterior over categories and group, and then marginalized (averaged) over groups in order to compare with the known-group accuracy.

```{r group-inferred-accuracy}

vowel_accuracy_marginal <-
  vowel_class_all %>%
  filter(group_is == 'Inferred',
         subsample_size == 3) %>%
  get_accuracy('Vowel', method=acc_method)

vot_accuracy_marginal <-
  vot_class_all %>%
  filter(group_is == 'Inferred') %>%
  get_accuracy('voicing', method=acc_method)

accuracy_marginal <-
  vowel_accuracy_marginal %>%
  select(-Age) %>%
  mutate(contrast = 'Vowels') %>%
  bind_rows(vot_accuracy_marginal %>% mutate(contrast = 'Stop voicing')) %>%
  filter(grouping != 'Marginal') %>%
  ungroup() %>%
  mutate(grouping = factor(grouping, levels=grouping_levels))

accuracy_marginal_summary <-
  accuracy_marginal %>%
  group_by(contrast, cues, grouping, group_is, Talker) %>%
  summarise(accuracy = mean(accuracy)) %>%
  do({ mean_cl_boot(.$accuracy) }) %>%
  rename(accuracy = y,
         accuracy_lo = ymin,
         accuracy_hi = ymax)

```

```{r group-inferred-acc-plot, fig.width=8.3, fig.height=4.2}

accuracy_marginal_summary %>%
  bind_rows(accuracy_summary) %>%
  ggplot(aes(x = grouping, y=accuracy,
             color = grouping, alpha = group_is)) +
  ## geom_point(data = accuracy %>%
  ##              group_by(grouping, Talker, cues, contrast) %>%
  ##              summarise(accuracy = mean(accuracy)),
  ##            position='jitter', alpha=0.2) +
  geom_pointrange(aes(ymin=accuracy_lo, ymax=accuracy_hi), stat='identity',
                  position = pd()) +
  facet_grid(.~contrast+cues, scales='free_x', space='free_x') +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  labs(x = 'Grouping (broadest to most specific)',
       y = 'Probability of correct recognition') +
  lims(y = c(0, 1)) +
  scale_alpha_manual(values = c(1, 0.4))

```

```{r group-inferred-vowel-acc-plot}

acc_marginal_by_vowel_summary <-
  vowel_accuracy_marginal %>%
  group_by(cues, grouping, group_is, Vowel, Talker) %>%
  summarise(accuracy = mean(accuracy)) %>%
  do({ mean_cl_boot(.$accuracy) }) %>%
  rename(accuracy = y,
         accuracy_lo = ymin,
         accuracy_hi = ymax)


acc_marginal_by_vowel_summary %>%
  filter(grouping != 'Marginal') %>%
  bind_rows(acc_by_vowel_summary) %>%
  ungroup() %>%
  mutate(grouping = factor(grouping, levels=grouping_levels)) %>%
  ggplot(aes(x=Vowel, y=accuracy, color=grouping, alpha=group_is)) +
  geom_pointrange(aes(ymin=accuracy_lo, ymax=accuracy_hi),
                  position = position_dodge(w=0.7)) +
  lims(y = c(0, 1)) +
  scale_alpha_manual(values = c(1, 0.4)) +
  facet_grid(cues~.)


```

```{r vowel-confusion-mats, eval=FALSE}

vowel_class_all %>%
  filter(subsample_size == 3) %>%
  ungroup() %>%
  select(group_is, Vowel, category_model, cues, grouping, Talker, Token,
         posterior_choice) %>%
  spread(group_is, posterior_choice) %>%
  mutate(posterior = Inferred-Known) %>%
  group_by(cues, grouping, Vowel, category_model) %>%
  summarise(p = mean(posterior)) %>%
  mutate(category_model=factor(category_model, levels=levels(Vowel))) %>%
  ggplot(aes(x=category_model, y=Vowel, fill=p)) +
  geom_tile() +
  facet_grid(cues ~ grouping) +
  scale_fill_gradient2()

vowel_accuracy_marginal %>%
  ungroup() %>%
  select(cues, grouping, Talker, Vowel, Token, group, category_model) %>%
  left_join(vowel_accuracy,
            by=c('cues', 'grouping', 'Talker', 'Vowel', 'Token', 'group'),
            suffix=c('.inferred', '.known')) %>%
  group_by(cues, grouping, category_model.inferred, category_model.known) %>%
  tally() %>%
  ggplot(aes(x=category_model.inferred, y=category_model.known, fill=n)) +
  geom_tile() +
  facet_grid(cues ~ grouping)

```


# Discussion

* Summary of results
    * on the whole, socio-indexical variables are _informative_ about linguistic cue distribution, _useful_ for improving speech recognition, and themselves can be inferred on the basis of acoustic-phonetic cues alone.
    * these variables differ in the extent to which this is true, both within a single phonetic contrast/set of cues, and between different contrasts/cues.



## What to track?

* Major question that ideal adapter raises is _what_ linguistic, socio-indexical, and acoustic/phonetic variables listeners are learning distributions for.
    * Doesn't directly answer this, per se, but rather provides a set of tools for thinking about it.
    * When combined with data like we have here, formulate hypotheses about what listeners _should_ do.
* Generally: don't want to track _everything_. Listeners need only track distributions of variables that are informative.
    * Applies at level of phonetic categories: don't track VOT given vowel place.
    * But also applies at the level of socio-indexical groupings.
    * Can actually _hurt_ you to track cue distributions at a level that's not informative (bias-variance tradeoff).
* However, informativity goes beyond better _speech_ recognition.
    * Listeners get a lot of non-linguistic information from speech signal.
    * Have to consider the _goals_ of speech perception, which go beyond just recognizing phonetic categories.
    * Sociolinguistics recognizes that the communication of social information is equally or more important in many cases.
    * Groupings that are socially meaningful can thus be informative and justify being tracked, even if ignoring them has a negligible effect on speech recognition, as long as the corresponding cue distributions carry some information about relevant social variables. 
    * Dialect is a good case: overall, ignoring dialect doesn't have huge consequences on recognition accuracy. But it can be inferred (at least above chance) based on vowel F1 and F2.
    * (( MAYBE )) And the vowels that are most informative about dialec group are not ones whose recognition suffers greatly when dialect is ignored.
    
## Consequences for adapting to unfamiliar talkers

* Adaptation/recalibration varies substantially across contrasts and cues, especially when it comes to generalization across talkers. The ideal adapter provides, in principle, and explanation for these differences: <explain>
* (( OR: ...provides a quantitative framework for explaining? for addressing these differences? ))
* The analysis here provides some evidence that this explanation is correct. When there's no differences across talkers, listeners benefit from assuming that talkers' distributions are the same, until they get evidence to the contrary.

TODO: work this in:

> The structure of how talkers actually vary in their use of VOT and formants can thus serve as a _prior_ on whether to group observed cue values from one talker with those from another talker. There's a tradeoff in adapting to an unfamiliar talker: On the one hand, the cues produced by a talker are the most informative observations about the cues that talker will produce in the future. On the other hand, listeners often have very few tokens from an unfamiliar talker themselves, and estimates of an underlying distribution from a small number of observations are either highly uncertain or noisy and more likely to be wrong. Thus, in cases where cue distributions are similar across talkers, listeners can benefit by treating those observations as if they came from the same underlying distribution. But in cases where talkers differ a lot, lumping observations from different talkers or groups together will lead to reduced comprehension accuracy and/or slowed adaptation to a talker's own cue distributions. The analyses reported here show that _both_ types of situations actually occur in variation across talkers.
> 
> Given this, it's hardly surprising that there are disagreements in the literature about whether listeners generalize what they learn from one talker to another, unfamiliar talker. When adapting to a voicing contrast, listeners are slower (which in the ideal adapter corresponds to stronger prior beliefs, based on more prior observations) and tend to lump different talkers together, requiring more evidence that two talkers produce _different_ cue distributions before they start to classify those talkers' VOT differently. For instance, @Kraljic2007 found that listeners did not recalibrate to a /d/-/t/ contrast in a talker-specific way, but listeners only heard 10 critical trials from each talker. Using a distributional learning paradigm, where listeners heard 300 trials from each talker, @Munson2011 found that listeners _did_ (eventually) learn talker-specific classification function, but that this took hundreds of trials to emerge. For contrasts that show more talker variability, there's (to my knowledge) no work on vowels themselves, but recalibration of fricative contrasts (which _do_ show similar levels of talker and gender variability; @Jongman2000; @McMurray2011a) consistently shows that listeners tend to recalibration separately to male and female talkers [among others: @Eisner2005; @Kraljic2005; @Kraljic2007; @Reinisch2014].

## Distributions vs. recognition accuracy

* Generative model: good for more than just correct/incorrect recognition accuracy. not always 100% certain, lots of uncertainty/overlap in normal speech, need to have a good model of cue distributions in order to deal with that uncertainty effectively.
    * (( evidence that people are sensitive to the distributions? ))
    * (( what does this have to do with what we've shown here...is there anywhere where these measures make opposite predictions? and where would that really matter? uncertainty maintenance is one place))

## A lower bound

A final caveat.

* Model each category as a single gaussian distribution.
* Really, each category is a _mixture_ of talker-specific distributions.
* For the purposes of _adaptation_, need to explicitly model this. But it's harder.
* And really what we're trying to get at here is how far the group-level prior _alone_ gets you. This is a __lower bound__ on the utility of group-level knowledge for dealing with an unfamiliar talker.
* Modeling each category as a single distribution of tokens provides an approximation of the predictive distribution you'd get for an _unfamiliar_ talker, where you marginalize out the talker-level models 
