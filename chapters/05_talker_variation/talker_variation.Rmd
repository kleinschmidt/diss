---
bibliography: /Users/dkleinschmidt/Documents/papers/library-clean.bib
---

```{r preamble, message=FALSE, warning=FALSE, error=FALSE}

knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE,
                      error=FALSE)

library(magrittr)
library(dplyr)
library(purrr)
library(tidyr)

library(ggplot2)
theme_set(theme_bw())

## devtools::install_bitbucket('hlplab/nspvowels')
library(nspvowels)

## devtools::install_bitbucket('hlplab/votcorpora')
library(votcorpora)

apply_groupings <- function(d, groupings) {
  groupings %>% 
    map(~ d %>% mutate(data = map(data, . %>% group_by_(.x)),
                       grouping = .x)) %>%
    reduce(bind_rows)
}

```

# Introduction

* Two traditions in speech perception: phonetics + sociolinguistics, very different approaches to _variation_.
* For phonetics is a __problem__:
    * Have to cope with talker variability/lack of invariance.
* For sociolinguistics, its a __rich source of social information__.
    * The particular variety of the language that you speak says a lot about who you are as a person.
* In recent work, these two approaches have converged.
    * Both _speech recognition_ and _socio-indexical perception_ can be thought of as processes of statistical inference. 
    * Basic idea: how well does each hypothesis explain observed data?
    * For speech recognition: hypotheses are linguistic units (e.g., phonetic categories), observations are acoustic cue values. Inference depends on knowing the _cue distribution_ for each category.
    * In this way of thinking, talker variability means that speech recognition is a _hierarchical_ inference process: talker variability means that the cue distributions change from situation to situation, and so successful speech recognition, requires that listeners infer what the appropriate cue distributions are for each linguistic category in the current situation.
    * For speech recognition, one prediction: listeners benefit from anything that is informative about cue distributions.
    * One thing we know from sociolinguistics is that socio-indexical variables---like age, gender, ethnicity, native language background, social class, region of origin, etc.---provide information about the cue distributions for many different phonetic categories. Hence, ideal adapter predicts that listeners should track group-level cue distributions, for categories where that grouping variable is informative about those distributions.
    * For social perception: knowledge of such group-level distributions is precisely what listeners need (in this way of considering the problem) in order to _infer_ socio-indexical variables. Recall that inference relies on evaluating how well each possible hypothesis explains observed data.
* Raises a critical question: _how_ informative are socio-indexical variables like age, sex, dialect, etc.?
    * ...about underlying cue distributions?
    * ...about phonetic categories?
    * ...about socio-indexical variables themselves?
* Most of the sociolinguistic work has been _descriptive_, and to the extent that it addresses any of these goals, doesn't quantify these factors.
* Use an ideal observer approach to quantitatively assess.
* ((meh)) Tell us something about what sorts of conditional distributions listeners should be tracking (if they're optimal), and lay the groundwork for future empirical work.

## Background

* Know that the amount and structure of talker variability differs between cues and phonetic categories.
    * Vowels/fricatives have a lot, much of it conditioned on sex [@Peterson1952; @Hillenbrand1995; @Jongman2000; @McMurray2011a; @Newman2001]. For vowels this can be largely [but not entirely; @Bladon1984; @Johnson2005; @Johnson2006] attributed to differences in vocal tract length, but not so for fricatives.
    * Stop voicing, less talker variation, and little systematic effect of sex [@Allen2003; @Lisker1964; @Chodroff2015]
    * Stylistic variation in both though. Dialect for vowels [@Clopper2005; @LabovDDDD], and dialect/native language background for voicing [French-English bilinguals, @Caramazza1973; @Flege1987; @Pineda2010; @Sumner2011; Scottish English, @Docherty2011].
* We also know that listeners are sensitive to (real or believed) socio-indexical groups:
    * @Niedzielski1999: say that talker is canadian, hear more canadian raising
    * gender stereotypicality and visual cues affect perception of vowels and fricatives [@Strand1999;@Johnson1999;@Strand1996]
* Listeners can infer socio-indexical variables but it's not clear what linguistic variables they use.
    * Dialect classification [@Clopper2006,@Clopper2007]
    * [cf. @Thomas2002 for more references]

## Goals

So the _existence_ of group differences is established, as is listeners' _sensitivity_ to them. So, qualitatively, the predictions of the ideal adapter hold up. But we don't have the _quantiative_ measurements we need to test these predictions in more detail. There are three major goals of this paper:

1. Quantify how _informative_ socio-indexical groups are about phonetic cue distributions.
2. Quantify the _utility_ of socio-indexical groups for accurate speech recognition.
3. Show how listeners can make inferences about socio-indexical variables in the same way as linguistic variables, based on cue distributions.

We address these goals at different levels of socio-indexical grouping, and for two different sets of phonetic cues/contrasts. Comparing multiple contrasts is a first step in refining the predictions of the ideal adapter framework about what distributions listeners should track.

# Methods

## Datasets

We analyze speech from two corpora.

### Vowels

```{r nsp-data}

nsp_vows <- nspvowels::nsp_vows %>%
  ungroup() %>%
  mutate(Marginal='all', Dialect_Sex = paste(Sex, Dialect, sep='_'))

nsp_vows_lob <- nsp_vows %>%
  group_by(Talker) %>%
  mutate_each(funs(. %>% scale() %>% as.numeric()), F1:F2) %>%
  ungroup()

## check normalization
nsp_vows_lob %>%
  gather(formant, value, F1:F2) %>%
  group_by(Talker, formant) %>%
  summarise_each(funs(mean, sd), value) %$%
  assert_that(all.equal(mean, rep(0, length(mean))),
              all.equal(sd, rep(1, length(sd))))

vowel_data <- data_frame(dataset = c('Un-normalized', 'Lobanov Normalized'),
                         data = list(nsp_vows, nsp_vows_lob))
vowel_groupings <- c('Marginal', 'Sex', 'Dialect', 'Dialect_Sex', 'Talker')
vowel_data_grouped <- apply_groupings(vowel_data, vowel_groupings)

token_per_vow <- nsp_vows %>% group_by(Talker, Vowel) %>% tally() %$% mean(n)
n_talkers <- nsp_vows %>% group_by(Talker) %>% summarise() %>% tally()

n_per_dialect_sex <- nsp_vows %>% group_by(Dialect, Sex, Talker) %>% summarise() %>% tally() %$% unique(n)
n_dialect <- nsp_vows %$% Dialect %>% unique() %>% length()

```

For vowels, we used data from the Nationwide Speech Project [@Clopper2005]. Specifically, we analyzed first and second formants (F1 and F2) recorded at vowel midpoints in isolated, read "hVd" words. This corpus contains `r n_talkers` talkers, `r n_per_dialect_sex` male and female from each of `r n_dialect` regional varieties of English: North, New England, Midland, Mid-Atlantic, South, and West [see map in @Clopper2005; regions based on @Labov2005].  Each talker provided approximately `r round(token_per_vow, 1)` repetitions of each of 11 English monophthong vowels (plus "ey"), for a total of `r nrow(nsp_vows)` observations.

Because much of the variability in talkers is due to overall differences in formant frequencies, we also used Lobanov-normalized formant frequencies as input, in addition to the un-normalized formant frequencies. Lobanov normalization z-scores F1 and F2 separately for each talker [@Lobanov1971].

### Stop voicing

```{r vot-data}

vot <-
  votcorpora::vot %>%
  filter(source == 'buckeye') %>%
  rename(Talker = subject,
         Sex = sex,
         Age = age_group) %>%
  group_by(phoneme, word, Talker) %>%
  mutate(Token = row_number()) %>%
  ungroup() %>%
  mutate(Marginal = 'all')

vot_by_place <-
  vot %>%
  group_by(place) %>%
  nest()

vot_groupings <- c('Marginal', 'Sex', 'Age', 'Talker')
vot_by_place_grouped <- apply_groupings(vot_by_place, vot_groupings)

n_vot_talkers <- vot %>% group_by(Talker) %>% summarise() %>% nrow()
n_vot_per_talker <- vot %>% group_by(phoneme, voicing, place, Talker) %>% tally()

```

We analyzed data on word-initial stop consonant voicing in conversational speech from the Buckeye corpus [@Pitt2007]. Voice onset time (VOT) was automatically extracted for `r nrow(vot)` word initial stops, `r vot %>% filter(voicing=='voiced') %>% nrow()` voiced and `r vot %>% filter(voicing=='voiceless') %>% nrow()` voiceless, for labial, coronal, and dorsal places of articulation (Wedel, _in prep_). Data came from `r n_vot_talkers` talkers, who were balanced male and female and younger/older than 40 years. On average, each talker produced `r round(mean(n_vot_per_talker$n))` tokens for each phoneme (range of `r min(n_vot_per_talker$n)` -- `r max(n_vot_per_talker$n)`).

## Modeling

Each phonetic category was modeled as a normal distribution: stop voicing as univariate distributions of VOT, and vowels as bivariate distributions of F1 and F2. We used the maximum likelihood estimators for the model parameters, which are the sample mean and covariance matrix.

For each socio-indexical grouping level, we trained separate models for each phonetic category based on all tokens from that category and grouping level (holding out test data when necessary, see below). The grouping levels we considered were

* Marginal (all tokens)
* Sex (male/female)
* Age (younger/older than 40, VOT only)
* Dialect (six regions, vowels only)
* Diaelct+Sex (12 levels, vowels only)
* Talker

### Comparing cue distributions

In order to evalute the _informativity_ of socio-indexical variables with respect to cue distributions themselves, we use Kullback-Leibler (KL) divergence to measure how much the group-specific cue distributions differ from the overall (marginal) cue distributions. This measures the expected cost (in bits of extra message length) of encoding data from each group using a code that's optimized for the marginal distribution.
We do this separately for each linguistic category and group, and then average the results, calculating bootstrapped confidence intervals over groups.

The KL divergence of $Q$ from $P$ is $DL(Q||P) = \int p(x) \log \frac{p(x)}{q(x)} \mathrm{d}x$ (with density functions $q$ and $p$ respectively). In our case, $P=\mathcal{N}_G$ is a multivariate[^multivariate] normal cue distribution for the group, with mean $\mu_G$ and covariance $\Sigma_G$, while $Q=\mathcal{N}_M$ is the marginal multivariate normal cue distribution with mean $\mu_M$ and covariance $\Sigma_M$. With some simplification[^gaus-kl], the KL divergence of the marginal from the group distribution works out to be
$$
DL(\mathcal{N}_M || \mathcal{N}_G) = \frac{1}{2} \left( \mathrm{tr}(\Sigma_M^{-1} \Sigma_G) + (\mu_M - \mu_G) \Sigma_M^{-1} (\mu_M - \mu_G) - d + \log\frac{|\Sigma_M|}{|\Sigma_G|} \right)
$$
where $d$ is the dimensionality of the distribution.

[^multivariate]: The math is the same for the univariate special case, as with VOT.
[^gaus-kl]: See, for instance, [http://stanford.edu/~jduchi/projects/general_notes.pdf](), p. 13.

### Speech recognition

Next, to address the _utility_ of socio-indexical grouping for speech recognition, we calculate, for each level of socio-indexical grouping, the probability of correct recognition of phonetic categories. We do this using an "ideal listener" model [@Clayards2008; @Kleinschmidt2015] that compute the posterior probability of a category given an observed cue value based on the likelihood of that cue being generated by each category's cue distribution.

We want to determine the phonetic category $v_i$[^notation] of each of the cues $x_i$ produced by a talker. If we assume that the listener knows that this talker belongs to group $g=j$, this inference is a straightforward application of Bayes Rule:
$$
p(v_i | x_i, g=j) \propto p(x_i | v_i, g=j) p(v_i)
$$
If, on the other hand, the listener does not know which group the talker belongs to, they have to marginalize out group. This ammounts to taking a weighted average of the posterior probabilities under each group, weighted by the probability that the talker belongs to that group, $p(g | x)$ (which we compute below):
$$
p(v_i | x_i) = \sum_j p(v_i | x_i, g=j) p(g=j | x)
$$
(where $x$ refers to all the tokens produced by this talker).

For vowels, we classified vowel categories directly. For voicing, the only cue available is VOT, which does not (reliably) distinguish place of articulation. Thus, we classified voicing separately for each place of articulation, and then average the resulting accuracy.

### Indexical group recognition

Finally, to address much cue distributions tell listeners about socio-indexical variables themselves, we classify each talker's socio-indexical group (at each level). We do this using an analogou ideal observer model. That is, we compute the posterior probability of each socio-indexical group $g=j$, given all of the talker's observed cue values $x$:
$$
p(g | x) \propto p(x | g) p(g) = \left(\prod_i p(x_i | g) \right) p(g)
$$

The only complication is that, without knowing the the phonetic category of each observation a priori, each observation may have been generated by any of the phonetic categories. Thus, to determine the _overall_ likelihood of observing a cue value $x_i$ under group $g$, we first have to marginalize over categories $v_i$:
$$
p(x_i | g) = \sum_k p(x_i | v_i=k, g) p(v_i=k | g)
$$

For all of these classifications, we assume a flat prior on categories/groups.

[^notation]: $x_i$ refers to a single observed cue value (possibly multidimensional, in the case of vowel formants), and $x$ (without subscript) refers to a _vector_ of multiple observations (from a single talker, unless otherwise specified). $v_i$ refers to observation $i$'s category, and $g$ to a talker's group. $\sum_j$ refer to a sum over all possible values of $j$.

### Controls

For classification, if test data is included in the training set, this artificially inflates accuracy at test [@James2013, Section 5.1].  Cross-validation controls for this by splitting data into training and test sets.  For group-level models (sex, age, dialect, and dialect+sex), we use leave-one-talker-out cross-validation: train each group's models with test talker's observations held out. For the talker-specific models, we use 6-fold cross-validation (or leave-one-out when there were fewer than 6 tokens in a category for a talker), where each phonetic category is split into 6 approximately equal subsets. Then, one subset of each category is selected for test, the models are trained on the remaining five, and the test data is classified as above.

For the vowel data, the different levels of grouping have very different group sizes, and this requires some caution. The broadest (sex) has 24 talkers per group (23 after holdout), while the most specific (dialect+sex) has only 4 (3 after holdout).  This introduces a systematic bias in favor of broader groupings, because small sample sizes lead to noisier estimates of the underlying model, and hence lower accuracy (on average) at test [@James2013, Section 2.2.2]. To correct for this, in addition to leave-one-out validation, we randomly subsampled each group to be the same size when training models. We use a different random subsample for each talker's training set, with two group sizes: 3 talkers per group (corresponding to the Dialect+Sex grouping) and 7 talkers per group (corresponding to Dialect). These underestimate the true group-level accuracy,  but provide a useful lower bound that allows accuracy to be compared across grouping levels.

# Results

# Discussion

## A lower bound

* Model each category as a single gaussian distribution.
* Really, each category is a _mixture_ of talker-specific distributions.
* For the purposes of _adaptation_, need to explicitly model this. But it's harder.
* And really what we're trying to get at here is how far the group-level prior _alone_ gets you. This is a __lower bound__ on the utility of group-level knowledge for dealing with an unfamiliar talker.
* Modeling each category as a single distribution of tokens provides an approximation of the predictive distribution you'd get for an _unfamiliar_ talker, where you marginalize out the talker-level models 

## What to track?

* Major question that ideal adapter raises is _what_ linguistic, socio-indexical, and acoustic/phonetic variables listeners are learning distributions for.
    * Doesn't directly answer this, per se, but rather provides a set of tools for thinking about it.
    * When combined with data like we have here, formulate hypotheses about what listeners _should_ do.
* Generally: don't want to track _everything_. Listeners need only track distributions of variables that are informative.
    * Applies at level of phonetic categories: don't track VOT given vowel place.
    * But also applies at the level of socio-indexical groupings.
    * Can actually _hurt_ you to track cue distributions at a level that's not informative (bias-variance tradeoff).
* However, informativity goes beyond better _speech_ recognition.
    * Listeners get a lot of non-linguistic information from speech signal.
    * Have to consider the _goals_ of speech perception, which go beyond just recognizing phonetic categories.
    * Sociolinguistics recognizes that the communication of social information is equally or more important in many cases.
    * Groupings that are socially meaningful can thus be informative and justify being tracked, even if ignoring them has a negligible effect on speech recognition, as long as the corresponding cue distributions carry some information about relevant social variables. 
    * Dialect is a good case: overall, ignoring dialect doesn't have huge consequences on recognition accuracy. But it can be inferred (at least above chance) based on vowel F1 and F2.
    * (( MAYBE )) And the vowels that are most informative about dialec group are not ones whose recognition suffers greatly when dialect is ignored.
    
## Consequences for adapting to unfamiliar talkers

* Adaptation/recalibration varies substantially across contrasts and cues, especially when it comes to generalization across talkers. The ideal adapter provides, in principle, and explanation for these differences: <explain>
* (( OR: ...provides a quantitative framework for explaining? for addressing these differences? ))
* The analysis here provides some evidence that this explanation is correct. When there's no differences across talkers, listeners benefit from assuming that talkers' distributions are the same, until they get evidence to the contrary.

TODO: work this in:
    > The structure of how talkers actually vary in their use of VOT and formants can thus serve as a _prior_ on whether to group observed cue values from one talker with those from another talker. There's a tradeoff in adapting to an unfamiliar talker: On the one hand, the cues produced by a talker are the most informative observations about the cues that talker will produce in the future. On the other hand, listeners often have very few tokens from an unfamiliar talker themselves, and estimates of an underlying distribution from a small number of observations are either highly uncertain or noisy and more likely to be wrong. Thus, in cases where cue distributions are similar across talkers, listeners can benefit by treating those observations as if they came from the same underlying distribution. But in cases where talkers differ a lot, lumping observations from different talkers or groups together will lead to reduced comprehension accuracy and/or slowed adaptation to a talker's own cue distributions. The analyses reported here show that _both_ types of situations actually occur in variation across talkers.
    > 
    > Given this, it's hardly surprising that there are disagreements in the literature about whether listeners generalize what they learn from one talker to another, unfamiliar talker. When adapting to a voicing contrast, listeners are slower (which in the ideal adapter corresponds to stronger prior beliefs, based on more prior observations) and tend to lump different talkers together, requiring more evidence that two talkers produce _different_ cue distributions before they start to classify those talkers' VOT differently. For instance, @Kraljic2007 found that listeners did not recalibrate to a /d/-/t/ contrast in a talker-specific way, but listeners only heard 10 critical trials from each talker. Using a distributional learning paradigm, where listeners heard 300 trials from each talker, @Munson2011 found that listeners _did_ (eventually) learn talker-specific classification function, but that this took hundreds of trials to emerge. For contrasts that show more talker variability, there's (to my knowledge) no work on vowels themselves, but recalibration of fricative contrasts (which _do_ show similar levels of talker and gender variability; @Jongman2000; @McMurray2011a) consistently shows that listeners tend to recalibration separately to male and female talkers [among others: @Eisner2005; @Kraljic2005; @Kraljic2007; @Reinisch2014].

## Distributions vs. recognition accuracy

