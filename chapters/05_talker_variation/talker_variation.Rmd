---
bibliography: /Users/dkleinschmidt/Documents/papers/library-clean.bib
output:
    html_document:
        code_folding: hide
        dev: png
        keep_md: true
        md_extensions: +implicit_figures
        pandoc_args:
        - --filter
        - pandoc-fignos
        - --filter
        - pandoc-tablenos
        - --filter
        - pandoc-eqnos
    pdf_document:
        md_extensions: +implicit_figures
        keep_tex: true
        pandoc_args:
        - --filter
        - pandoc-fignos
        - --filter
        - pandoc-tablenos
        - --filter
        - pandoc-eqnos
---

```{r preamble, message=FALSE, warning=FALSE, error=FALSE, echo=FALSE, results='hide'}

library(knitr)
opts_chunk$set(message=FALSE,
               warning=FALSE,
               error=FALSE,
               echo=opts_knit$get("rmarkdown.pandoc.to") != 'latex',
               cache=TRUE)

## Produce markdown-formatted figures so that pandoc knows what to do with
## the captions. requires pandoc-fignos to parse the IDs. refer to figures
## in text with {@fig:label} or just @fig:label
## 
## (see https://github.com/tomduck/pandoc-fignos)
knit_hooks$set(plot = function(x, options) {
  paste0('![', options$fig.cap, ']',
         '(', opts_knit$get('base.url'), paste(x, collapse='.'), ')',
         '{#fig:', options$label, '}')
})
## Produce markdown-formatted table captions with anchors for cross-refs.
## Requires pandoc-tablenos to parse the IDs. Refer to tables
## in text with {@tbl:label} or @tbl:label.
## Based partly on http://stackoverflow.com/a/18672268
##
## (see https://github.com/tomduck/pandoc-tablenos)
knit_hooks$set(tbl.cap = function(before, options, envir) {
  if(!before){
    paste0('\n\nTable: ', options$tbl.cap,
           ' {#tbl:', options$label, '}', sep = '')
  }
})

library(magrittr)
library(dplyr)
library(purrr)
library(tidyr)
library(stringr)

library(svglite)
library(ggplot2)
library(cowplot)
## theme_set(theme_bw())

## cowplot theme + y axis gridlines
theme_set(theme_cowplot() %+replace%
            theme(panel.grid.major = element_line(colour='gray90', size=0.2),
                  panel.grid.minor = element_line(colour='gray98', size=0.5),
                  panel.grid.major.x = element_blank(),
                  panel.grid.minor.x = element_blank()))
rotate_x_axis_labs <- function(by=45) theme(axis.text.x = element_text(angle=by, hjust=1))


## devtools::install_github('kleinschmidt/daver')
library(daver)

## devtools::install_bitbucket('hlplab/nspvowels')
library(nspvowels)

## devtools::install_bitbucket('hlplab/votcorpora')
library(votcorpora)

apply_groupings <- function(d, groupings) {
  groupings %>% 
    map(~ d %>% mutate(data = map(data, . %>% group_by_(.x)),
                       grouping = .x)) %>%
    reduce(bind_rows)
}

grouping_levels <- c('Marginal', 'Dialect', 'Age', 'Sex', 'Dialect_Sex', 'Talker')

```

# Quantifying the role of socio-indexical structure in talker variation {#chap:talker-variation}

Variability is one of the defining features of speech.
There are two broad traditions in the study of speech, with
 very different approaches to the role of _variability_ in speech. On the one
hand, for the cognitive/psycholinguistic tradition, variability is one of the central _problems_ of speech perception, a challenge
that listeners must cope with. In this view, variability is such a severe
problem that it is traditionally referred to only indirectly, as the "lack of
invariance" [@Liberman1967]. The sociolinguistic tradition, on the other hand,
views variability as a rich source of social information. The particular variety
of language that you speak says a lot about who you are as a person, and sociolinguistics takes this sort of variation as one of its primary objects of study [@Labov1972; @Eckert1989].

These two approaches have recently begun to converge in how they
approach variability. In particular, psycholinguistic theories of speech
perception have started to investigate the consequences of variability for comprehension.
<!-- something about exemplar theories would be appropriate here --> In part
this realization comes from computational-level analyses of speech perception
[@Clayards2008; @Feldman2009a; @Feldman2013a; @Norris2008; @Kleinschmidt2015]. These
approaches start from the hypothesis that the speech perception system is
organized in order to be good at speech perception in the world that it has to
operate in. In the spirit of ideal observer approaches to other domains
[like visual perception, @Marr1982; or memory, @Anderson1990; @Anderson1991]
these approaches focus on spelling out how the nature of the task, the available
information, and the structure of the world constrain, in principle, how well a
listener can do.

Applied to speech perception, this approach offers two important insights.
First, it suggests that speech perception can be thought of as a process of
_inference under uncertainty_ [@Clayards2008; @Norris2008]:
each linguistic unit is realized---even by a single
talker---as a _distribution_ of acoustic cues
[cf. @Lisker1964; @Peterson1952; @Hillenbrand1995; @Allen2003; @Newman2001].
This places a fundamental constraint on speech perception: the best a listener can do is to _infer_ how likely each possible
linguistic unit is as an explanation of the cues they observe, based on their
knowledge of these cue distributions.

Second, this approach provides a new perspective on talker variability. One consequence
of talker variability is that the distribution of cues for each linguistic unit
_changes_ from one situation to the next, depending on who's talking
[@Clopper2005; @Newman2001; @Allen2003; @Johnson2005; @Foulkes2015a].
Moreover, these differences cannot be entirely reduced to constant effects of physiological differences [like vocal tract size, @Johnson2005; for review see @WeatherholtzInPress].
In this perspective, effective speech perception depends on good knowledge of the
underlying cue distributions. Because these distributions change across
situations, listeners must _also_ constantly be inferring the current talker's
linguistic generative model (the probabilistic distributions of cues they
produce for each underlying linguistic structure). This is captured by the
ideal adapter framework [@Kleinschmidt2015].

<!-- TODO: more of a "bang"; "exploring prediction which has the potential to..." -->
This second insight leads to the following prediction: an ideal adapter will
take advantage of any additional structure in the world that is informative
about how cue distributions vary from one situation to the next. This structure
may be as simple as the fact that individual talkers tend to be consistent in
the cue distributions they produce [@Heald2015; see also @WeatherholtzSubmitted2016], meaning that prior experience
with a familiar talker is informative about the cue distributions they will
produce in the future. But structure can occur at other levels, too [e.g., male talkers produce vowels with systematically lower formant frequencies than female talkers; @Peterson1952; @Hillenbrand1995]. This is where
psycholinguistic theories begin to parallel sociolinguistics.  To
the extent that variables like gender, class, regional origin, etc. are
sociolinguistically relevant, they are reliably informative about how linguistic
variables are realized, and hence helpful for speech perception.

This means that a listener can potentially learn a lot about a talker's cue
distributions just by knowing _who_ a talker is. Conversely, listeners can learn
a lot about who a talker is based on the distributions of cues that they
produce. This bi-directional relation between phonetic cues and social identity has long been recognized in sociolinguistics.
This relationship _also_ follows 
straightforwardly from the ideal adapter framework. However, the extent to which
these two types of inferences---socio-indexically-conditioned linguistic inference and linguistically-conditioned inference about social identity---are feasible _in practice_ depends on exactly how much a
particular socio-indexical variable---a particular sense of _who_ a talker
is---actually influences the cue distributions that talkers produce.

One advantage of the ideal adapter framework over previous approaches to these
issues is that it provides a set of computational and theoretical tools to
quantify this relationship. The central question we address in this paper is
twofold: 1) how much can listeners actually gain by considering socio-indexical
variables for speech perception (both in absolute terms and relative to other
grouping variables), and 2) how accurately could listeners infer socio-indexical
grouping variables for a talker on the basis of their cue distributions. This
work builds on the initial steps reported in @WeatherholtzSubmitted2016,
developing the computational tools provided by the ideal adapter framework into
a quantitative framework with the potential to unify explanations of linguistic and socio-indexical judgments.

### Our goals

At a conceptual level, the goal of this paper is to show that the idea of _inference_ provides a bridge between sociolinguistic and psycholinguistic perspectives on variation in speech. The ideal adapter framework treats both linguistic and socio-indexical judgments as inference, jointly informed by a listener's knowledge of cue distributions. In addition to this conceptual unification, the theoretical tools of this framework offer a common computational currency for developing models of linguistic and socio-indexical inference, and, critically, for grounding the parameters of those models in actual data. This common currency is the _distribution_ of cues _conditioned_ on linguistic and socio-indexical variables. These distributions can be measured, given the appropriate production data.

This brings us to the more concrete, immediate goals of this paper, which are to quantify the relationship between some prominent socio-indexical variables, linguistic variables, and acoustic-phonetic cue distributions.
Most
of the work on this
relationship has been descriptive [but see, e.g., @Clopper2005; @McMurray2011a], aimed at establishing that
differences between particular groups of talkers _exist_ in the first place, and
that listeners are sensitive to these differences at all. But the mere existence
of differences does not establish exactly how _informative_ or _useful_ such
grouping variables are for speech perception.

This paper is motivated by three interrelated questions about the relationship
between socio-indexical variables and phonetic categories/cues. We begin by outlining each of 
these questions and their motivations, before presenting a series of computational studies that address them. In developing these studies, we lay out a novel, principled, and quantitative
approach to understanding the relationship between socio-indexical variables and
speech perception.

#### Question 1: how _informative_ are socio-indexical variables about cue distributions?

According to the ideal adapter, effective adaptation to different talkers---and
hence robust speech perception---depends on knowing how and how much cue
distributions vary across talkers for each phonetic category/contrast. If a
category's cue distribution is strongly conditioned on a socio-indexical
variable like talker identity (i.e., varies a lot between talkers), then
listeners need to be prepared to rapidly adapt. If, on the other hand, a
category's cue distributions are _not_ strongly conditioned on talker, then
listeners are better off using their prior experience with _other_ talkers and
_not_ rapidly adapting their beliefs about that particular category. The same
logic applies for other socio-indexical variables.  If a category's cue
distributions are strongly conditioned on, for instance, the sex of the talker,
then a listener's previous experience with other male talkers will be highly
informative about an unfamiliar male talker's realization of that category,
while their experience with female talkers would be *mis*informative.

In principle, it is possible that the amount and structure of between-talker
variability is more or less the same across phonetic categories and cues.  In
such a world, listeners should adapt uniformly to talker variability in
different cues/contrasts, drawing---or not--- on previous experience with other
talkers in the same way across phonetic categories.  This would also mean that
every phonetic category provides an equally good substrate for communicating
socio-indexical meaning.  If, on the other hand, phonetic categories/contrasts
_vary_ in how strongly their cue distributions vary across talkers, then
listeners should adjust their adaptation to an unfamiliar talker's phonetic cue
distributions differently across categories/contrasts.
<!-- TODO: MORE here? -->

There is qualitative evidence suggesting that the structure of talker variation
is, in fact, different depending on the phonetic contrast and cues. For example,
vowels exhibit substantial variability conditioned on the gender and the
regional background of the talker
[@Peterson1952; @Hillenbrand1995; @Clopper2005; @Labov2005, among others].
Talker variation in word-initial stop voicing, on the other hand, is largely
unstructured (at least in American English), exhibiting little systematic
variation by gender, regional dialect, or age [@Allen2003; @Chodroff2015].
There is moreover qualitative evidence that the overall _level_ of cross-talker
variation is also different between these two contrasts.  For instance, visual
inspection of the results of @Chodroff2015 (Figure 1) suggests that cross-talker
variance in voiceless word-initial stop VOT is roughly _half_ of within-category
variation (standard deviations of approximately 10ms and 20ms VOT respectively),
whereas inspection of similar results for vowels from @Hillenbrand1995 (Figure
4) suggests that cross-talker variability in vowel production is approximately
_double_ the within-category variability.

However, no previous work has quantified the relative level of between-talker
vs. within-talker variability across phonetic contrasts.  In part, the lack of
quantitative comparisons reflects the fundamental difficulty of comparing very
different phonetic contrasts and cues in a principled way.  We address this here
by quantifying how _informative_ socio-indexical variables---including talker
identity---are about the distribution of acoustic-phonetic cues.  We do this for
two sets of contrasts/cues: vowels (cued by first and second formant
frequencies; F1xF2) and word-initial stop voicing (cued by voice onset time;
VOT). We selected these particular contrasts based on the qualitative evidence
that they exhibit different degrees and types of cross-talker
variability. 
Our approach is based on that of @WeatherholtzSubmitted2016, and compares
_conditional_ cue distributions (e.g., for male vs. female talkers) to
_marginal_ cue distributions (e.g., for all talkers) using an information
theoretic measure of similarity between distributions.
Focusing on information theoretic properties of the cue
distributions provides a principled common currency for comparing different
socio-indexical variables, phonetic categories, and cues.

#### Question 2: How _useful_ are socio-indexical variables for speech recognition?

<!-- TODO: awkward, revise -->

The _utility_ of a socio-indexical variable for guiding speech
recognition is not necessarily the same as its _informativity_ about the
underlying cue distributions themselves.
By the utility of a socio-indexical variable, we mean the benefit
for speech perception from 1) tracking the cue distributions for phonetic
categories _conditional_ on group (e.g., for male and female talkers) and 2)
knowing the value of that variable for a talker (e.g., whether a talker is male
or female).
Informativity provides an upper bound on utility,
since tracking conditional cue distributions can only improve recognition to the extent
that they are actually different from marginal (un-conditioned)
distributions. 

It is possible, in principle, that two equally informative
socio-indexical variables may not be equally useful for the purposes of speech
perception, and, for instance, not equally tracked by listeners.
For instance, if one
category varies across talkers in a way that never causes it to overlap with
another category, listeners can safely ignore talker-level variation in that
category without increasing the chance of errors in speech recognition. On the
other hand, when a category varies across talkers or groups in a way that _does_
cause it overlap with other categories, ignoring talker-conditioned
distributions would lead to slowed or inaccurate speech recognition. The same is
true of broader socio-indexical groupings, like sex or dialect.

We know that listeners do, at least in some cases, use socio-indexical variables
to guide speech perception. For example, perception of vowels (and
fricatives) is affected by the perceived gender of the talker, which can be cued
by voice quality, visual presentation of a male or female face, or even explicit
instruction [@Strand1999;@Johnson1999;@Strand1996].  Listeners use other
socio-indexical variables to guide recognition of vowels as well:
@Niedzielski1999 found that if listeners believe that a talker is Canadian, they
hear more Canadian raising than if they believe the talker is American. @Hay2010
found a similar sensitivity to dialect group using an even subtler manipulation,
manipulating listeners perceptions based on a stuffed animal that cued New
Zealand or Australia.  More broadly, listeners are better at comprehending
speech in noise from familiar talkers than unfamiliar talkers [@Nygaard1998].

What we do not know is whether the extent to which listeners _are_ sensitive to
socio-indexical variables for different contrasts is predictable based on the
underlying structure of talker variability for these contrasts.  This depends on
exactly how useful these socio-indexical variables would be for speech
recognition, which is the second question we address in this paper.  The ideal
adapter provides a principled way to link talker- and group-conditioned
variability in cue distributions with the utility of those variables.  This link
comes from treating speech perception as an inference process, which predicts
phonetic classification from the underlying cue distributions.  We use this link
to quantify, in Study 2, how much speech recognition is improved by conditioning
cue distributions on a particular socio-indexical variable. As in Study 1, our
approach is based on @WeatherholtzSubmitted2016, extended here to a wider range
of cues, categories, and socio-indexical variables.

One advantage of the ideal adapter framework is that quantifying the utility of
socio-indexical variables in this way leads to further, testable predictions
about the role that socio-indexical variables play in speech perception.  One
sense of utility that is of particular interest is how listeners _generalize_
from experience with one talker to another in speech recognition.  A variety of
experiments on phonetic recalibration (or perceptual learning) have shown that
listeners sometimes generalize from experience with, for instance, a male talker
to inform perception of an unfamiliar, female talker
[@Kraljic2006; @Kraljic2007; @Reinisch2014; @Munson2011], but sometimes they do
not [@Eisner2005; @Kraljic2007; @Reinisch2014].  The ideal adapter predicts that
listeners will generalize when they think that the two talkers both belong to a
group that is informative about the particular contrast being recalibrated.
Assessing the utility of different levels of grouping (including gender) is a
necessary first step towards making these predictions precise and testable

<!-- TODO: Something about normalization mirth be appropriate here: utility can depend on the input representation -->


#### Question 3: How well could listeners _infer_ socio-indexical variables given cue distributions?

The utility of tracking socio-indexically-conditioned cue distributions goes
beyond the recognition of phonetic categories.  The purpose of spoken language
is not limited to communicating linguistic information.  Every time we speak we
also communicate socio-indexical information, information about aspects of our
social identity such as gender, age, class,
etc. [cf. @Eckert1989; @Eckert2012; @Labov1972].
<!-- TODO: more refs here. --> The approach outlined so far also lends itself to
reasoning about these sorts of social inferences as well. In particular, a
listener can use knowledge about group-conditioned cue distributions in order to
evaluate which group best explains the cues they have observed from a talker,
and hence which group they are likely to belong to.[^dynamic-socio]

[^dynamic-socio]: There are arguably other, more dynamic non-linguistic aspects
    that are coded in the speech signal, such as mood or attitude
    towards our interlocutors [@Eckert2010; @Podesva2007]. The approach laid out here---at
    least in principle---extends to these aspects of communication as well.

As with the previous two questions, there is some evidence that listeners can in
fact make these sorts of socio-indexical inferences on the basis of speech alone
[for a review, see @Thomas2002].  For instance, listeners can classify talker's
regional dialect at above-chance accuracy based on a short except of their
speech [a single sentence read from a standard set @Clopper2006; @Clopper2007].
We include a talker's _identity_ as a socio-indexical variable (in the sense of
what individual person they are).  There is evidence that listeners can infer a
talker's identity from sine-wave speech [@Remez1997], speech which has been
processed to remove most non-phonetic voice quality cues to identity but
preserve most phonetic information [@Remez1981].

However, the extent to which knowledge of group-conditioned phonetic cue
distributions would actually be useful in practice depends on the actual
distribution of cues within and between groups, as does the relative usefulness
of different aspects of the speech signal. Thus our final study aims to quantify
how well listeners would be able to infer socio-indexical properties based on
phonetic cue distributions alone. We do this by treating these socio-indexical
inferences in mathematically the same way that we treat linguistic
inferences. This highlights the potential of the ideal adapter framework
as a unifying perspective on linguistic and socio-indexical inferences.

We chose to focus on socio-indexical variables that are categorical and
generally _stable_ over time for a single talker. This choice is based on
feasibility: our analysis depends on having sufficient data to estimate cue
distributions for multiple talkers, within groups defined by socio-indexical
variables. Sociolinguistics increasingly recognizes that the meanings of
socio-indexical variables are dynamically constructed and not necessarily static
within a single individual
[either producer or perceiver; @Eckert2012; @Podesva2001; @Podesva2007; @Foulkes2015a; @Levon2014].
The techniques we introduce here can---in principle---be applied to more dynamic
and contextually conditioned variables, but they present unique and interesting
challenges that are beyond the scope of the current paper.


## General methods

We next describe the datasets we use and the motivations for selecting these datasets. Then, we introduce our general modeling approach. The specific methods by which we quantify informativity, utility, and inferability of socio-indexical grouping variables are discussed in their respective studies.

### Data

Guided by our goals, we selected two datasets. The first is a collection of
vowel formants from the Nationwide Speech Project [NSP; @Clopper2005], and the
second a collection of voice onset times (VOT) for word-initial voiced and
voiceless stops from the Buckeye corpus
[@Pitt2007, extracted by Wedel, _in prep_].  We chose to compare vowels and stop
voicing because there appears to be different amounts and types of talker
variability for these contrasts.  Our specific goals further constrain our
particular choice of datasets.

First, in order to estimate the _between_ vs. _within_ talker variability, we
need data with enough talkers who each produce enough tokens of the relevant
phonetic categories in order to estimate the corresponding cue distributions.
Second, in order to estimate the _utility_ of socio-indexical variables, we need
data where each talker produces tokens from _each_ of the categories for the
contrast.  Third, to assess the influence of different socio-indexical
variables, we need data where different socio-indexical variables are actually
annotated.
These rule out many existing datasets which either do not contain enough
tokens per talker, or (especially for stop voicing) only contain tokens from a
subset of categories (e.g., only voiceless stops).

Based on the variables annotated in the available data, we consider cue
distributions conditioned on the following socio-indexical grouping variables:

* __Marginal__: is a control grouping, which includes all tokens from all
  talkers. This serves as a baseline against which more specific group
  distributions can be compared, and as a lower bound for speech recognition
  accuracy.
* __Sex__: this is coded as male/female for both vowels and stop voicing,
  allowing us to compare the role of sex for two different contrasts.
* __Age__: this is only relevant for VOT, because the talkers in the NSP are
  uniformly young. In the Buckeye corpus, age is coded as a binary variable
  (younger/older than 40, VOT only).
* __Dialect__: the NSP contains data from talkers from six dialect regions (see
  below for details).
* __Dialect+Sex__: @Clopper2005 found that sex modulates dialect differences, so
  we also examining cue distributions conditioned on dialect and sex together (12
  levels, vowels only).
* __Talker__: we also consider cue distributions conditioned on talker identity,
  as an upper bound on informativity and utility.

#### Vowels

```{r nsp-data, cache=TRUE, results='hide'}

nsp_vows <- nspvowels::nsp_vows %>%
  ungroup() %>%
  mutate(Marginal='all', Dialect_Sex = paste(Sex, Dialect, sep='_'))

nsp_vows_lob <- nsp_vows %>%
  group_by(Talker) %>%
  mutate_each(funs(. %>% scale() %>% as.numeric()), F1:F2) %>%
  ungroup()

## check normalization
nsp_vows_lob %>%
  gather(formant, value, F1:F2) %>%
  group_by(Talker, formant) %>%
  summarise_each(funs(mean, sd), value) %$%
  assert_that(all.equal(mean, rep(0, length(mean))),
              all.equal(sd, rep(1, length(sd))))

vowel_data <- data_frame(cues = c('Un-normalized F1xF2', 'Lobanov Normalized F1xF2'),
                         data = list(nsp_vows, nsp_vows_lob))
vowel_groupings <- c('Marginal', 'Sex', 'Dialect', 'Dialect_Sex', 'Talker')
vowel_data_grouped <- apply_groupings(vowel_data, vowel_groupings)

token_per_vow <- nsp_vows %>% group_by(Talker, Vowel) %>% tally() %$% mean(n)
n_talkers <- nsp_vows %>% group_by(Talker) %>% summarise() %>% tally()

n_per_dialect_sex <- nsp_vows %>% group_by(Dialect, Sex, Talker) %>% summarise() %>% tally() %$% unique(n)
n_dialect <- nsp_vows %$% Dialect %>% unique() %>% length()

```

For vowels, we used data from the Nationwide Speech Project [@Clopper2005]. Specifically, we analyzed first and second formant frequencies (F1xF2, measured in Hertz) recorded at vowel midpoints in isolated, read "hVd" words. This corpus contains `r n_talkers` talkers, `r n_per_dialect_sex` male and female from each of `r n_dialect` regional varieties of English: North, New England, Midland, Mid-Atlantic, South, and West [see map in @Clopper2005; regions based on @Labov2005].  Each talker provided approximately `r round(token_per_vow, 1)` repetitions of each of 11 English monophthong vowels (plus `ey`), for a total of `r nrow(nsp_vows)` observations.

One of our primary goals is to assess the informativity of different grouping variables. Sex differences in vocal tract size are a major source of variability in vowel production, and thus may be more informative than other factors. However, this likely depends on the details of how acoustic cues are represented. Differences in vocal tract size, for instance, lead to overall shifts in the resonant frequencies and hence formant frequencies across _all_ vowels, but leave the relative positions of vowel categories more or less intact [e.g., @Hillenbrand1995]. Moreover, there is evidence that domain-general auditory normalization or adaptation processes removes some or all of this overall shift, and hence using un-normalized formant frequencies may overestimate the informativity of sex relative to other grouping factors.

For this reason, we also used Lobanov-normalized formant frequencies as input, in addition to the un-normalized formant frequencies in Hertz. 
Lobanov normalization z-scores F1 and F2 separately for each talker
[@Lobanov1971], which effectively aligns each talker's vowel space at its center
of gravity, and scales it so they have the same size (as measured by standard
deviation). This controls for overall offset in formant frequencies caused by
varying vocal tract sizes (from both sex differences and individual variation).
It does this while preserving the structure of each talker's vowel space, so that (for instance) dialect-specific vowel shifts are maintained.

```{r vowel-data-plot, eval=FALSE}

## Plot group-level distributions of vowels. kind of a mess.

vowel_data_grouped_long <- vowel_data_grouped %>%
  mutate(group_size = map2_dbl(data, grouping, ~ .x %>%
                                                 group_by_(.y) %>%
                                                 summarise() %>%
                                                 nrow())) %>%
  unnest(map2(data, grouping, ~ rename_(.x, group=.y))) %>%
  mutate(grouping = factor(grouping, levels=grouping_levels))

p_vow_group_hz <- vowel_data_grouped_long %>%
  filter(cues == 'Un-normalized F1xF2') %>%
  ggplot(aes(x=F2,y=F1,color=Vowel)) +
  stat_ellipse(aes(group=Vowel), data = nsp_vows, type='norm', color='#888888') +
  stat_ellipse(aes(group=paste(group, Vowel), alpha=1/group_size), type='norm') +
  facet_grid(cues~grouping) +
  scale_x_reverse('F2 (Hz)') +
  scale_y_reverse('F1 (Hz)') +
  scale_alpha_continuous(range=c(0.3, 1))

p_vow_group_lob <- vowel_data_grouped_long %>%
  filter(str_detect(cues, 'Lobanov')) %>% 
  ggplot(aes(x=F2,y=F1)) +
  stat_ellipse(aes(group=Vowel), data = nsp_vows_lob, type='norm', color='#888888') +
  stat_ellipse(aes(color=Vowel, group=paste(group, Vowel), alpha=1/group_size), type='norm') +
  facet_grid(cues~grouping) +
  scale_x_reverse('F2 (Lobanov normalized)') +
  scale_y_reverse('F1 (Lobanov normalized)') +
  scale_alpha_continuous(range=c(0.3, 1)) +
  theme(legend.position = 'none')

plot_grid(p_vow_group_hz, p_vow_group_lob, ncol=1)

```


#### Stop voicing

```{r vot-data, cache=TRUE}

vot <-
  votcorpora::vot %>%
  filter(source == 'buckeye') %>%
  rename(Talker = subject,
         Sex = sex,
         Age = age_group) %>%
  group_by(phoneme, Talker) %>%
  mutate(Token = row_number(),
         cues = 'VOT') %>%
  ungroup() %>%
  mutate(Marginal = 'all')

vot_by_place <-
  vot %>%
  group_by(place, cues) %>%
  nest()

vot_groupings <- c('Marginal', 'Sex', 'Age', 'Talker')
vot_by_place_grouped <- apply_groupings(vot_by_place, vot_groupings)

n_vot_talkers <- vot %>% group_by(Talker) %>% summarise() %>% nrow()
n_vot_per_talker <- vot %>% group_by(phoneme, voicing, place, Talker) %>% tally()

```

We analyzed data on word-initial stop consonant voicing in conversational speech from the Buckeye corpus [@Pitt2007, extracted by Wedel, _in prep_]. Voice onset time (VOT) was automatically extracted for `r nrow(vot)` word initial stops, `r vot %>% filter(voicing=='voiced') %>% nrow()` voiced and `r vot %>% filter(voicing=='voiceless') %>% nrow()` voiceless, for labial, coronal, and dorsal places of articulation. Data came from `r n_vot_talkers` talkers, who were balanced male and female and younger/older than 40 years. On average, each talker produced `r round(mean(n_vot_per_talker$n))` tokens for each phoneme (range of `r min(n_vot_per_talker$n)` -- `r max(n_vot_per_talker$n)`).

Our primary reason for considering VOT/voicing at all is to get a sense of the range of informativity and utility of socio-indexical variables across different phonetic categories. VOT is thought to be relatively stable across talkers, and formant frequencies relatively variable. The strength of this particular VOT corpus is that it contains observations of both voiced and voiceless stops from the same talkers. This allows us to directly assess how much talker variability in VOT distributions [@Allen2003] actually impacts recognition of voiced vs. voiceless stops, and hence estimate an upper bound on the usefulness of _any_ socio-indexical grouping variable for this contrast. 

However, the downside is that this corpus does not not contain data from talkers who vary on socio-indexical variables that are known to correlate with differences in VOT distributions, like native language background. Preliminary analyses of a collection of VOTs for only voiceless stops from French-English bilinguals [@Lev-Ari2013] suggests that, even though these groups are known to produce different VOT distributions, the size of this effect is much smaller than even talker-level variability _within_ the monolingual talkers in the Buckeye corpus (which, to foreshadow our results, is substantially smaller than for vowels).

### Modeling approach

All of our analyses depend on knowing the _distribution_ of cues for each phonetic category, at different levels of socio-indexical grouping. We obtain estimates of these distributions in the following way. We assume that each phonetic category can be modeled as a normal distribution over cue values (stop voicing as univariate distributions over VOT, and vowels as bivariate distributions of F1 and F2). These distributions are parameterized by their mean and their covariance matrix (or, equivalently, variance in the case of VOT). We fit these parameters to data from our corpora via maximum likelihood, using the sample mean and covariance of tokens from each category. We do this separately for each group. For example, for gender, we obtain one estimate of the `ae` distribution based on all the tokens from male talkers, and one from all tokens from female talkers. Likewise, for dialect, we estimate one distribution based on all talkers from the North dialect region, another one from tokens from Mid-Atlantic talkers, and so on.

Assuming that each category is a normal distribution is not a critical part of our approach, but rather a standard and convenient assumption. In particular, the normal distribution has a small number of parameters and this allows us to efficiently estimate the distribution for each category with a limited amount of data (e.g., five tokens per talker-level vowel distribution).


## Study 1: Informativity of socio-indexical groupings about cue distributions

Our first goal is to assess how _informative_ socio-indexical variables are about the cue distributions for each phonetic category, across levels of socio-indexical grouping and phonetic categories/cues.

### Methods {#sec:kl-methods}

One way to quantify how informative a socio-indexical grouping variable is about cue distributions is by comparing the group-level cue distributions with the _marginal_ distribution of cues from all groups. The reason for this is that if a socio-indexical grouping variable (e.g., sex) is _not_ informative about cue distributions, then the the cue distributions for each group (e.g., male and female talkers) will be essentially identical, and hence indistinguishable from the _overall_ cue distribution. If, on the other hand, a socio-indexical variable _is_ informative about cue distributions, then the distribution for each group will deviate substantially from other groups, and by extension from the overall distribution as well.
The particular measure we use to compare distributions is the Kullback-Leibler (KL) divergence, which explain next.[^kl-vs-var-explained]

[^kl-vs-var-explained]: This measure is intuitively similar to the proportion of
    variance explained by the socio-indexical grouping variable
    [as used in @McMurray2011a]. However, it is a more general approach that
    does not require that we assume that the underlying distributions are normal
    distributions, and can be applied even to categorical variables (like
    distributions of words or syntactic structures). It also, as we show here,
    naturally extends to multidimensional cue spaces, taking into account the
    correlations between cues, and supporting comparisons to other cue spaces.

<!-- TODO: expand this and put it into the discussion. it's an important point of comparison. -->
<!-- TODO: are these equivalent? -->

#### Technical details

KL divergence (or relative entropy) is an information-theoretic comparison of two distributions [@Mackay2003, p. 34]. In our usage, it measures the expected cost (in bits of extra message length) of encoding data from each group using a code that's optimized for the marginal distribution [@Mackay2003, p. 98].[^vs-marginal-or-other]

[^vs-marginal-or-other]: @Richter2016 apply a similar approach to vowel data
    from the NSP, with an important difference: they compare group-specific
    distributions to _each other_, instead of the marginal distribution. Their
    goal is to evaluate how well different normalization schemes remove talker
    variability, while ours is to evaluate how informative different
    socio-indexical variables are about the underlying distributions. These are
    related, but not identical goals, and thus our methods are similar but not
    identical.

In general, the KL divergence of $Q$ from $P$ is
$$DL(Q||P) = \int p(x) \log \frac{p(x)}{q(x)} \mathrm{d}x$$ {#eq:kl}
(with density functions $q$ and $p$ respectively). Intuitively, we can think of the KL divergence in the following way. Take data that has been sampled from a true distribution $P$. The KL divergence of another distribution $Q$ from $P$ is how much better, on average, $P$ explains the sampled data than $Q$. The KL divergence increases as $Q$ diverges more from $P$, and has a minimum value of zero, which is only achieved when $P=Q$ [@Mackay2003, p. 34].

In our case, $P=\mathcal{N}_G$ is a multivariate[^multivariate] normal cue distribution conditioned on a socio-indexical group, with mean $\mu_G$ and covariance $\Sigma_G$, while $Q=\mathcal{N}_M$ is the marginal (not conditioned on group) cue distribution with mean $\mu_M$ and covariance $\Sigma_M$. With some simplification,[^gaus-kl] the KL divergence of the marginal from the group distribution works out to be
$$
DL(\mathcal{N}_M || \mathcal{N}_G) = \frac{1}{2} \left( \mathrm{tr}(\Sigma_M^{-1} \Sigma_G) + (\mu_M - \mu_G) \Sigma_M^{-1} (\mu_M - \mu_G) - d + \log\frac{|\Sigma_M|}{|\Sigma_G|} \right)
$$ {#eq:klnorm}
where $d$ is the dimensionality of the distribution, and $\log$ is the natural logarithm. For ease of interpretation, we report KL divergence in bits, which corresponds to using $\log_2$ in equation @eq:kl, or dividing equation @eq:klnorm by $\log(2)$).

[^multivariate]: The math is the same for the univariate special case, as with VOT.
[^gaus-kl]: See, for instance, [http://stanford.edu/~jduchi/projects/general_notes.pdf](), p. 13.

For each phonetic category, we calculate the KL divergence of each group's cue distribution from the marginal distribution of cues from all talkers. We then average these single-category scores for each group to calculate the overall divergence for that group.[^category-num] Finally, for each grouping level, we average the divergence across groups, and compute bootstrapped confidence intervals over groups for this mean.
<!-- TODO: illustrate what 1 bit of KL divergence looks like -->
<!-- TODO: does dimensionality matter? I don't think so, but prove it. actually it might with increasing variance (determinant increases as $p^d$ for scaling covariance by $p$. -->

[^category-num]: We average over categories for two reasons. First, it's
    mathematically convenient, because the KL between to normal distributions
    can be computed in closed form, but for mixture models must be estimated
    with numerical integration. Second, by averaging over categories, we control
    for any effects that could be due to the overall difference between the
    number of vowel (11) and stop voicing (2) categories.



```{r kl-helpers, cache=TRUE}

run_kl <- function(data_grouped, reference_grouping,
                   category_col='Vowel', cue_cols=c('F1', 'F2'), ...) {
  ## check input format
  assert_that(has_name(data_grouped, 'data'),
              has_name(data_grouped, 'grouping'))

  train <- partial(train_models, grouping=category_col, formants=cue_cols,  ...)
  
  models <- data_grouped %>%
    mutate(models = map2(data, grouping, ~ train(.x) %>% rename_(group=.y)))

  models %>%
    filter(grouping == reference_grouping) %>%
    mutate(reference_models = map(models, rename_, reference_group = 'group')) %>%
    select(-grouping, -data, -models) %>%
    left_join(models %>% filter(grouping != reference_grouping)) %>%
    mutate(kl_from_reference = map2(models, reference_models,
                                    ~ left_join(.x, .y, by=category_col) %>%
                                      mutate(KL = map2_dbl(model.x, model.y,
                                                           KL_mods)) %>%
                                      select_('group', 'reference_group',
                                              category_col, 'KL')
                                    )
           ) %>%
    unnest(kl_from_reference)
    
}

```

```{r vowel-kl, cache=TRUE, dependson=c('kl-helpers', 'vowel-data')}

vowel_kl <-
  vowel_data_grouped %>%
  run_kl(reference_grouping = 'Marginal',
         category_col = 'Vowel',
         cue_cols = c('F1', 'F2')) %>%
  filter(!is.na(KL))                    # NAs come from one vowel ('uh') that
                                        # only has one token for one talker.


```

```{r vot-kl, cache=TRUE, dependson=c('kl-helpers', 'vot-data')}

vot_kl <-
  vot_by_place_grouped %>%
  run_kl(reference_grouping = 'Marginal',
         category_col = 'voicing',
         cue_cols = 'vot')

```

### Results {#sec:kl-results}

```{r vowel-vot-kl-plot, fig.width=8.3, fig.height=4.2, fig.cap="Socio-indexical variables are more informative about cue distributions for vowel (formants) than for stop voicing (vot). On top of this, more specific groupings (like Talker and Dialect+Sex) are more informative than broader groupings (Sex). This is indicated by higher KL divergence of each grouping level from marginal (each point shows one group's average KL divergence from marginal distributions, and large points with errorbars show the mean and bootstrapped 95% CIs over groups)."}

bind_rows(vot_kl %>% mutate(contrast = 'Stop voicing'),
          vowel_kl %>% mutate(contrast = 'Vowels')) %>%
  mutate(grouping = factor(grouping, levels=grouping_levels)) %>%
  group_by(contrast, cues, grouping, group) %>%
  summarise(KL = mean(KL)) %>%
  ggplot(aes(x=grouping, y=KL, color=grouping)) +
  geom_point(position='jitter', alpha=0.2) + 
  geom_pointrange(stat='summary', fun.data=mean_cl_boot,
                  position = position_dodge(w=0.5)) +
  facet_grid(.~contrast+cues, scales='free', space='free') +
  rotate_x_axis_labs() +
  labs(x = 'Grouping',
       y = 'KL Divergence of cue distributions\nfrom marginal (bits)')

```

<!-- TODO: pretty name for "Dialect_Sex" -->

Figure {@fig:vowel-vot-kl-plot} plots the KL divergence of cue distributions at different levels of grouping from marginal distributions, across contrasts (vowels and stop voicing) and cues (VOT, raw/Lobanov-normalized F1xF2). There are two clear patterns. 

First, we find that talker identity is an order of magnitude more informative
about vowel distributions than about VOT distributions. That is, knowing a
talker's identity is significantly provides significantly more information about
their vowel formant frequency distributions than it does about their VOT
distributions. This is important because it uses a quantitative measure to
confirm the qualitative finding from previous work that there is less talker
variability in VOT than in formant frequencies
[e.g., @Allen2003; @Lisker1964; vs. @Peterson1952; @Hillenbrand1995].
Strikingly, the _most_ informative variable for VOT---talker identity---is
roughly as informative as the _least_ informative variable for
Lobanov-normalized F1xF2 (Sex). Figure {@fig:vot-vs-vowel-dists} illustrates
what this means in terms of the conditional distributions for these two
combinations of grouping variables and categories: on average, individual
talkers' VOT distributions diverge from the marginal distribution (B) at roughly
the same level that the male and female distributions of normalized F1xF2
diverge from the marginal normalized F1xF2 distributions (A).

```{r vot-vs-vowel-dists, fig.width=6.2, fig.height=6.0, fig.cap='Male and female distributions of Normalized F1xF2 diverge from the marginal distributions (A) only slightly less than talker-specific VOT distributions diverge from marginal (B) (see Figure {@fig:vowel-vot-kl-plot}'}

gg_vowels_by_sex <-
  ggplot(nsp_vows_lob, aes(x=F2, y=F1, group=Vowel)) +
  ## stat_density2d(data = nsp_vows_lob %>% select(-Sex),
  ##                geom='contour', bins=4, color='#888888') +
  ## stat_density2d(geom='contour', bins=4, aes(color=Sex)) +
  stat_ellipse(data = nsp_vows_lob %>% select(-Sex),
               color='#888888', type='norm') +
  stat_ellipse(aes(color=Sex), type='norm') +
  scale_x_reverse() +
  scale_y_reverse() +
  facet_grid(.~Sex) +
  labs(color = 'Sex',
       x='F2 (Lobanov normalized)',
       y='F1 (Lobanov normalized)') +
  theme(panel.grid = element_blank())

gg_vot_by_talker <-
  ggplot(vot, aes(x=vot, group=voicing)) +
  stat_density(aes(group=paste(Talker,voicing), color='Talker'),
               geom='line',
               position='identity',
               alpha = 0.5) +
  stat_density(aes(color='Marginal'),
               geom='line',
               position='identity') +
  scale_color_manual("", values = c('black', 'gray')) +
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid = element_blank()) +
  labs(x = 'VOT (ms)')

ggdraw() +
  draw_plot(gg_vowels_by_sex, 0, 0.5, 1, 0.5) +
  draw_plot(gg_vot_by_talker, 0.07, 0, 0.93, 0.5) +
  draw_plot_label(c("A", "B"), c(0, 0), c(1, 0.5), size = 15)

## plot_grid(gg_vowels_by_sex, gg_vot_by_talker, labels=c('A', 'B'), nrow=2)

```

Second, on the whole, we find that more specific groupings are more informative
than less specific groupings. This suggests that, within a group, talkers are
more consistent than they are between groups. This is to be expected when there
is talker variability, and when that variability is greater between groups than
within groups. The exception to this pattern is that, for un-normalized
formants, sex is more informative than dialect, even though dialect is more
specific (8 talkers per dialect, vs. 24 per sex). The likely reason for this is
that sex differences in physiology (e.g., vocal tract length) change formant
frequencies for all vowels [@Johnson2006], but dialect variation is limited to
certain dialect-vowel combinations [@Clopper2005; @Labov2005].

As Figure {@fig:vowel-kl-by-category} shows, while the relative ordering of grouping variables' informativity is consistent across vowels, their actual degree of informativity varies quite a bit. Dialect (and Dialect+Sex) is particularly informative for `aa`, `ae`, `eh`, and `uw`, vowels with distinctive variants in at least one of the dialect regions from the NSP. As noted by @Labov2005, `aa` is undergoing a merger with `ao` in some regions, `ae` and `eh` participate in the Northern Cities Chain Shift, and `uw` is fronted in some regions [and in others only by female talkers; @Clopper2005].

```{r vowel-kl-by-category, fig.width=7, fig.height=3.45, fig.cap='Individual vowels vary substantially in the informativity of grouping variables about their cue distributions. Only normalized F1xF2 is shown to emphasize dialect effects.'}

vowel_kl %>%
  mutate(grouping = factor(grouping, levels=grouping_levels)) %>%
  filter(str_detect(cues, 'Normalize')) %>%
  group_by(cues, Vowel, grouping, group) %>%
  summarise(KL = mean(KL)) %>%
  ggplot(aes(x=Vowel, y=KL, color=grouping)) +
  geom_pointrange(stat='summary', fun.data=mean_cl_boot,
                  position = position_dodge(w=0.5)) +
  facet_grid(cues ~ ., scales='free', space='free') +
  labs(x = 'Vowel',
       y = 'KL Divergence of cue distributions\nfrom marginal (bits)')

```

Figure @fig:vowel-kl-by-dialect shows the KL divergence of each dialect's vowel
distributions, demonstrating that dialects do indeed vary in how informative
they are, both overall and by vowel.  Some of this variability corresponds to known patterns of dialect variability. In particular, talkers from the North
dialect region produce vowels---`ae` and `aa` in particular---with formant
distributions that deviate markedly more from the marginal distributions than
any of the other dialects. Both of these vowels participate in the Northern Cities Shift [@Labov2005; @Clopper2005]. The Mid-Atlantic `aa` is, like the
Northern `aa`, non-merged with `ao` [@Clopper2005] and hence deviates from the
marginal `aa` substantially.

Other particularly high divergences do not correspond to known sociolinguistic
variants, and may be flukes of this particular dataset which includes a small
number of talkers per region. This includes the high divergence of `uw` from New
England talkers, which is the result of these talkers producing a very
low-variance `uw` distribution.  Similarly, Mid-Atlantic talkers produce a
low-variance `ey` distribution that is slightly higher and fronter than the
marginal distribution.

```{r vowel-kl-by-dialect, fig.width=7.5, fig.height=4.75, fig.cap='A small number of dialect/vowel combinations account for most of the divergence of dialect-specific vowel formant distributions. In particular, the distribution of `ae` and `aa` produced by Northern talkers diverge markedly more than any other vowel/dialect combination.'}

## vowel_kl %>%
##   filter(grouping=='Dialect') %>%
##   ggplot(aes(x=group, y=KL)) +
##   geom_pointrange(stat='summary', fun.data=mean_cl_boot) +
##   facet_grid(.~cues)

vowel_kl %>%
  filter(grouping=='Dialect') %>%
  ggplot(aes(x=group, y=KL)) +
  geom_line(aes(group=Vowel, color=Vowel)) +
  geom_text(data = vowel_kl %>%
               filter(grouping == 'Dialect', str_detect(cues, 'Lobanov')) %>%
               arrange(desc(KL)) %>%
               head(5),
            aes(label = Vowel, color=Vowel), show.legend=FALSE,
            nudge_x = 0.25, nudge_y=0.05) +
  facet_grid(.~cues) +
  rotate_x_axis_labs() +
  labs(x = 'Dialect',
       y = 'KL divergence from marginal (bits)')

```

Finally, these results show that, even after controlling for overall shifts in
formant frequencies across all vowels via Lobanov normalization, there is still
substantial talker variability. Normalization also substantially reduces the
informativity of sex alone, which is to be expected given that most of the
difference between male and female vowel distributions is due to the overall
shifts in formant frequencies that Lobanov normalization removes. Interestingly,
while normalization reduces the informativity of dialect and sex considered
together, their combination is still more informative than dialect alone. This
suggests that dialect differences themselves are modulated by sex
[as noted by @Clopper2005].
<!-- TODO: is this true? permutation test on sex within dialect. -->
<!-- TODO-paper: summary statement -->

This asymmetry in informativity across both dialects and vowels raises the
question of how listeners adapt to variation across categories and cue
dimensions. All else being equal, a listener should be more confident in their
prior beliefs about a category that varies less across talkers, and hence adapt
less flexibly [@Kleinschmidt2015]. But it is not clear at what level listeners
track variability for the purpose of determining how quickly to adapt. For
instance, as we have seen, vowels vary substantially more across talkers than
stop categories, but there are further differences in how much individual vowels
vary. It remains to be seen whether listeners adapt to all vowels with the same
degree of flexibility, or are sensitive to these vowel-specific differences in
cross-talker variability.

## Study 2: utility of socio-indexical groupings for speech recognition

Next, we evaluate the _utility_ of each grouping variable for speech
recognition.  One way to estimate the utility of a socio-indexical variable is
based on actual human speech perception behavior. The downside of this method is
that it is impossible to control listeners' prior experience, and hence what
group-level distributions they may already be tracking. It can moreover
difficult to manipulate listeners' beliefs about the socio-indexical properties
of the talker whose speech they are classifying, especially for variables like
dialect or class.

We take a different approach that avoids these problems (at the cost of certain
well-understood assumptions, described below), based on patterns of
socio-indexically condition variation in cue distributions themselves. One
principled way quantify utility is in terms of _probability of correct
recognition_ of phonetic categories. By treating speech perception as an
inference problem, the ideal adapter
[@Kleinschmidt2015; and the ideal listener models that it draws on, e.g. @Clayards2008]
provides a link between group-specific cue distributions and the probability of
correct recognition. We describe this link and how we actually calculate it, and
then present the results for the datasets we consider here.

### Methods {#sec:recog-methods}

In the ideal adapter/listener, the link between cue distributions and
recognition of phonetic categories is provided by Bayes Rule.[^gen-vs-discrim] Bayes Rule says
that the _posterior probability_ that a particular cue value came from a
category is the _likelihood_ of that cue value being generated by that
category's cue distribution, divided by the total likelihood of it being
generated by any category.[^flat-prior] For instance, the posterior probability
that a VOT of 20ms is a /b/ is the likelihood of a 20ms VOT under the /b/ VOT
distribution, divided by the total likelihood of 20ms VOT under both /b/ and
/p/.

<!-- TODO: expand and put this in the discussion, maybe under "lower bound" -->
[^gen-vs-discrim]: Our approach is based on a _generative_ model of
    categorization, which starts from cue distributions and derives
    classification boundaries. This contrasts with _discriminative_ models which
    directly map cue values to categories, like the logistic regression model
    that @McMurray2010a used to evaluate the utility of different cue encoding
    schemes for classifying fricatives. The main advantage to using a generative
    framework, overall, is that it provides a _learning model_, and does not
    require us to assume that listeners actually _know_ the underlying
    distributions for each talker [@Kleinschmidt2015]. But it remains a question
    for future work exactly how the predictions of our approach here differ from
    those of the discriminative modeling approach of @McMurray2010a.


[^flat-prior]: This assumes---for the sake of convenience---that the prior
    probability of each category is equal. This is not a limitation of our
    approach or the ideal adapter/observer framework. Bayes rule naturally
    extends to situations where there is a prior expectation that some
    categories are more likely than others. It would be possible to incorporate,
    for instance, the base frequencies of the phonetic categories we examine
    into account, based on phonotactic corpora [e.g., @Vitevitch2004].

<!-- TODO-paper: illustrate this -->

If two categories' distributions overlap a lot, then they will frequently assign
similar likelihood to cue values generated by the other, leading to confusion
and recognition errors. Separating distributions based on a socio-indexical
grouping variable can affect the overlap between different phonetic categories,
and thus the rate of correct recognition is a measure of the _utility_ of that
grouping. If lumping together tokens from, for instance, both male and female
talkers causes vowel distributions to overlap more, that will decrease the
probability of correct recognition, relative to the male- and female-only
distributions (that is, distributions _conditioned on_ gender).

More broadly, by calculating the probability of correct recognition using, for instance, the cue distributions of each category produced by female talkers, we can  estimate how well a listener would be able to recognize speech from an unfamiliar female talker if all they knew was the talker's sex. This measure is meaningfully different from the KL-based informativity measure from Study 1, because a category might vary substantially between, e.g., male and female talkers, but if it is already highly distinguishable from other categories this variability may not impact correct recognition.

As we discuss in more detail in the general discussion, this measure is best
considered a _lower bound_ on how useful a particular grouping variable is for
comprehending speech from an unfamiliar talker. The reason for this is that the
marginal cue distribution for a group does not necessarily reflect the
distribution of talker-specific cue distributions in that group, and hence
reflects an approximation of where a listener would _start_ when adapting to a
new talker from that group, but not where they would end up after adapting to
that particular talker. It is possible that a grouping variable might have
relatively low utility by the measure we use here, but still make adaptation
substantially more efficient, making its actual utility higher. Quantifying this
notion of utility is computationally more demanding and thus we leave it as a
question for future work.

#### Technical details

We want to determine the phonetic category $v_i$[^notation] of each of the cues $x_i$ produced by a talker. If we assume that the listener knows that this talker belongs to group $g=j$, this inference is a straightforward application of Bayes Rule:
$$
p(v_i | x_i, g=j) \propto p(x_i | v_i, g=j) p(v_i)
$$
If, on the other hand, the listener does not know which group the talker belongs to, they have to marginalize out group. This amounts to taking a weighted average of the posterior probabilities under each group, weighted by the probability that the talker belongs to that group, $p(g | x)$:
$$
p(v_i | x_i) = \sum_j p(v_i | x_i, g=j) p(g=j | x)
$$
(where $x$ refers to all the tokens produced by this talker). For the sake of brevity, we do not present results when group is unknown, limiting ourselves to the case where the listener _knows_ the value of the relevant socio-indexical variable.  This is not a completely unreasonable assumption, because listeners often have _some_ information about the sex, regional origin, etc. of a talker, based on non-linguistic cues.
<!-- TODO-paper: add results when group is unknown? -->

[^notation]: $x_i$ refers to a single observed cue value (possibly multidimensional, in the case of vowel formants), and $x$ (without subscript) refers to a _vector_ of multiple observations (from a single talker, unless otherwise specified). $v_i$ refers to observation $i$'s category, and $g$ to a talker's group. $\sum_j$ refer to a sum over all possible values of $j$.

For vowels, we classified vowel categories directly. For voicing, the only cue available is VOT, which does not (reliably) distinguish place of articulation. Thus, we classified voicing separately for each place of articulation, and then average the resulting accuracy.

To assess statistical significance, we bootstrap over talkers. This is a
non-parametric approach that provides confidence intervals and $p$ values for
effects at the population level, based on the limited sample of talkers in our
datasets, without assuming a particular parametric form (like a normal
distributions) for the variability across talkers.

#### Avoiding anti-conservative estimates through cross-validation

For classification, if test data is included in the training set, this artificially inflates accuracy at test [@James2013, Section 5.1].  Cross-validation controls for this by splitting data into training and test sets.  For group-level models (sex, age, dialect, and dialect+sex), we use leave-one-talker-out cross-validation, training each group's models with test talker's observations held out. For the talker-specific models, we use 6-fold cross-validation (or leave-one-out when there were fewer than 6 tokens in a category for a talker), where each phonetic category is randomly split into 6 approximately equal subsets. Then, one subset of each category is selected for test, the models are trained on the remaining five, and the test data is classified as above.

Cross-validation is not only important because it provides an unbiased measure of classifier accuracy. It is also essential for testing the hypothesis that _group-level_ cue distributions are useful to listeners.  If the test talker is included in the training dataset, then the utility of that talker's own productions is confounded with any utility of the group itself.

#### Avoiding biases from different group sizes

For the vowel data, the different levels of grouping have very different group sizes, and this requires some caution. The broadest (sex) has 24 talkers per group (23 after holdout), while the most specific (dialect+sex) has only 4 (3 after holdout).  This introduces a systematic bias in favor of broader groupings, because small sample sizes lead to noisier estimates of the underlying model, and hence lower accuracy (on average) at test [@James2013, Section 2.2.2]. This bias would make it impossible to meaningfully compare probability of correct recognition _across_ grouping levels, defeating the purpose of this analysis.

To correct for this, we randomly subsampled three talkers (without replacement) within each group in the training set (that is, after holding out the test talker). This ensures that every group has is the same size as the smallest group across grouping levels. For the studies reported here, that corresponds to the test talker's Dialect+Sex group.[^seven-talkers]

We use 20 different random subsamples for each test talker, averaging accuracy over
each resampled training set. A different subsampling is used for every talker,
and thus any additional variance introduced by this procedure is accounted for
by bootstrapping talkers.  The estimates obtained in this way allow us to
compare accuracy across groupings with different group sizes, but at the cost of
underestimating the true group-level accuracy across the board. As such, they
must be considered a useful lower bound on the utility of socio-indexical
groupings.

[^seven-talkers]: We also ran the analyses resampling each group to 7 talkers, which corresponds to the Dialect-level group size after holdout (excluding the Dialect+Sex grouping from comparison, since there are only 4 talkers per group before holdout). Besides a small increase in overall accuracy (because of the reduced variance of the distribution estimates), this did not substantially change the results.

Subsampling was not necessary for the VOT data: the Buckeye Corpus is balanced by age and sex, the two socio-indexical variables we consider.

```{r classification-helpers, cache=TRUE}

## 1. Likelihood of each token under each vowel for each dialect model
#' Compute posterior vowel category conditional on group
#'
#' Applies classify_vowels to test data for each group_model.
#'
#' @param data_test test data to calculate posteriors for
#' @param group_models named list of group models, each of which is a named list
#'   of vowel models
#' @return data frame with one row per data_test row x group x vowel model, with
#'   added columns group_model (name of group model), vowel_model (name of vowel
#'   model), lhood p(x | vowel_model, group_model), posterior (p(vowel_model |
#'   x, group_model)).
compute_category_post_given_group <- function(data_test, group_models) {
  group_models %>%
    map(~ unlist_models(., 'category')) %>%
    map(~ classify(data_test, ., 'category')) %>%
    data_frame(group_model=names(.),
               x=.) %>%
    unnest(x) %>%
    rename(category_model=model)
}

#' Combine category | group posteriors with group posteriors
#'
#' @param group_category_posteriors category posterior probabilities conditional
#'   on group, in the form of a data frame with at least columns category_model,
#'   group_model, and posterior (e.g., output of
#'   compute_category_post_given_group)
#' @param group_posterior marginal group posterior probabilities, in the form of
#'   a data frame with columns group_model and log_posterior (e.g., output of
#'   compute_group_marginal_posterior)
#' @return a data frame with the joint posterior of category category and group,
#'   in posterior and log_posterior.
#' 
compute_joint_category_group_post <- function(group_category_posteriors, group_posteriors) {
  group_posteriors %>%
    select(group_model, group_log_posterior=log_posterior) %>%
    inner_join(group_category_posteriors, by = 'group_model') %>%
    mutate(log_posterior = log(posterior) + group_log_posterior,
           posterior = exp(log_posterior))
}

#' Compute joint indexical-linguistic posterior
#'
#' @param trained data frame with \code{models} and \code{data_test} (as
#'   produced by \code{\link{train_models_indexical_with_holdout}}).
#' @param obs_vars quoted names of columns in test data that together indentify
#'   a single observation (e.g., \code{c('Vowel', 'Token')})
#' @return a data frame with one observation per combination of group (e.g.,
#'   Dialect), category (e.g. "ae"), and row in the ORIGINAL, un-nested data
#'   set, with new columns \code{group_model}, \code{category_model},
#'   \code{lhood}, \code{posterior}, and \code{log_posterior}. Posterior
#'   probabilities sum to 1 within each cross-validation fold (e.g., Talker) +
#'   observation (e.g., Vowel+Token) combination, over all values of category
#'   and group.
#' 
trained_to_joint_post <- function(trained, obs_vars) {

  trained %>%
    mutate(conditional_posteriors = map2(data_test, models,
                                         compute_category_post_given_group),
           group_posteriors = map(conditional_posteriors,
                                  . %>%
                                    group_by_(.dots=obs_vars) %>%
                                    mutate(log_lhood = log(lhood)) %>%
                                    marginalize_log('log_lhood', 'group_model') %>%
                                    ungroup() %>%
                                    aggregate_log_lhood('log_lhood', 'group_model') %>%
                                    normalize_log_probability('log_lhood')),
           joint_posteriors = map2(conditional_posteriors, group_posteriors,
                                   compute_joint_category_group_post)) %>%
    unnest(joint_posteriors)

}


#' @param d data frame
#' @param holdout Column defining cross validation folds
#' @param ... additional arguments passed to \code{\link{train_models}}.
classify_by_talker_cv <- function(d, holdout='Token', category='Vowel', ...) {
  train <- partial(train_models, grouping=category, ...)

  d %>%
    nspvowels::train_test_split(holdout=holdout) %>%
    mutate(models_trained = map(data_train,
                                . %>% group_by(Talker) %>% train()),
           models_tested = map2(data_test, models_trained, classify,
                                category=category)) %>%
    unnest(models_tested) %>%
    mutate(grouping = 'Talker',
           group_is = 'Known',
           group = Talker) %>%
    rename(category_model = model)
}


```

```{r vowel-classification-models-group-known, cache=TRUE, dependson=c('vowel-data', 'classification-helpers')}

min_talker_per_group <- function(d) {
  d %>%
    do(n = length(unique(.$Talker))) %>%
    select_('n') %>%
    unlist() %>%
    min()
}

#' @param d data frame with columns for groups, category, and holdout
#' @param category name of column with phonetic category (e.g. 'Vowel')
#' @param holdout name of column with factor to define cv folds (e.g. 'Talker')
#' @param subsample_size number of levels of 'holdout' to sample as subset for
#'   training data
#' @param n_repetitions =10 number of times to repeat resampling
#' @return data frame with accuracy of each observation by repetition.
train_test_acc_same_group <- function(d, category, holdout,
                                      subsample_size, n_repetitions=10, ...) {
  assert_that(has_name(d, holdout))
  assert_that(has_name(d, category))
  
  train <- partial(train_models, grouping = category, ...)

  ## split data into train/test split, restricting to same group
  ## (assumes that d is already grouped)
  split_within_group <- function(d) {
    d %>%
      by_slice(train_test_split, holdout=holdout) %>%
      unnest(.out)
  }

  ## classify and get accuracy
  ## don't care about groups within train/test split (already restricted to same
  ## group) so we can just use train_models and classify directly
  train_test_acc <- function(data_train, data_test) {
    data_train %>%
      train() %>%
      rename_(category=category) %>%
      classify(data_test, ., 'category') %>%
      rename_(category_model = 'model') %>%
      get_accuracy(category_col = category)
  }

  ## repeat for some random subsamplings
  repeat_w_subsample <- function(data_train, data_test) {
    replicate(n_repetitions,
              data_train %>%
                sample_n_groups(group=holdout, n=subsample_size) %>%
                train_test_acc(data_test),
              simplify=FALSE) %>%
      do.call(bind_rows, .)
  }

  ## on resample talkers if there are > subsample_size talkers in the smallest
  ## group
  group_size <- min_talker_per_group(d)
  if (group_size <= subsample_size+1) {
    get_acc <- train_test_acc
  } else {
    get_acc <- repeat_w_subsample
  }

  ## put it all together
  acc <- d %>%
    split_within_group() %>%
    mutate(acc = map2(data_train, data_test, get_acc)) %>%
    unnest(acc)

}

set.seed(100)

vowel_acc_same_group_rep <-
  vowel_data_grouped %>%
  mutate(group_size = map_dbl(data, min_talker_per_group)) %>%
  right_join(cross_d(list(group_size = unique(.$group_size),
                          subsample_size = c(3,7))) %>%
               filter(group_size > subsample_size)) %>%
  mutate(acc = map2(data, subsample_size,
                    train_test_acc_same_group,
                    category = 'Vowel', holdout = 'Talker',
                    n_repetitions=20)) %>%
  unnest(map2(acc, grouping, ~ rename_(.x, group=.y)))

```

```{r vowel-classification, cache=TRUE, dependson=c('classification-helpers')}

vowel_talker_class <-
  vowel_data %>%
  unnest(map(data, classify_by_talker_cv, holdout='Token')) %>%
  mutate(group_is = 'Known')

```

```{r vot-classification-models, cache=TRUE, dependson=c('vot-data', 'classification-helpers')}

set.seed(101)

vot_models <-
  vot_by_place_grouped %>%
  filter(grouping != 'Talker') %>%
  mutate(trained = map2(data, grouping,
                        ~ train_models_indexical_with_holdout(.x,
                                                              .y,
                                                              category = 'voicing',
                                                              formants = 'vot')),
         joint_posteriors = map(trained, trained_to_joint_post,
                                obs_vars = c('voicing', 'Token')))

```

```{r vot-classification, cache=TRUE, dependson=c('vot-classification-models', 'classification-helpers')}

vot_joint_class <-
  vot_models %>%
  unnest(map2(joint_posteriors, grouping, ~ rename_(.x, group=.y))) %>%
  group_by(place, cues, grouping)

vot_marginal_class <-
  vot_joint_class %>%
  group_by(Talker, voicing, Token, group, category_model, add=TRUE) %>%
  marginalize_log('log_posterior') %>%
  normalize_log_probability('log_posterior') %>%
  mutate(group_is = 'Inferred')

vot_true_group_class <-
  vot_joint_class %>%
  filter(group == group_model) %>%
  group_by(Talker, voicing, Token, add=TRUE) %>%
  normalize_log_probability('log_posterior') %>%
  mutate(group_is = 'Known')

vot_talker_class <-
  vot_by_place %>%
  unnest(map(data, . %>%
                     group_by(Talker, voicing) %>%
                     mutate(split=ntile(runif(length(Talker)), 6)) %>%
                     classify_by_talker_cv(holdout='split',
                                           category='voicing',
                                           formants='vot')
             )
         )

vot_class <-
  bind_rows(vot_marginal_class,
            vot_true_group_class,
            vot_talker_class)

```

```{r check-classification, results='hide', cache=TRUE, dependson=c('vot-classification')}

vot_class %>%
  group_by(cues, place, group_is, grouping, Talker, voicing, Token) %>%
  summarise(n_choice = sum(posterior_choice),
            sum_post = sum(posterior)) %$%
  assert_that(all(n_choice == 1),
              all.equal(sum_post, rep(1, length(sum_post))))

```


```{r classification-accuracy}

acc_method <- 'choice'

vot_accuracy <-
  vot_class %>% get_accuracy('voicing', method=acc_method)


vowel_talker_acc <-
  vowel_talker_class %>% get_accuracy('Vowel', method=acc_method)

vowel_accuracy <-
  vowel_acc_same_group_rep %>%
  filter(subsample_size==3) %>%
  group_by(cues, grouping, subsample_size, group, Talker, Vowel, Token) %>%
  summarise(accuracy = mean(accuracy)) %>%
  bind_rows(vowel_talker_acc) %>%
  mutate(group_is = 'Known')

accuracy <- 
  vowel_accuracy %>%
  select(-Age) %>%
  mutate(contrast = 'Vowels') %>%
  bind_rows(vot_accuracy %>% mutate(contrast = 'Stop voicing')) %>%
  ungroup() %>%
  mutate(grouping = factor(grouping, levels=grouping_levels))

accuracy_summary <-
  accuracy %>%
  group_by(contrast, cues, grouping, group_is, Talker) %>%
  summarise(accuracy = mean(accuracy)) %>%
  do({ mean_cl_boot(.$accuracy) }) %>%
  rename(accuracy = y,
         accuracy_lo = ymin,
         accuracy_hi = ymax)

## Talker advantage
## (TODO: incorporate)
talker_advantage_acc <- 
  accuracy %>%
  group_by(cues, contrast, grouping, Talker) %>%
  summarise(accuracy=mean(accuracy)) %>%
  rename(Talker_=Talker) %>%
  spread(grouping, accuracy) %>%
  gather(comparison, accuracy, -cues, -contrast, -Talker, -Talker_) %>%
  mutate(talker_advantage = Talker - accuracy) %>%
  group_by(contrast, cues, comparison) %>%
  do({ boot_ci(.$talker_advantage, function(d,i) mean(d[i], na.rm=TRUE), h0=0) }) %>%
  filter(is.finite(observed))


```

```{r vowel-dialect-advantage}

boot_dialect_advantage <- function(d) {
  d %>%
    filter(grouping != 'Talker') %>%
    group_by(Talker, grouping, add=TRUE) %>%
    summarise(proportion = mean(accuracy),
              logodds = log(sum(accuracy)+0.5) - log(sum(1-accuracy)+0.5)) %>%
    gather('measure', 'accuracy', proportion, logodds) %>%
    group_by(measure, add=TRUE) %>%
    spread(grouping, accuracy) %>%
    transmute(Dialect_over_marginal = Dialect - Marginal,
              Dialect_Sex_over_sex = Dialect_Sex - Sex) %>%
    gather('comparison', 'value', Dialect_over_marginal, Dialect_Sex_over_sex) %>%
    group_by(measure, comparison, add=TRUE) %>%
    do({ boot_ci(.$value, function(d,i) mean(d[i]), h0=0) })
}

## compute within-talker dialect advantage
dialect_advantage_boot_ci <-
  vowel_accuracy %>%
  group_by(cues, grouping) %>%
  boot_dialect_advantage()

dialect_avg_advantage <-
  dialect_advantage_boot_ci %>%
  filter(measure == 'proportion') %>%
  ungroup() %>%
  summarise(boot_p = max(boot_p),
            observed = mean(observed))

dialect_advantage_by_vowel_boot_ci <-
  vowel_accuracy %>%
  group_by(cues, grouping, Vowel) %>%
  boot_dialect_advantage()

dialect_advantage_by_dialect_boot_ci <-
  vowel_accuracy %>%
  select(-Dialect) %>%
  left_join(nsp_vows %>% group_by(Talker, Dialect) %>% summarise(),
            by = 'Talker') %>%
  rename(Dialect_ = Dialect) %>%
  group_by(cues, grouping, Dialect_) %>%
  boot_dialect_advantage() %>%
  rename(Dialect = Dialect_)

format_advantage <- function(d, ci_descrip='95% CI', p=TRUE, paren=TRUE) {
  adv_string <- sprintf('%.0f%%', 100*d$observed)
  ci_string <- sprintf('%s %.0f--%.0f%%',
                       ci_descrip, 100*d$ci_lo, 100*d$ci_high)
  p_string <- paste(',', daver::p_val_to_less_than(d$boot_p))

  if (paren) paste0(adv_string, ' (', ci_string, ifelse(p, p_string, ''), ')')
  else paste0(adv_string, ', ', ci_string, ifelse(p, p_string, ''))
}

north_adv_lob <-
  dialect_advantage_by_dialect_boot_ci %>%
  filter(Dialect == 'North',
         measure == 'proportion',
         str_detect(cues, 'Lobanov'))

north_adv_raw <-
  dialect_advantage_by_dialect_boot_ci %>%
  filter(Dialect == 'North',
         measure == 'proportion',
         !str_detect(cues, 'Lobanov'))

midatl_adv_lob <-
  dialect_advantage_by_dialect_boot_ci %>%
  filter(Dialect == 'Mid-Atlantic',
         measure == 'proportion',
         str_detect(cues, 'Lobanov'))

midatl_adv_raw <-
  dialect_advantage_by_dialect_boot_ci %>%
  filter(Dialect == 'Mid-Atlantic',
         measure == 'proportion',
         !str_detect(cues, 'Lobanov'))

```

```{r stars-and-bars}

make_stars_and_bars <- function(advantage, data_summary,
                                ci_hi_col = 'accuracy_hi') {
  advantage %>%
    ungroup() %>%
    filter(boot_p< 0.05, measure=='proportion') %>%
    mutate(boot_p_stars = p_val_to_stars(boot_p)) %>%
    separate(comparison, c('To', 'From'), sep='_over_') %>%
    mutate(From = tools::toTitleCase(From)) %>%
    left_join(data_summary) %>%
    filter(To == grouping | From == grouping) %>%
    group_by_('To', 'From',
              .dots = groups(data_summary) %>% discard(equals, 'grouping'),
              add=TRUE) %>%
    filter_(lazyeval::interp(~ x == max(x), x=as.name(ci_hi_col)))
}

## draw significance stars
geom_stars <- function(...) {
  stars_and_bars <- make_stars_and_bars(...)
  geom_text(data=stars_and_bars,
            aes(x=To, y=accuracy_hi, label=boot_p_stars),
            nudge_x=-0.5, nudge_y = 0.03, color='black')
}

## draw bars connecting significantly different pairs
geom_bars <- function(...) {
  stars_and_bars <- make_stars_and_bars(...)
  geom_segment(data=stars_and_bars,
               aes(x=From, xend=To, y=accuracy_hi+0.02, yend=accuracy_hi+0.02),
               color='black')
}

```

### Results {#sec:recog-results}

```{r overall-accuracy-group-known, fig.width=8.3, fig.height=4.2, fig.cap='Speech recognition accuracy using for marginal, group-level, and talker-specific cue distributions. Small points show individual talkers, and large points and lines show mean and bootstrapped 95% CIs over talkers. Marginal and group-level accuracy is based on leave-one-talker out cross-validation, and talker-specific on 6-fold cross-validation (or leave-one-token-per-category out if there are fewer than 6 tokens per category). Bars and stars show significant increases in accuracy when conditioning on dialect, alone or in addition to sex. For clarity, only some significant comparisons are shown. Here and elsewhere: `*` $p<0.05$, `**` $p<0.01$, and `***` $p<0.001$.'}

accuracy_summary %>%
  ggplot(aes(x = grouping, y=accuracy,
             color = grouping)) +
  geom_point(data = accuracy %>%
               group_by(grouping, Talker, cues, contrast) %>%
               summarise(accuracy = mean(accuracy)),
             position='jitter', alpha=0.2) +
  geom_pointrange(aes(ymin=accuracy_lo, ymax=accuracy_hi), stat='identity') +
  geom_stars(dialect_advantage_boot_ci, accuracy_summary) +
  geom_bars(dialect_advantage_boot_ci, accuracy_summary) +
  facet_grid(.~contrast+cues, scales='free_x', space='free_x') +
  rotate_x_axis_labs() +
  labs(x = 'Grouping',
       y = 'Probability of correct recognition') +
  lims(y = c(NA, 1))

```

Figure {@fig:overall-accuracy-group-known} shows the probability of correct
recognition for stop voicing/vowel, based on the cue distributions at each level
of grouping.  As with informativity about the distributions themselves, there's
an asymmetry between vowels and stop voicing in the overall utility of
socio-indexical variables for speech recognition. Probability of correct
recognition is overall higher for stop voicing than vowels.  Voicing recognition
is also less sensitive to the particular grouping variable: knowing whether a
talker is male vs. female (or young vs. old) provides no advantage when
classifying their VOTs as voiced or voiceless. This is consistent with the
finding above that VOT distributions themselves do not differ across
groups. Even using a talker's own distributions provides only a minimal
advantage (on the order of 2% increase in accuracy over marginal, age-, and
sex-conditioned distributions, all three 
`r talker_advantage_acc %>% filter(cues=='VOT') %$% boot_p %>% max() %>% daver::p_val_to_less_than()`).[^logodds]

For vowels, normalized input results in higher recognition accuracy across the
board, again paralleling the findings about the cue distributions
themselves. The one exception is at the level of talker-specific distributions,
where recognition accuracy is unchanged (since Lobanov normalization is a linear
transformation of the input, which leaves the structure of the categories within
each talker unchanged).

Conditioning vowel cue distribution on sex---either alone, relative to marginal,
or in addition to dialect---provides the biggest boost in recognition for
un-normalized formant accuracy. For normalized input, none of the
socio-indexical grouping factors provide much of an advantage over the marginal
distributions. In both cases, dialect provides a small but consistent advantage
for recognition, both alone and in combination with sex (increasing accuracy by
`r dialect_avg_advantage %$% round(observed*100)`% on average, all 
$`r dialect_avg_advantage %$% p_val_to_less_than(boot_p)`$).

```{r by-vowel-acc-group-known, fig.width=10, fig.height=5, fig.cap='Probability of correct recognition varies across vowels, overall and according to the socio-indexical grouping variable. Bars and stars show significant improvement from conditioning on dialect, above marginal or in addition to sex alone.'}

acc_by_vowel <- accuracy %>%
  filter(contrast == 'Vowels') %>%
  group_by(cues, group_is, grouping, subsample_size, Vowel, Talker) %>%
  summarise_each(funs(mean), accuracy)

acc_by_vowel_summary <-
  acc_by_vowel %>%
  do({ mean_cl_boot(.$accuracy) }) %>%
  rename(accuracy = y,
         accuracy_lo = ymin,
         accuracy_hi = ymax)

acc_by_vowel_summary %>%
  ggplot(aes(x=grouping, y=accuracy, color=grouping)) +
  geom_pointrange(aes(ymin=accuracy_lo, ymax=accuracy_hi),
                  position = position_dodge(w=0.7)) +
  facet_grid(cues~Vowel) +
  labs(y = 'Probability of correct recognition') +
  geom_stars(dialect_advantage_by_vowel_boot_ci,
             acc_by_vowel_summary) +
  geom_bars(dialect_advantage_by_vowel_boot_ci,
            acc_by_vowel_summary) +
  theme(axis.text.x= element_blank())

```

The utility of socio-indexical grouping for recognizing individual vowels
largely mirrors the overall pattern (Figure {@fig:by-vowel-acc-group-known}). As
with informativity, the utility of dialect varies substantially from one vowel
to the next. For most vowels, conditioning on dialect makes little difference to
correct recognition. But for a few---particularly `ae` and `eh`---conditioning
on dialect increases accuracy by nearly 10%. In the case of normalized formant
input, accuracy using dialect-conditioned distributions actually equals or
surpasses the accuracy with talker-specific distributions.

The overall utility of dialect also varies substantially based on the talker's
actual dialect (Figure @fig:overall-acc-by-dialect). This is consistent with the
fact that dialects themselves vary in how much they deviate from both the norms
of Standard American English [@Clopper2005] and from the marginal cue
distributions in this dataset (Figure @fig:vowel-kl-by-dialect). Most notably, conditioning on dialect provides a consistent advantage for talkers from the North dialect region, on the order of 10%.

As with the cue distributions themselves in Study 1, this suggests that there
may be a differential advantage to tracking cue distributions for different
vowels, and hence listeners might be more sensitive to variation in some
categories than others. On the other hand, it is possible that listeners' degree
of flexibility depends on the _cue_ rather than the _category_ specifically, in
which case they would be predicted to track all cues sharing the same cue with
the same level of flexibility.

[^logodds]: These comparisons are also significant when using log-odds of
    correct recognition, rather than raw probabilities.

```{r overall-acc-by-dialect, fig.width=10, fig.height=5, fig.cap='The utility of socio-indexical variables varies across dialects. Dialect itself is particularly informative only for talkers from the Mid-Atlantic and North regions. Each line shows a single talker, to emphasize within-talker changes in accuracy with grouping level, and large points and confidence intervals show mean accuracy and bootstrapped 95% CIs over talkers.'}

pd <- function() position_dodge(w=0.7)

vowel_acc_by_talker <-
  vowel_accuracy %>%
  group_by(cues, grouping, group, Talker) %>%
  summarise(accuracy = mean(accuracy)) %>%
  left_join(nsp_vows %>%
              group_by(Talker, Dialect) %>%
              summarise()) %>%
  ungroup() %>%
  mutate(grouping=factor(grouping, levels=grouping_levels))

vowel_acc_by_talker_summary <-
  vowel_acc_by_talker %>%
  group_by(cues, Dialect, grouping) %>%
  do({ mean_cl_boot(.$accuracy) }) %>%
  rename(accuracy = y,
         accuracy_lo = ymin,
         accuracy_hi = ymax)

dialect_stars_and_bars <-
  make_stars_and_bars(dialect_advantage_by_dialect_boot_ci,
                      vowel_acc_by_talker_summary)

## TODO-paper: graph is confusing. separate panels better.

ggplot(vowel_acc_by_talker, aes(x=grouping, y=accuracy, color=grouping)) +
  geom_line(aes(group=Talker), color='grey90') +
  geom_pointrange(data=vowel_acc_by_talker_summary, stat='identity',
                  aes(ymin=accuracy_lo, ymax=accuracy_hi)) +
  facet_grid(cues ~ Dialect) +
  scale_y_continuous('Probability of correct recognition', limits=c(0,1)) +
  geom_bars(dialect_advantage_by_dialect_boot_ci,
            vowel_acc_by_talker_summary) +
  geom_stars(dialect_advantage_by_dialect_boot_ci,
             vowel_acc_by_talker_summary) +
  rotate_x_axis_labs()

```


## Study 3: Inferring socio-indexical variables from cue distributions

Finally, we assess how well listeners would be able to determine values of
socio-indexical variables based on cue distributions alone. Qualitatively, a
listener should be able to determine a talker's socio-indexical group based on
phonetic cues to the extent that the distributions of those cues differ across
groups. Here we quantify this using the same computational tools that we used to
quantify the utility of socio-indexical grouping for speech perception.
Specifically, we ask how well a listener could _infer_ a talker's group, based
on the distributions of cues produced by other talkers in the same and different
groups.

### Methods {#sec:soc-infer-methods}

The process of inferring socio-indexical group parallels the process of
inferring phonetic categories: the posterior probability that a talker belongs
to a particular group is proportional to the likelihood of the cues they produce
under that group's cue distributions, relative to the total likelihood under all
groups' distributions.

We formalize this with the same kind of ideal observer model used for phonetic recognition. That is, we compute the posterior probability of each socio-indexical group $g=j$, given all of the talker's observed cue values $x$:
$$
p(g | x) \propto p(x | g) p(g) = \left(\prod_i p(x_i | g) \right) p(g)
$$

<!-- TODO: why marginalize here but not for unknown talker/group? -->
The only complication is that, without knowing the the phonetic category of each observation _a priori_, each observation may have been generated by any of the phonetic categories. Thus, to determine the _overall_ likelihood of observing a cue value $x_i$ under group $g$, we first have to marginalize over categories $v_i$:
$$
p(x_i | g) = \sum_k p(x_i | v_i=k, g) p(v_i=k | g)
$$

We perform this analysis separately for each level of socio-indexical
grouping. For instance, for each talker we compute both $p(\mathrm{sex} | x)$
and $p(\mathrm{dialect} | x)$.  We assess whether accuracy is significantly
above chance using a binomial test, rather than bootstrapping as in the previous
studies, because accuracy is measured at the subject (rather than token) level
and thus follows a (non-hierarchical) binomial distribution.  We also computed
the bootstrapped $p$ values and confidence intervals, and they did not differ
substantially from the exact ones.  Note that chance level accuracy depends on
the number of groups, and this varies by socio-indexical variable.


```{r indexical-classification, cache=TRUE}

## have to re-train models without subsampling
vowel_index_class <-
  vowel_data_grouped %>%
  filter(! grouping %in% c('Marginal', 'Talker')) %>%
  mutate(trained = map2(data, grouping, train_models_indexical_with_holdout),
         posteriors = map(trained, classify_indexical_with_holdout)) %>%
  unnest(map2(posteriors, grouping, ~ rename_(.x, group = .y)))

vot_index_class <-
  vot_by_place_grouped %>%
  filter(! grouping %in% c('Marginal', 'Talker')) %>%
  mutate(trained = map2(data, grouping,
                        ~ train_models_indexical_with_holdout(.x,
                                                              .y,
                                                              category='voicing',
                                                              formants='vot')),
         posteriors = map(trained, classify_indexical_with_holdout)) %>%
  unnest(map2(posteriors, grouping, ~ rename_(.x, group = .y))) %>%
  group_by(cues, grouping, Talker, group, model) %>%
  aggregate_log_lhood('log_posterior') %>%
  normalize_log_probability('log_posterior')

```

```{r indexical-accuracy, cache=TRUE, dependson=c('indexical-classification')}

vowel_index_acc <-
  vowel_index_class %>%
  filter(as.logical(posterior_choice)) %>%
  mutate(accuracy = group == model)

vot_index_acc <-
  vot_index_class %>%
  filter(posterior_choice) %>%
  mutate(accuracy = group == model)

index_acc <-
  bind_rows(vowel_index_acc %>% mutate(contrast = 'Vowel'),
            vot_index_acc %>% mutate(contrast = 'Stop voicing')) %>%
  mutate(grouping = factor(grouping, levels = grouping_levels))

## compute chance accuracy
index_chance_acc <-
  bind_rows(vowel_index_class, vot_index_class) %>%
  mutate(grouping = factor(grouping, levels = grouping_levels)) %>%
  group_by(cues, grouping, group) %>%
  summarise() %>%
  summarise(chance = 1/n())

## bootstrapped CIs
index_acc_summary <-
  index_acc %>%
  left_join(index_chance_acc) %>%
  group_by(cues, grouping) %>%
  do({ boot_ci(.$accuracy, function(d,i) mean(d[i]), h0=.$chance[1]) })

## exact binomial CIs (kosher because talker accs are single binary observations)
binomial_ci <- function(p, x) qbeta(p, sum(x)+0.5, length(x)-sum(x)+0.5)
binomial_p  <- function(prob_0, x) pbinom(sum(x), length(x), prob=prob_0,
                                          lower.tail=FALSE)
index_acc_summary_exact <-
  index_acc %>%
  group_by(contrast, cues, grouping) %>%
  summarise_each(funs(mean,
                      ci_low=binomial_ci(0.025, .),
                      ci_high=binomial_ci(0.975, .),
                      sum_acc=sum,
                      n_acc  =length),
                 accuracy) %>%
  left_join(index_chance_acc) %>%
  mutate(p_binom = pbinom(sum_acc, n_acc, chance, lower.tail=FALSE),
         stars_binom = p_val_to_stars(p_binom))

```

```{r indexical-acc-plot, fig.width=8.25, fig.height=3.5, fig.cap="Probability of correctly classifying a talker's socio-indexical group varies with the grouping variable, contrast, and cues. A talker is correctly classified if the overall posterior probability of their actual group given their unlabeled productions is the highest of all groups."}

index_acc_summary_exact %>%
  left_join(index_chance_acc) %>%
  ggplot(aes(x=grouping, y=mean)) +
  geom_point(aes(shape='Observed', color=grouping), size=3) +
  geom_linerange(aes(ymin=ci_low, ymax=ci_high, color=grouping)) +
  geom_point(aes(y=chance, shape='Chance', color=grouping), size=3) +
  geom_text(aes(y=ci_high, label = stars_binom), nudge_y = 0.02) +
  facet_grid(.~contrast + cues, scales='free_x', space='free_x') +
  ylim(c(0,1.02)) +
  scale_shape_manual('', values = c(1, 16), breaks = c('Chance', 'Observed')) +
  labs(x = 'Grouping',
       y = 'Probability of correct classification\n(Socio-indexical group)') +
  rotate_x_axis_labs()

```

### Results {#sec:soc-infer-results}

For many groupings, it is possible to infer each talker's group with above chance accuracy, given only that talker's unlabeled observed cues (Figure {@fig:indexical-acc-plot}; all $p<0.01$, except for inferring a talker's sex based on their VOT distributions, and dialect from un-normalized F1xF2 distributions, both $p>0.15$). Surprisingly, despite the fact that VOT distributions do not differ markedly by age (cf. Figure @fig:vowel-vot-kl-plot), it is still possible to infer a talker's age based on VOT with above chance accuracy (`r index_acc_summary %>% filter(cues=='VOT', grouping=='Age') %>% format_advantage(paren=FALSE)`).

In some respects, these results mirror the patterns of informativity about the cue distributions themselves (Figure {@fig:vowel-vot-kl-plot}). Vowel formant distributions vary more according to group than do stop VOT distributions, and likewise socio-indexical group can, on the whole, be inferred with higher accuracy based on vowel formants than on VOT. For vowels specifically, much of the variability across talkers is driven by sex differences, and this is the grouping variable that's easiest to infer of the three we tested.

However, in other respects, these results do _not_ simply mirror informativity. For instance, un-normalized F1xF2 distributions diverge from marginal more for dialect+sex than they do for sex alone, but accuracy at inferring a talker's sex is nearly at ceiling, while accuracy is barely above chance for inferring a talker's dialect+sex. Likewise, normalized F1xF2 distributions given dialect and dialect+sex diverge from marginal less than non-normalized, but accuracy at inferring these two grouping variables is higher for normalized than non-normalized.

Why the discrepancy? The informativity measure we used was the average
divergence of _each category's_ cue distribution. For inferring indexical
groups, we assumed that listeners do not know the intended category of each
observation, and so the relevant likelihoods are each based on a _mixture_ of
the category-specific distributions. Even if there are some categories whose
individual distributions diverge across groups, the overall mixture distribution
of all categories may still be too similar to allow for the group to be reliably
inferred.

## General Discussion

Our results show that, on the whole, socio-indexical grouping variables are
_informative_ about phonetic cue distributions, _useful_ for improving speech
recognition, and _inferable_ from phonetic cues themselves. The
extent to which these are true depends on the particular grouping variable and
particular phonetic categories/cues involved. At least for our American English data, socio-indexical variables are more
useful, more informative, and more easily inferred for vowels than for stop
consonant voicing. Some variables are broadly useful (sex, talker identity)
while others are useful only in certain, specific contexts (dialect for certain
vowels/dialect combinations).

Our results also speak to the relationship between informativity, utility, and
inferability themselves. In general, informativity and utility mirror each
other: conditioning on a socio-indexical variable is more useful for speech
recognition when the corresponding conditional cue distributions diverge more
from the overall or marginal distributions. But being useful for speech
recognition does not always mean that a socio-indexical variable can be easily
inferred from phonetic cues alone, or vice-versa.

In the remainder of this paper, we discuss the implications of these results. First, the ideal adapter
generally predicts that listeners should track conditional distributions for
groups that are informative and useful for speech recognition. By directly
quantifying the utility and informativity of a number of grouping variables, our
results are an important step towards making more specific predictions about
what group-level representations listeners should maintain if, as assumed by the
ideal adapter, they are taking advantage of the structure that is actually
present in cross-talker variability. Second, our results shed light on
discrepancies between phonetic contrasts in listeners' willingness to generalize
recalibration/perceptual learning from one talker to another. Third, this paper
provides a proof of concept for the idea that, like phonetic judgments,
socio-linguistic judgments can be productively viewed as a sort of inference
under uncertainty. This suggests the potential for a tighter integration of
sociolinguistic and psycholinguistic perspectives on speech perception.


### What to track?

<!-- this is the major discussion point. --> 

<!-- TODO: clarify how this connects. gentler intro -->

Treating speech perception as a problem of inference under uncertainty---as the
ideal adapter does---highlights the importance of listeners' knowledge about the
distributions of cues that are produced for each linguistic unit.  A major
question that this perspective raises is _what_ linguistic, socio-indexical, and
acoustic/phonetic variables listeners are learning distributions for. The ideal
adapter does not directly answer this question, but provides a set of conceptual
and quantitative tools for addressing it. The studies reported here take these
tools and apply them to data on how many different talkers produce two different
sets of phonetic categories. We hope that by doing so we provide a proof of
concept for the broad usefulness of these tools. One purpose they might be put
to is to formulate hypotheses about what distributions listeners should track.

Even prior to taking into account cognitive limitations, the ideal adapter predicts that listeners should not track
_everything_. Rather, listeners need only track the joint distributions of
variables that are informative. At the level of phonetic categories themselves,
this means that (for instance) there's no reason for listeners to track each
vowel's distribution of preceding VOT[^vot-caveat] (or more absurdly, completely
unrelated physical quantities like temperature or barometric pressure). This
also applies at the level of socio-indexical grouping variables: listeners get
no benefit for tracking separate distributions for different groups of talkers
for a cue that does not systematically vary between those groups.

[^vot-caveat]: Barring, of course, the possibility that VOT is systematically
    affected by neighboring vowels, and hence informative about them.

In fact, it can actually _hurt_ a listener to track cue distributions at a level
that's not informative. The reason for this is related to the idea of
bias-variance tradeoff from machine learning [@James2013, Section 2.2.2]. Given
the same amount of data, tracking multiple, specific distributions will result
in noisier, less accurate estimates than lumping together all the observations
in a single distribution.  This price may be worth paying for a listener when
there are large enough differences between groups that treating all observations
as coming from the same distribution _biases_ the estimates of the underlying
distribution (and hence the inferences that listeners make based on those
distributions) far enough away from the true structure of the data. To take a
concrete example, modeling each vowel as a single distribution of
(un-normalized) formants across all talkers results in high-variance,
overlapping distributions which have low recognition accuracy. But modeling them
as two distributions---one for males, and one for females---provides more
specific estimates and higher classification accuracy, as shown by Figures
@fig:vowel-vot-kl-plot and @fig:overall-accuracy-group-known
[and in @Hillenbrand1995; @Feldman2013a].

Thus, the ideal adapter predicts that listeners should learn separate cue
distributions for levels of a socio-indexical grouping variable when that
variable has high _informativity_ about some categories' cue distributions
and/or high _utility_ for speech recognition.  However, the notions of
informativity and utility apply beyond better _speech_ recognition per
se. Listeners extract a lot of non-phonetic/linguistic information from speech
signal. To properly define the informativity or utility of a particular grouping
variable, we need to consider the _goals_ of speech perception, which go beyond
just recognizing phonetic categories [or linguistic representations at any particular level, cf. @Kuperberg2016a]. Sociolinguistics recognizes that, in many
cases, the communication of social information is just as---if not more---important
than the communication of linguistic information. Groupings that are _socially
meaningful_ can thus be informative and justify being tracked, even if ignoring
them has a negligible effect on speech recognition, as long as the corresponding
cue distributions carry some information about relevant social variables. In our
results, dialect is a good example: on the whole, ignoring dialect doesn't have
huge consequences on recognition accuracy. But it can be inferred
based on vowel F1 and F2, and listeners are plausibly interested
in determining a talker's regional origins for a variety of reasons. <!--
unlearning southern accents? -->
<!-- TODO: extend to less stable social categories? -->
<!-- TODO-paper: connect more with socio literature and theory -->

An additional consideration is that listeners are not simply told which
variables are informative and which are not. They must actually _infer_ what
distributions are actually worth tracking. Moreover, every listener's experience
with talker variability will be different, and so a variable that is informative
in one listener's experience may be irrelevant in another's. While the analyses
we present here go a long way toward focusing the predictions of the ideal
adapter framework, they must be combined with knowledge of each listener's own
personal history---either assumed, or somehow measured, even approximately---in
order to make specific predictions for a particular subject or population of
subjects. This same logic applies to which socio-indexical variables are of
direct interest to a listener: social categories that are highly important in
one person's social world may be completely meaningless in another's. An
important aspect of the research program laid out by the ideal adapter framework
is to probe listeners' prior beliefs _directly_ (which the previous chapter is a
first step towards) <!-- TODO-paper: remove "chapter" for TopiCS -->

It is important to note that listeners' associations between linguistic and
socio-indexical variables are not always based on _objective_ informativity of
those variables. Rather, some variants can become disproportionately _salient_
or _enregistered_, can further vary based on context
[@Eckert2012; @Podesva2001; @Podesva2007; @Foulkes2015a; @Levon2014].
<!-- TODO: insert W&J salience ref here -->
These deviations between objective informativity and subjective salience remain
to be explained and specified in more detail.  Our work here provides a set of
tools for assessing objective informativity, a critical first step in
understanding this relationship. More generally, by treating listeners'
decisions about which socio-indexical and linguistic variables to jointly track
as a form of _inference_ (like linguistic and socio-indexical judgments
themselves), this perspective provides a framework for testing different ways
that listeners' experience and beliefs shape their subjective notions of
socio-indexical associations with linguistic variables.


Finally, our results suggest that the input representation---the cue space over
which categories are distributions---can affect which variables are informative
or not. For vowels, using Lobanov-normalized formants as input substantially
reduces the informativity and utility of sex as a grouping factor, but
_increases_ the utility of dialect in many cases. From a listener's perspective,
dialect would appear to be relatively uninformative without normalization. This
points to a complex interaction between normalization and adaptation/perceptual
learning as strategies for coping with talker variability. Both strategies are,
in fact, used by listeners, but the interaction between them is poorly
understood [@WeatherholtzInPress].

### Consequences for adapting to unfamiliar talkers

The results of this study also tell us something about how listeners might adapt
to an unfamiliar talker. The ideal adapter links informativity to adaptation,
and the results here allow us to make more specific predictions based on the
ideal adapter, in two ways.

First, the informativity of talker identity is a measure of the variability
across talkers. When talker identity is highly informative, there's more
variability across talkers, and the ideal adapter predicts that prior experience
with other talkers will be less relevant, resulting in faster and more complete
adaptation to an unfamiliar talker. We found here that talker identity is less
informative about VOT distributions than it is for vowel formant
distributions. Hence, the ideal adapter predicts that listeners will adapt to
talker-specific VOT distributions more slowly, and be more constrained by prior
experience with other talkers. The first prediction is borne out by
@Kraljic2007, who compared recalibration of a voicing contrast with a fricative
place contrast. It's also borne out indirectly by the modeling work in Chapters
\ref{chap:ideal-adapter} and \ref{chap:infer-priors}, which found that the effective prior sample size for /b/-/d/
(which, like vowels, is primarily cued by formant frequencies) is much lower than
for /b/-/p/ (cued by VOT).
<!-- TODO-paper: remove chapter refs -->

Second, the informativity of higher-level grouping variables is linked to
_generalization_ across talkers: if two talkers are from groups that tend to
differ, listeners should treat them separately and not generalize from
experience with one talker to the other. Likewise, if two talkers are from the
same group, listeners _should_ generalize. We found that talker sex is
informative for vowel formant distributions, but not for VOT, which means that
listeners _should_ generalize from a male to a female talker (and vice-versa)
for a voicing contrast, but _not_ for a vowel contrast. Listeners do, in fact,
tend to generalize voicing recalibration across talkers of different sexes
[@Kraljic2006;@Kraljic2007]. While there's (to our knowledge) no data on
cross-talker generalization for vowel recalibration, listeners tend not to
generalize across talkers for fricative recalibration
[@Eisner2005;@Kraljic2007], which (like vowels) are cued by spectral cues that
vary across talkers and by gender [@Newman2001; @Jongman2000; @McMurray2011a].

There is also evidence for the prediction of generalization _within_ informative
groups. In the absence of evidence that two talkers from the same group
(e.g. two males) produce a contrast differently, experience with one provides an
informative starting point for comprehending (and adapting to) the other. Along
these lines, @VanderZande2014 found that listeners generalize from experience
with one male talker's pronunciation of a /b/-/d/ contrast to another,
unfamiliar male.

Finally, it's important to point out that these predictions are best thought of
as _biases_ that might be overcome with enough of the right kind of evidence
[@Kleinschmidt2015]. For instance, listeners can overcome their bias to
generalize experience with VOT and learn talker-specific VOT distributions, but
it requires hundreds of observations from talkers who produce very different VOT
distributions [@Munson2011]. Likewise, listeners will generalize recalibration
of a fricative contrast from a female to a male talker given the right kind of
test stimuli [@Reinisch2014].

### Sociolinguistic inference

Our approach characterizes socio-linguistic judgments---like linguistic
judgments---as probabilistic inference.[^inference-empirical] In this view, both social and
linguistic judgments rely on knowledge of how different underlying
categories---social and linguistic---are probabilistically realized as
distributions of observable cues. Just like each vowel (for instance) is
realized as a distribution of F1 and F2 values, each dialect is _also_ realized
as an F1xF2 distribution (along with many other cues). When a listener hears a
talker produce particular cue values, they can use knowledge of these
distributions to compare how well each possible underlying social variable
_explains_ the speech they've observed.  We find that this kind of model can
classify a talker's dialect at roughly the same accuracy (10-40%) as human
listeners in a forced-choice task based on sentences spoken by the same talkers
[@Clopper2006].

[^inference-empirical]: It is ultimately an empirical question whether this
    perspective is productive in understanding how listeners process
    language. At the very least, we hope that the formal and computational tools
    are helpful in formulating and test precise hypotheses about linguistic
    and socio-indexical judgments.

The idea of socio-linguistic judgments of inference fits naturally within the ideal adapter framework, which holds that listeners are simultaneously making at least three kinds of inferences in the normal course of speech perception: 

1. _What_ a talker is saying
2. _How_ that talker says things
3. _Who_ that talker is, in relation to other talkers

The third level of inference is essential for talker-invariant speech
perception: knowing _who_ a talker is allows listeners to take advantage of
their prior experience with other, similar talkers [@Kleinschmidt2015]. Of
course, listeners likely also want to know who a talker is for reasons that have
nothing to do with accurate speech recognition per se [e.g., @Babel2014; @Pardo2006; @Foulkes2015a; @Eckert2012; @Labov1972]. To the extent that a
talker's way of realizing linguistic variables says anything about who they are
their speech is informative about their identity, at the same time as their
identity is informative about their speech. Thus both sociolingusitic and
psycholinguistic considerations lead to the idea that social inferences may well
be inextricable from linguistic inferences.

Realizing that socio-linguistic judgments can be treated as a kind of inference
is a potentially powerful idea, but it is important to realize that it is not,
per se, a complete _model_ of socio-linguistic judgments. Rather, it is a
framework for developing such a model. In this view, the particular inferences
that a listener would draw based on particular linguistic input depends not only
on the distributions of cues in the world but just as much on the listener's
own, internal model of how social variables relate to each other. Or, as it's
more commonly put, a listener's stereotypes or ideologies about language use and
social identity.

Careful sociolinguistic work is required to tease these factors out. One example
comes from @Levon2014. He finds that when UK listeners hear a male talker with
high /s/ spectral center of gravity (COG), they infer that the talker is a gay
man. But when they hear a male talker with high /s/ spectral COG _and_
TH-fronting (i.e., /f/ for /TH/), they judge the talker to be a working class
straight man. That is, the inference that the talker is working class _blocks_
the inference that he is gay. These sorts of effects are perfectly compatible
with an inference-based perspective, but they depend on the specific contents of
the listeners internal model of how social variables are related to each other
and to observable cues [for examples of similar blocking effects in other domains, see @Jacobs2010; and in speech perception, see @Kraljic2008a; @LiuInPrep2016]. Such
internal models are not directly derivable from production data like we analyze
here, but rather require probing a listener's subjective, implicit beliefs (as in
the previous chapter).


### A lower bound

Finally, it is important to note that our results here constitute a _lower
bound_ on the informativity or utility of different levels of socio-indexical
grouping.[^lower-bound-caveat] We model cue distributions for a particular group
as a _single_ normal distribution over observed cue values. In reality, a
hierarchical model is more appropriate, since different levels of grouping can
nest within each other. For instance, each dialect group is likely better
modeled as a _mixture_ of talker-specific distributions, which each exhibit
dialect features to a varying degree. This is especially important for
_adaptation_ to an unfamiliar talker, since a group-level distribution conflates
_within_ and _between_ talker variation, both of which have separate roles to
play in belief updating.

The approach to group-level modeling that we take here is roughly equivalent to
the _posterior predictive_ distribution of a fully hierarchical model, which
integrates over lower levels of grouping to provide a single distribution of
cues given the group (and phonetic category). This corresponds to the best guess
a listener would have _before_ hearing anything from an unfamiliar talker, if
the only information they had about that talker was their group membership. As
the listener hears more cue values from the talker, the hierarchical nature of
grouping structure becomes more important and can provide (in principle) a
significant advantage over what we measured here. But modeling this process is
quite a bit more complicated and we leave it for future work. Nevertheless,
modeling each category as a single, "flat" distribution per group may well prove
a useful approximation, or even a boundedly-rational model of how listeners take
advantage of different levels of grouping structure
[and similar approaches have been used in, e.g., motor control @Kording2007].

[^predictive-approx]: Depending on how variability at lower levels of grouping
    is modeled.

[^lower-bound-caveat]: Even above and beyond the limitations imposed by unequal
    numbers of talkers in each group, which necessitates subsampling talkers in
    the larger groups in order to meaningfully compare accuracy.

## Conclusion

Socio-linguistic variables like age, sex, and regional origin have been
identified by sociolinguistics as factors that systematically affect the
realization of linguistic categories. Using an ideal observer framework, we
quantified the extent to which a range of these variables are _informative_
about the distributions of acoustic cues corresponding to linguistic categories,
_useful_ for recognizing those categories, and can themselves be _inferred_ from
unlabeled cues. Our results show that the utility and informativity of a
particular socio-indexical variable are closely related but not identical, while
inferability is distinct.  Moreover, we demonstrate how this method for
quantifying these factors allows them to be compared across phonetic categories
as well as cues/contrasts (VOT vs. F1xF2).

Together, these results show that the idea of inference under uncertainty, when
applied to speech perception, provides a unifying perspective on both linguistic
and socio-linguistic perception. This perspective leads to conceptual and
computational tools for addressing questions that are of interest to
psycholinguistics and sociolinguistics, as well as developing new bridges
between the two.

