---
Title: What do you expect from an unfamiliar talker?
Author: Dave Kleinschmidt
bibliography: /Users/dkleinschmidt/Documents/papers/library-clean.bib
output:
    html_document:
        code_folding: hide
        dev: png
        keep_md: true
        md_extensions: +implicit_figures
        pandoc_args:
        - --filter
        - pandoc-fignos
        - --filter
        - pandoc-tablenos
    pdf_document:
        md_extensions: +implicit_figures
        pandoc_args:
        - --filter
        - pandoc-fignos
        - --filter
        - pandoc-tablenos
---

```{r preamble, warning=FALSE, message=FALSE, error=FALSE}

library(assertthat)
library(magrittr)
library(dplyr)
library(tidyr)
library(purrr)
library(stringr)
library(lme4)
library(ggplot2)

## devtools::install_github('kleinschmidt/daver')
library(daver)

## devtools::install_github('kleinschmidt/phonetic-sup-unsup')
library(supunsup)
## devtools::install_github('kleinschmidt/beliefupdatr')
library(beliefupdatr)

library(knitr)
opts_chunk$set(warning = FALSE,
               message = FALSE,
               error = FALSE,
               echo=opts_knit$get("rmarkdown.pandoc.to") != 'latex')

options(digits=2)

## Produce markdown-formatted figures so that pandoc knows what to do with
## the captions. requires pandoc-fignos to parse the IDs. refer to figures
## in text with {@fig:label} or just @fig:label
## 
## (see https://github.com/tomduck/pandoc-fignos)
knit_hooks$set(plot = function(x, options) {
  paste0('![', options$fig.cap, ']',
         '(', opts_knit$get('base.url'), paste(x, collapse='.'), ')',
         '{#fig:', options$label, '}')
})
## Produce markdown-formatted table captions with anchors for cross-refs.
## Requires pandoc-tablenos to parse the IDs. Refer to tables
## in text with {@tbl:label} or @tbl:label.
## Based partly on http://stackoverflow.com/a/18672268
##
## (see https://github.com/tomduck/pandoc-tablenos)
knit_hooks$set(tbl.cap = function(before, options, envir) {
  if(!before){
    paste0('\n\nTable: ', options$tbl.cap,
           ' {#tbl:', options$label, '}', sep = '')
  }
})

```

# What do you expect from an unfamiliar talker? Infering listeners' priors beliefs

## Introduction ##

A longstanding problem in speech perception is how listeners manage to
cope with substantial differences in how individual talkers produce
speech. Recent evidence suggests that one strategy listeners employ is
to *rapidly adapt* to unfamiliar talkers [@Bertelson2003; @Clarke2004; @Kraljic2007 among others]. Such adaptation can be
understood as a form of statistical inference. This insight is captured
by a recent proposal, the *ideal adapter* framework [@Kleinschmidt2015].
Each talker's particular accent (way of talking) can be formalized as
the distribution of acoustic cues that they produce for each phonetic
category (or other underlying linguistic unit). Listeners are taken to
adapt to an unfamiliar talker via *distributional learning*, inferring
the underlying talker-specific cue distributions from the talker's
productions.

Critically, this statistical inference process draws on implicit beliefs
about *how* talkers tend to differ from each other. As a consequence,
adaptation to an unfamiliar talker should depend on a listener's prior
experience with other talkers, rather than only on the speech produced
by the unfamiliar talker themselves. Specifically, a listener's
experience with other talkers provides the starting point for the
distributional learning required for adaptation, or, in Bayesian terms,
a *prior belief* about the probability of different possible accents
(cue distributions). More informative prior beliefs can substantially
reduce the amount of direct evidence needed to converge on accurate
beliefs about the current talker's cue distributions.

The goals of the present work are two-fold. First, we test a critical
prediction of the ideal adapter framework. To the extent that a
listener's prior beliefs are informative, they must take some
probability *away* from unlikely accents. Confronted by a talker whose
accent falls well outside the range of what they expect based on their
previous experience, the ideal adapter framework predicts that a
listener will require more evidence to adapt, leading to slowed or
incomplete adaptation. 

There is some evidence that this is the case. For instance, @Idemaru2011 found
that listeners have difficulty adapting to a talker who produces anti-correlated
distributions of two cues that are typically positively or un-correlated. 
Listeners had no trouble adapting to un- or positively correlated distributions 
of these cues.
<!-- idea here is to emphasize that they get as close as they can within the
limits of what talkers actually do. --> 

For instance, @Idemaru2011 tested how well listeners adapt to distributions of
two cues that distinguish voicing (e.g., /b/ vs. /p/), voice onset time (VOT)
and the pitch of the following vowel (f0).  These two cues are typically
positively correlated in English, with /p/ corresponding to high VOT and high
f0, and /b/ to low values of both cues. When exposed to a talker who produced an
accent with a positively correlated distribution of these cues, listeners used
f0 to categorize stops with ambiguous VOTs. Listeners also learned from a talker
who produced an _un_correlated distribution, ignoring f0 even for ambiguous
VOTs. But when exposed to a talker who produced an _anti_correlated
distribution, listeners did not adapt, instead ignoring f0, despite the fact
that f0 was just as informative for this accent as for the positively correlated
accent. This suggests that listeners have ruled out the possibility of a
reversed mapping between f0 and voicing (/b/ vs. /p/), possibly perhaps American
English talkers typically do not typically produce it [e.g., @House1953].
Likewise, @Sumner2011 found that listeners had trouble adapting to a talker who
produced VOT distributions for /b/ and /p/ that had substantially lower means
than a typical talker.

<!-- this might be too strong. both @Idemaru2011 and @Sumner2011 do in fact manipulate distributions parametrically -->
However, no studies have systematically probed whether and how a listener's
prior expectations constrain phonetic adaptation, or even what kind of prior
beliefs listeners have. To that end, in two experiments we expose listeners to a
range of different accents, which differ (only) in the distributions of voice
onset time (VOT), the primary cue to word-intial consonant voicing (e.g., /b/
vs. /p/).  By parametrically manipulating these distributions, we create a range
of accents that are more or less similar to what a typical talker of English
produces. We then assess the degree to which listeners adapt their beliefs about
the novel talker's cue distributions, depending on the *a priori* typicality of
these distributions.

To anticipate the results, we find that typicality of the novel talker's
cue distribution predicts the degree to which listeners adapt to the
talker. This suggests that listeners not only have beliefs about the cue
distributions for a *particular* single talker [as suggested by previous
work, @Clayards2008; @Feldman2009; @Kleinschmidt2015; @Kronrod2012]),
but also have implicit beliefs about the ways in which talkers tend to
*differ* from each other, and hence what to expect from an unfamiliar
talker. This leads to the second question we address here: what is the
content of listeners' prior beliefs about inter-talker variability? To
this end, we use a Bayesian belief-updating model to work backwards from
listeners' adaptation behavior across talkers and *infer* listeners'
shared prior beliefs. This approach provides a more direct assessment of
listeners' subjective prior beliefs than production data from many
different talkers, which moreover requires costly and time-consuming
annotation. If successful, this would provide a powerful method for
investigating listeners' prior beliefs that could be applied to other
cues, categories, and even social variables (like gender, native
language background, etc.).

We first present the design and results of Experiment 1. Then we use a belief
updating model to infer listeners' prior beliefs based on the results of that
experiment. Finally, we present the results of Experiment 2 which uses a wider
range of distributions, and assess how well the inferred prior beliefs predict
listeners' adaptation behavior in Experiment 2.

## Experiment 1 ##

```{r typical-talker}

prior_stats_by_talker <-
  votcorpora::vot %>%
  filter(source %in% c('gva13', 'bbg09', 'buckeye'),
         place == 'lab') %>%
  mutate(source = ifelse(source %in% c('gva13', 'bbg09'),
                         'goldricketal',
                         source)) %>%
  group_by(source, prevoiced, subject, phoneme) %>%
  summarise(mu = mean(vot),
            sigma2 = var(vot),
            sigma = sd(vot),
            n = n()) %>%
  rename(category = phoneme)

## plot single-talker distributions to get a sense of talker variability
## prior_stats_by_talker %>%
##   rename(mean=mu, sd=sigma) %>%
##   group_by(source, prevoiced, subject) %>%
##   by_slice(stats_to_lhood, xlim=c(-100, 100), noise_sd=0, .collate='rows') %>%
##   left_join(votcorpora::vot %>%
##               group_by(subject, prevoiced) %>%
##               tally() %>%
##               mutate(prop_prevoiced = n / sum(n))) %>% 
##   group_by(source, subject, category, vot) %>%
##   summarise(lhood=sum(lhood * prop_prevoiced)) %>%
##   ggplot(aes(x=vot, y=lhood, color=source, group=paste(subject, category))) +
##   geom_line()

prior_stats <-
  prior_stats_by_talker %>%
  filter(source == 'goldricketal', category == 'b') %>%
  group_by(source, prevoiced, category) %>%
  summarise_each(funs(mean, var, sum), mu, sigma2, sigma, n) %>%
  transmute(category,
            mean = mu_mean,
            sd = sqrt(sigma2_mean),
            n = n_sum) %>%
  bind_rows(supunsup::prior_stats %>%
              filter(source=='kronrod2012') %>%
              mutate(n = 1))

prior_lhood <- 
  prior_stats %>%
  filter(source == 'kronrod2012') %>%
  supunsup::stats_to_lhood()

prior_class <- prior_lhood %>% lhood_to_classification()

```

```{r expt1-data}

data_exp1 <- supunsup::supunsup_clean %>%
  filter(supCond == 'unsupervised') %>%
  mutate(trueCat = respCategory,
         subjNum = as.numeric(factor(subject)),
         trueCatNum = as.numeric(trueCat),
         respCatNum = as.numeric(respCat))

```


```{r vot-dists-exp1, fig.width=8, fig.height=2, fig.cap="Each subject heard one of these five synthetic accents, which differ only in the distribution of VOTs of the word-initial stops. Black dashed lines show VOT distributions from a hypothetical typical talker [as estimated by @Kronrod2012]. Note that the 0 and 10ms shifted accents are reasonably close to this typical talker, while the -10, 20, and 30ms shifted accents deviate substantially."}

exposure_stats <- data_exp1 %>%
  group_by(bvotCond, category=trueCat) %>%
  summarise(mean=mean(vot), sd=sd(vot))

sd_noise = sqrt(82)

exposure_lhood <- exposure_stats %>%
  group_by(bvotCond) %>%
  do(supunsup::stats_to_lhood(., sd_noise))

data_exp1 %>%
  group_by(bvotCond, vot) %>%
  filter(subject == first(subject)) %>%
  tally() %>%
  ggplot(aes(x=vot)) +
  geom_bar(stat='identity', aes(y=n, fill=bvotCond)) +
  geom_line(data=prior_lhood, aes(y=lhood*1600, group=category),
            color="black", linetype=2) +
  geom_text(data=data.frame(bvotCond=-10), x = 10, y = 60,
            label = 'Typical Talker',
            color='black', hjust=0, vjust=0.3, size=3) +
  geom_text(data=data.frame(bvotCond=-10), x = 40, y = 50,
            label = 'Exposure\nTalker',
            color=hcl(h=15, c=100, l=65), hjust=0, vjust=0.8, size=3,
            lineheight=1) + 
  facet_grid(.~bvotCond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Frequency') +
  scale_fill_discrete('/b/ mean\nVOT') ## +
  ## theme(legend.position='none')

``` 

We tested the role that listeners prior expectations play in adapting to an
unfamiliar talker by exposing them to one of five synthetic "accents" (Figure
@fig:vot-dists-exp1). These accents differed only in the distribution of
voice onset time (VOT), the primary cue to word-initial stop consonant voicing
in English (e.g., "beach" vs. "peach"). Adaptation was assessed based on
listeners' classification function, or how they labeled each VOT as /b/ or /p/.

### Methods {#sec:methods}

#### Subjects {#sec:subjects}

```{r participants-exp1}

n_subj <- data_exp1 %>% group_by(subject) %>% summarise() %>% tally()

excluded <- supunsup::supunsup_excluded %>%
  filter(supCond == 'unsupervised') %>%
  group_by(subject) %>% 
  summarise() %>% 
  right_join(supunsup::excludes) %>%
  select(subject, exclude80PercentAcc, rank)

n_excluded <- nrow(excluded)
n_subj_repeat <- sum(!is.na(excluded$rank))
n_subj_bad <- sum(!is.na(excluded$exclude80PercentAcc))
n_both <- n_subj_repeat + n_subj_bad - n_excluded 

n_total <- n_subj + n_excluded

```

We recruited `r n_total` subjects via Amazon's Mechanical Turk, who were paid
\$2.00 for participation, which took about 20 minutes. We excluded subjects who
participated more than once ($n=`r n_subj_repeat`$) or whose accuracy at 0 and
70ms VOT---as extrapolated via a logistic GLM---was less than 80% correct 
($n=`r n_subj_bad`$; $n=`r n_both`$ for both reasons).
Excluded subjects were roughly equally distributed across conditions (maximum of
5 in 0ms /b/ VOT condition, and minimum of 1 in 20ms /b/ VOT condition). After
these exclusions, data from `r n_subj` subjects remained for analysis for
analysis.

#### Procedure ####

![Example trial display (beach/peach). Listeners first click on the
    green button to play the word, then click on one picture to indicate what
    they heard.](figure_manual/beach_peach.png){#fig:beach-peach}

Our distributional learning procedure is based on @Clayards2008. On each trial,
two response option images appeared, which corresponded to one of three /b/-/p/
minimal pairs (beach/peach, bees/peas, or beak/peak). Subjects then clicked on a
central button which played the corresponding minimal pair word, and then
clicked on the picture to indicate whether they heard the /b/ or /p/ member of
the minimal pair (Figure @fig:beach-peach). Subjects performed 222 of these
trials, evenly divided between the three minimal pairs.

Each trial's word was synthesized with a voice onset time (VOT) that was
randomly drawn from a bimodal distribution, with low and high VOT clusters
implicitly corresponding to /b/ and /p/, respectively. This distribution defined
the *accent* that the subject heard, and each subject was pseudorandomly
assigned to one of five accent conditions (Figure @fig:vot-dists-exp1). The cue
distributions for each of these accents implies a different /b/-/p/ category
boundary, and listeners' adaptation was evaluated by comparing their /b/-/p/
classification function (fitted by a logistic GLM) to the classification
function derived from the typical talker's cue distributions (corresponding to
no adaptation) and the exposure distribution (corresponding to maximal---but not
necessarily optimal---adaptation).

### Results and Discussion

```{r class-curves, fig.height=2, fig.width=8, fig.cap="Listeners' responses, smoothed with logistic functions (thin lines), compared with the classification functions expected based on a typical talker (no learning; dashed black lines) and complete adaptation to the exposure distributions (thick dashed colored lines). Listeners' actual category boundaries lie between the typical talker and exposure talker boundaries (see Table {@tbl:boundary-shift})."}

## generate predicted classification functions assuming Bayes-optimal classifier
## + noise

perfect_learning <- exposure_stats %>%
  group_by(bvotCond) %>%
  do(stats_to_lhood(.)) %>%
  lhood_to_classification

no_learning <- prior_lhood %>%
  lhood_to_classification

prior_bound <- no_learning %>%
  arrange(abs(prob_p - 0.5)) %>%
  filter(row_number() ==1) %$%
  vot

vot_limits <- data_exp1 %$% vot %>% range()

ggplot(data_exp1, aes(x=vot, y=respP, color=bvotCond)) +
  geom_line(aes(group=subject), stat='smooth', method='glm', 
            method.args=list(family='binomial'), alpha=0.2) +
  facet_grid(.~bvotCond) +
  geom_line(data=perfect_learning, aes(y=prob_p), group=1, linetype=2, size=1) +
  geom_line(data=no_learning, aes(y=prob_p), group=1, linetype=2, color='black') +
  geom_text(data=data.frame(bvotCond=-10),
            x = 30, y = 0, label = 'Typical\ntalker',
            size = 3.5, hjust=0, vjust = 0, color='black',
            lineheight=1) + 
  geom_text(data=data.frame(bvotCond=-10),
            x = 12, y = 1, label = 'Expo-\nsure',
            size = 3.5, hjust=1, vjust=1, color=hcl(15, c=100, l=65),
            lineheight=1, fontface='bold') + 
  geom_text(data=data.frame(bvotCond=-10),
            x = 90, y = 0.75, label = 'Actual\nlisteners',
            size = 3.5, hjust=1, vjust=1, color=hcl(15, c=100, l=65),
            lineheight=1) + 
  scale_x_continuous('VOT (ms)', limits = vot_limits) +
  scale_y_continuous('Probability /p/ response') + 
  scale_color_discrete('/b/ mean\nVOT')

```

```{r boundaries-exp1, cache=TRUE}

boundaries_exp1 <- data_exp1 %>%
  group_by(bvotCond, subject) %>%
  do({ glm(respP ~ vot, family='binomial', data=.) %>%
         broom::tidy() %>%
         select(term, estimate)
  }) %>%
  ungroup() %>%
  spread(term, estimate) %>%
  mutate(boundary = -`(Intercept)` / vot,
         ideal_boundary = as.numeric(as.character(bvotCond)) + 20,
         prior_boundary = prior_bound,
         prop_shift = (boundary-prior_boundary)/(ideal_boundary-prior_boundary))

boundary_summary_exp1 <- boundaries_exp1 %>%
  group_by(bvotCond) %>%
  summarise(median_shift_perc = round(100*median(prop_shift)),
            shift_text = paste(median_shift_perc, '%', sep=''),
            prop_between = mean(xor(boundary > ideal_boundary,
                                    boundary > prior_boundary))
            )

```


```{r boundary-violin-plots-exp1, fig.width=6.5, fig.height=4.5, fig.cap="Most listeners' individual boundaries fall between the boundaries implied by cue distributions from a typical talker and exposure talker. White points and CIs show mean and bootstrapped 95% CIs for mean boundary in each condition."}

ggplot(boundaries_exp1, aes(y = boundary, x = bvotCond, fill = bvotCond)) +
  ## this is an awful hack: plot the violins first to force continuous x
  geom_violin(color=NA,draw_quantiles=c(0.25, 0.5, 0.75), show.legend=FALSE,
              alpha = 0) +
  geom_segment(data = boundaries_exp1 %>%    # prior boundary
                 summarise(xmin = min(as.numeric(bvotCond))-0.5,
                           xmax = max(as.numeric(bvotCond))+0.5,
                           y = unique(prior_bound)),
               aes(x=xmin, xend=xmax, y=y, yend=y, fill=NA),
               color='black', linetype=2) + 
  geom_segment(aes(x=as.numeric(bvotCond)-.5, # exposure boundaries_exp1
                   xend=as.numeric(bvotCond)+.5,
                   y=ideal_boundary,
                   yend=ideal_boundary,
                   color=bvotCond),
               linetype = 2, size = 1,
               data = boundaries_exp1 %>% group_by(bvotCond, ideal_boundary) %>% summarise()) +
  geom_violin(color=NA,
              ## draw_quantiles=c(0.25, 0.5, 0.75),
              alpha = 0.5,
              show.legend=FALSE) +
  coord_flip() +
  geom_text(data=(boundaries_exp1 %>% filter(bvotCond==20) %>% head(n=1)),
            aes(y=ideal_boundary, color=bvotCond),
            ## x=3.5, y=41, 
            label='Exposure talker-\nspecific boundary',
            hjust = 0, vjust = 0,
            nudge_x = -0.5, nudge_y = 1) +
  geom_text(data=(boundaries_exp1 %>% filter(bvotCond==20) %>% head(n=1)),
            aes(y=prior_boundary), color='black',
            ## x=3.5, y=41, 
            label='Typical talker\'s \nboundary',
            hjust = 1, vjust = 0,
            nudge_x = -0.5, nudge_y = -1) +
  geom_pointrange(stat='summary', fun.data='mean_cl_boot', color='white') +
  theme(legend.position='none') +
  labs(x = 'Condition (Mean /b/ VOT)',
       y = 'Category boundary (ms VOT)') +
  scale_color_discrete()

```


Figure {@fig:class-curves} shows the classification functions for each
individual listener. In each accent, these classification functions tend to fall
in between the boundaries predicted by the typical talker distributions and the
boundaries implied by the exposure distributions.  Figure
{@fig:boundary-violin-plots-exp1} shows this directly, by plotting the
distribution of individual listeners' /b/-/p/ boundaries, relative to the
exposure and typical talker's boundaries.

We can quantify how much listeners shifted their category boundaries by the
percentage of the predicted shift in category boundary from the classification
function for the typical talker to the boundary implied by the input
distribution (Table {@tbl:boundary-shift}). A 0% shift corresponds to no
adaptation at all, while a shift of 100% corresponds to complete adaptation to
the exposure distributions, with no (remaining) influence of any prior beliefs.

In all conditions, the average shift percentage was between 0% and 100%
(except the 0ms shift condition, which is so close to the typical talker that
estimating the percentage is numerically unstable). More interestingly, the more
extreme conditions show less complete adaptation than the less extreme conditions.
Together, these results suggest that listeners' adaptation was constrained by
their prior expectations (given the finite amount of evidence they received
about the unfamiliar talker).  This provides qualitative evidence that listeners
combine their prior expectations with observed cue distributions in order to
rapidly adapt to unfamiliar talkers, as predicted by the ideal adapter framework
[@Kleinschmidt2015].


```{r boundary-shift, results='asis', tbl.cap="Percentage of boundary shift from typical talker to each exposure talker (see Figure {@fig:class-curves}), averaged over subjects with 95% bootstrapped confidence intervals.  0% shift corresponds to no adaptation at all, while 100% corresponds to perfect adaptation, ignoring any prior  beliefs. Typical and exposure talker boundaries were too close together to reliably determine boundary shift percentage in the 0ms condition."}

## bootstrapped boundary summary
boundaries_exp1 %>% 
  group_by(bvotCond) %>% 
  do(daver::boot_ci(., function(d,i) {mean(d$prop_shift[i])})) %>%
  mutate(observed = round(observed * 100),
         ci_lo = round(ci_lo * 100),
         ci_high = round(ci_high * 100)) %>%
  ungroup() %>%
  transmute(`/b/ mean VOT` = bvotCond,
            `Mean shift` = ifelse(bvotCond == 0,
                                  '---',
                                  sprintf('%d%%', observed)),
            `95% CI` = ifelse(bvotCond == 0,
                              '---',
                              sprintf('%d--%d%%', ci_lo, ci_high))) %>%
  knitr::kable(escape = FALSE)

```

## Inferring prior beliefs about talker variability {#sec:model}

Our second goal in this paper is to test whether it is possible to infer
listeners' prior beliefs about talker variability, based on their
patterns of adaptation to different accents. To that end, we use a
variant of a Bayesian belief-updating model that has previously provided
a good account of how listeners incrementally update their beliefs in
order to rapidly adapt to an unfamiliar talker @Kleinschmidt2015. This
previous modeling work has treated the content of listeners prior
beliefs---the category means and variances they think are most likely---as
known and fixed, setting them based on pre-adaptation classification
data, and then fitting the confidence in those prior beliefs as a free
parameter. Here, we wish to fit both the content of (prior expected mean
and variance of each category) and the confidence in prior beliefs,
based on adaptation data presented above.

### Methods {#sec:methods-model}

We denote the listener's beliefs about the *current* talker's generative
model as the parameters of a two-component mixture of gaussians
$$
\theta = \{ \mu_\mathrm{b}, \sigma^2_\mathrm{b}, \mu_\mathrm{p},
            \sigma^2_\mathrm{p} \}
$$ 
where $\mu$ is a category's mean VOT and $\sigma^2$ is its variance. As in
@Kleinschmidt2015, we use an independent, conjugate Normal-$\chi^{-2}$ prior for
each category, with parameters [@Gelman2003]

$$\begin{aligned}
\phi &= \{ \mu_{0,\mathrm{b}}, \sigma^2_{\mathrm{b}},
            \mu_{0,\mathrm{p}}, \sigma^2_{\mathrm{p}}, 
            \kappa_0, \nu_0 \} \\ 
\theta | \phi &\sim
    \prod_{c \in \{\mathrm{b,p}\}} 
    \mathrm{Normal} \left(\mu_c | \mu_{0,c}, \frac{\sigma_c^2}{\kappa_0}\right) 
    \chi^{-2} (\sigma_c^2 | \sigma^2_{0,c}, \nu_0)
\end{aligned}$$ 
where $\mu_{0}$ and $\sigma^2_0$ are a category's prior expected mean VOT and
variance, respectively, and $\kappa_0$ and $\nu_0$ are the listener's confidence
in these prior expectations, measured as pseudo-counts. Note that, as in
previous modeling work in this framework, these prior confidence parameters are
shared between the two categories. Preliminary simulations showed that it wasn't
possible to uniquely identify the model using separate prior confidence
parameters for the two categories.

To estimate the listeners' prior beliefs, we infer values for these
parameters given the observed adaptation behavior (category responses
$y$ and input VOTs $x$) using Bayesian inference, marginalizing over
$\theta$:

$$\begin{aligned}
  p(\phi | x, y) &\propto p(y | \phi, x) p(\phi) \\
                 &\propto \int \mathrm{d} \theta p(y | \theta, x) p(\theta | x, \phi)
                           p(\phi)\end{aligned}$$

We two simplifying assumptions. First, we assume that the order of the
trials does not matter. Second, we assume that listeners pick up on the
cluster structure of the input they receive, accurately detecting the
mean and variance of each cluster, but for categorizing they use their
*posterior* beliefs which combine their prior beliefs with these
observed statistics. 

At first blush, this seems to leave a lot of information in our dataset on the
table, since we don't consider listeners' own judgements about the category of
each observation when modeling how beliefs are updated. However, incorporating
these judgements into the belief updating part of the model itself is
surprisingly subtle and challenging. For instance, one seemingly obvious fix is
to treat listeners' responses themselves as the cateogry labels to be used for
belief updating (rather than the experimenter-defined categories). But doing
this eliminate the possibility that listeners could go back and re-evaluate
their previous judgements, which is anethema to a Bayesian belief updating
model, because from the models' point of view these labels are fixed.

It is, of course, possible to back off on this assumption in better ways, but
they are substantially more computationally demanding, and we leave it for
future work to determine how they might affect our results. 

Finally, we also add a lapse rate parameter (maximum a posteriori value of
$5%$), that allows for some proportion of responses to be attributed to random
guessing
[e.g., because of attentional blinks, see @Clayards2008 for a discussion].

```{r run-model, eval=FALSE}

## run the belief-updating model for inferring prior. the model source is in
## the beliefupdatr package, or will be soon :)
##
## devtools::install_github('kleinschmidt/beliefupdatr')

data_exp1_stan_conj <- data_exp1 %>%
  supunsup::supunsup_to_stan_conj()
  
library(rstan)
mod_lapsing <- beliefupdatr::compile_stan('conj_id_lapsing_fit.stan')

fit_lapsing <- stan(fit = mod_lapsing,
                    data = data_exp1_stan_conj,
                    chains = 4,
                    iter = 1000)

head(summary(fit_lapsing)$summary, n=10)

mod_summary <- summary(fit_lapsing)$summary
mod_samples <- rstan::extract(fit_lapsing)

saveRDS(mod_samples, file='models/samples_lapsing.rds')
saveRDS(mod_summary, file='models/summary_lapsing.rds')

```

```{r load-samples}

mod_samples <- readRDS('models/samples_lapsing.rds')

```

```{r load-summary}
mod_summary <- readRDS('models/summary_lapsing.rds')
```

```{r model-analysis-exp1, cache=TRUE}


## rename dimensions to make melting easier
rename_dims <- function(x, var, new_names) {
  names(dimnames(x[[var]])) <- new_names
  return(x)
}

mod_samples %<>%
  rename_dims('mu_0', c('iterations', 'cat_num')) %>%
  rename_dims('sigma_0', c('iterations', 'cat_num')) %>%
  rename_dims('mu_n', c('iterations', 'cat_num', 'subject_num')) %>%
  rename_dims('sigma_n', c('iterations', 'cat_num', 'subject_num')) %>%
  rename_dims('kappa_n', c('iterations', 'cat_num', 'subject_num')) %>%
  rename_dims('nu_n', c('iterations', 'cat_num', 'subject_num'))
  

max_Rhat <- max(mod_summary[, 'Rhat'])
lapse_rate <- mean(mod_samples$lapse_rate)

## correspondence between subject IDs and subject indices in model
subjects <-
  data_exp1 %>%
  mutate(subject_num = as.numeric(factor(subject))) %>%
  group_by(subject, bvotCond, subject_num) %>%
  summarise() %>%
  arrange(subject_num)

categories <-
  data_frame(cat_num = 1:2,
             category = c('b', 'p'))

# helper function to melt a mult-dimensional array of samples into a df
melt_samples <- function(samples, varname) {
  reshape2::melt(samples[[varname]], value.name=varname) %>%
    tbl_df
}

## create a data_frame with samples for prior parameters
prior_samples_df <- 
  c('mu_0', 'sigma_0', 'kappa_0', 'nu_0') %>%
  map( ~ melt_samples(mod_samples, .x)) %>%
  reduce(inner_join) %>%
  tbl_df %>%
  left_join(categories)

## create a data_frame with samples for updated parameters
updated_samples_df <- 
  c('mu_n', 'sigma_n', 'kappa_n', 'nu_n') %>%
  map( ~ melt_samples(mod_samples, .x)) %>%
  reduce(inner_join) %>%
  tbl_df %>%
  left_join(categories) %>%
  left_join(subjects) %>%
  group_by(bvotCond) %>%
  filter(subject == first(subject)) %>% # just need one per condition
  select(-cat_num, -subject_num, -subject)

## create a data_frame for lapsing rate samples
lapse_rate_samples <- melt_samples(mod_samples, 'lapse_rate')


```


The posterior distributions of each of these parameters (the shared
prior beliefs plus lapsing rate) were estimated using MCMC with the Stan
software package [@Stan2015]. Weakly informative hyperpriors were used
that were centered at 0 with standard deviations of 100 for the prior
expected means and variances (making them roughly constant over
reasonable values) and 888 (four times the total number of trials that
listeners heard) for the prior confidence pseudocounts (which is
essentially uniform on the whole range from completely ignoring prior
beliefs to not adapting at all). The prior for the lapsing rate was
uniform on $[0,1]$. We ran four chains for 1000 samples each, discarding
the first 500 as burn-in for a total of 2000 samples overall. This
sampler converged well and achieved good mixing [maximum
$\hat{R}= `r round(max_Rhat, 3)`$; @Gelman1992].

### Results ###

```{r model-goodness-of-fit}

mod_fitted <-
  data_exp1 %>%
  supunsup::supunsup_to_stan_conj() %$%
  z_test_counts %>%
  data.frame() %>%
  tbl_df() %>%
  mutate(prob_p = apply(mod_samples$p_test_conj[, , 2], 2, mean),
         prob_p_lapse = prob_p * (1-lapse_rate) + lapse_rate/2)

mod_goodness_of_fit <- 
  mod_fitted %>%
  mutate(LL_mod = dbinom(x = p, size = b+p, prob = prob_p_lapse, log = TRUE),
         LL_null = dbinom(x = p, size = b+p, prob = mean(p/(b+p)), log = TRUE)) %>%
  summarise(LL_mod = sum(LL_mod),
            LL_null = sum(LL_null),
            rho = cor(p/(b+p), prob_p_lapse, method='spearman'),
            n = n()) %>%
  mutate(LL_ratio = LL_mod - LL_null,
         pseudo_R2_mcfadden = 1 - LL_mod/LL_null,
         pseudo_R2_nagelkerke = (1 - exp(2/n * -LL_ratio)) / (1-exp(2/n*LL_null)))

```

```{r model-fit-classification, fig.width=8, fig.height=2, fig.cap="The classification functions (shaded ribbons, 95% posterior predictive intervals) predicted by the belief updating model fit listeners' responses well (dots with lines showing bootstrapped 95% confidence intervals)."}


## pick a random subset of iterations to do the MCMC integration for posterior
## predictive checks
some_iterations <- 
  updated_samples_df %>%
  group_by(iterations) %>%
  summarise() %>%
  sample_n(200)

## convert samples into distributions and then classification functions for each
## condition
mod_class_funs <- 
  updated_samples_df %>%
  right_join(some_iterations) %>%
  mutate(mean=mu_n, sd=sigma_n) %>%
  select(iterations, bvotCond, category, mean, sd) %>%
  group_by(iterations, bvotCond) %>%
  do(stats_to_lhood(., noise_sd=0)) %>%
  lhood_to_classification() %>%
  left_join(lapse_rate_samples) %>%
  mutate(prob_p = (1-lapse_rate)*prob_p + lapse_rate/2) %>%
  select(bvotCond, vot, prob_p) %>%
  group_by(bvotCond, vot) %>%
  summarise(prob_p_low = quantile(prob_p, 0.025),
            prob_p_high = quantile(prob_p, 0.975),
            prob_p = mean(prob_p))

data_by_subject <- data_exp1 %>%
  group_by(subject, bvotCond, vot) %>%
  summarise(prob_p = mean(respP))



## plot observed and model-predicted classification functions
ggplot(mod_class_funs, aes(x=vot, y=prob_p, color=bvotCond, fill=bvotCond)) +
  geom_ribbon(aes(ymin=prob_p_low, ymax=prob_p_high), size=0, alpha=0.5) + 
  geom_point(data = data_by_subject, stat='summary', fun.y='mean') + 
  geom_linerange(data=data_by_subject, stat='summary', fun.data='mean_cl_boot') + 
  facet_grid(.~bvotCond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Probability /p/ response') + 
  scale_color_discrete('/b/ mean\nVOT') + 
  scale_fill_discrete('/b/ mean\nVOT')


```

The first way we evaluate this model is to ask how well it fits listeners'
behavior. Figure {@fig:model-fit-classification} shows listeners' average
classification functions, compared with the posterior predictive classification
functions from the belief-updating model.  The first thing to notice is that the
model fits the data well (log-likelihood ratio vs. an intercept-only binomial
null model of $`r mod_goodness_of_fit[['LL_ratio']] %>% sprintf('%.1e', .)`$,
and Spearman's $\rho$ = `r mod_goodness_of_fit[['rho']]`, $p<10^{-10}$ in both
cases) capturing the different classification functions that result from
exposure to each input distribution.  This in and of itself is an interesting
result: it shows that there does exist some set of prior beliefs such that the
range of adaptation behavior we observed can be explained by a model where the
listeners assigned to the different accent conditions all start from a common
set of prior beliefs.


```{r inferred-prior, fig.width=6, fig.height=2.5, fig.cap="Expected cue distributions based on the prior beliefs inferred here from behavioral adaptation data. Plotted with VOT distributions measured by @Kronrod2012 based on a combination of classification and discrimination behavior, and from production data by @Goldrick2013 for /b/, including pre-voicing."}

prior_summary <- 
  prior_samples_df %>% 
  gather('stat', 'val', mu_0:sigma_0) %>% 
  unite(stat_cat, stat, category) %>% 
  select(-cat_num) %>% spread(stat_cat, val) %>%
  gather('stat', 'value', kappa_0:sigma_0_p) %>% 
  group_by(stat) %>%
  summarise(mean=mean(value), 
            low=quantile(value, 0.025), 
            high=quantile(value, 0.975)) %>%
  mutate(units = ifelse(str_detect(stat, '(kappa|nu)'), 
                        'observations', 
                        'ms VOT'))

prior_expected <-
  prior_summary %>%
  select(stat, mean) %>%
  spread(stat, mean) %>%
  map(round) %>%
  as_vector()


typical_talker_lhoods <- 
  prior_stats %>%
  group_by(source, prevoiced, n) %>%
  by_slice(stats_to_lhood, xlim=c(-100, 100), .collate='rows') %>%
  group_by(source, category, vot) %>%
  summarise(lhood = sum(lhood * n) / sum(n)) # combine prevoiced and non, weighted

## Plot prior vs. typical talker from various sources
prior_samples_df %>% 
  group_by(category) %>% 
  summarise(mean = mean(mu_0), sd = mean(sigma_0)) %>%
  stats_to_lhood(xlim=c(-100,100), noise_sd = 0) %>%
  ggplot(aes(x=vot, y=lhood, group=category)) +
  geom_line(data = typical_talker_lhoods %>%
              filter(source %in% c('goldricketal', 'kronrod2012')),
            aes(color=source, group=paste(source, category)),
            size = 1) +
  geom_line(aes(color='Inferred prior'), size=2) +
  scale_color_discrete('Source') +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Likelihood')

```

```{r inferred-prior-params, results='asis', tbl.cap="Expected values and 95% highest posterior density intervals for the prior parameters, given the adaptation data."}

format_params <- function(s) {
  s %>%
    str_replace('_0_([bp])$', '_{0,\\\\mathrm{\\1}}') %>%
    str_c('$\\', ., '$')
}

prior_summary %>% 
  transmute(Parameter = format_params(stat),
            Expected = mean,
            `95% HPD Int.` = sprintf('%.0f--%.0f', low, high),
            Units = units) %>%
  as.data.frame() %>%
  knitr::kable(escape=FALSE, digits=0)

```

The second way to evaluate this model is based on the prior beliefs it infers
listeners to have.  Table {@tbl:prior-params} shows the posterior expectation
and 95% highest posterior density intervals for each of the prior belief
parameters given the adaptation data above.  The behavioral data was consistent
with high confidence in prior beliefs with the prior confidence about category
variances ($E(\nu_0) = `r prior_expected['nu_0']`$) higher than confidence
in the category means ($E(\kappa_0) = `r prior_expected['kappa_0']`$).  Both
of these (measured in pseudo-observations) are larger than the number of trials
that listeners heard in the experiment (222), which means that as far as the
belief-updating model is concerned, listeners updated beliefs reflected their
prior beliefs as much as (in the case of the means) or more than (for the
variances) the distributions they actually observed.  This is consistent with
the qualitative finding that listeners' category boundaries are intermediate
between the boundaries corresponding to a typical talker and the experimental
exposure talker.

Figure {@fig:inferred-prior} shows the cue distributions corresponding to the
posterior expected values of the prior expected mean and variance parameters
given the behavioral data, compared with the distributions corresponding to one
typical talker [as determined by a combination of classification and
discrimination data, @Kronrod2012; see also @Lisker1964].  The
inferred prior beliefs are in reasonably good agreement with this typical
talker, with one exception: the mean for /b/ is slightly lower
($E(\mu_{0,\mathrm{b}}) = `r prior_expected['mu_0_b']`$ ms), and the
standard deviation of /b/ is slightly higher
($E(\sigma_{0,\mathrm{b}}) = `r prior_expected['sigma_0_b']`$ ms).

```{r prior-variance, eval=FALSE}

## Compare with actual variance across talkers

## devtools::install_bitbucket('hlplab/votcorpora')
prior_stats_variability <-
  votcorpora::vot %>%
  filter(source %in% c('gva13', 'bbg09', 'buckeye'),
         place == 'lab') %>%
  group_by(source, prevoiced, subject, phoneme) %>%
  summarise_each(funs(mu=mean,sigma2=var,sigma=sd), vot) %>%
  group_by(source, prevoiced, phoneme) %>%
  summarise_each(funs(mean, var), mu, sigma2, sigma)

## plot inferred prior vs. typical talker from different sources
prior_stats_variability %>%
  transmute(category = phoneme,
            mean = mu_mean,
            sd = sqrt(sigma2_mean)) %>%
  bind_rows(supunsup::prior_stats %>%
              filter(source=='kronrod2012'),
            prior_samples_df %>%
              group_by(category) %>%
              summarise(mean = mean(mu_0), sd = mean(sigma_0)) %>%
              mutate(source='inferred prior') ) %>%
  by_slice(stats_to_lhood, xlim=c(-100,100), noise_sd=0, .collate='rows') %>%
  group_by(source, category, vot) %>%
  summarise(lhood = mean(lhood)) %>%
  ggplot(aes(x=vot, y=lhood, color=source, group=paste(source, category))) +
  geom_line()

```

* how does model fit cross-talker variability?
    * not very well. much lower than actually predicted
    * problem is with the prior: mathematically convenient but at the price of
      linking cross-talker variability with within talker variability and
      confidence.
    * there's a more general problem though: ideal adapter predicts that
      _confience_ is still linked to the expected variability across talkers. so
      the high level of confidence required to explain the relatively strong
      constraints on adaptation we observed may be incompatible with the actual
      level of talker variability.
    * is that a problem? it may be the case that there's just not enough of the
      right kind of talker variability for listeners to track it across
      situations. that the cost of doing so might outweigh any benefits (see
      next chapter)
    * if that's true, then it suggests that there are interesting limits on
      adaptation that go beyond the in-principle, computational limitations that
      the ideal adapter proposes.
        * like: some kind of particle filter/digging in thing where previous
          judgements can't be revised after a certain point. how much
          uncertainty is maintained?


### Discussion ###

These modeling results show that the pattern of adaptation behavior observed
above is consistent with a belief-updating model of phonetic adaptation that
combines prior expectations with input statistics in order to infer the current
talker's cue distributions. Specifically, it shows that there exists a single
set of prior beliefs that captures the range of adaptation to different input
distributions that stretches from nearly complete adaptation to partial
adaptation at best.

These prior beliefs are reasonably consistent with other attempts to determine
what listeners think the underlying cue distributions are @Kronrod2012, as well
as the distributions produced by actual talkers @Lisker1964. In fact, the prior
expected VOT distribution for /p/ that our model inferred is almost identical to
that observed by both @Kronrod2012 and @Lisker1964. The distribution for /b/
deviates from prior work, however. One possible reason for this is that a
substantial minority of English speakers produce pre-voiced /b/ @Lisker1964,
which is characterized by a lower (negative) VOT and a higher variance (often
higher even than /p/). That is, across talkers, the /b/ VOT distribution
parameters (mean and variance) have a *bimodal* distribution. We assumed a
single, unimodal prior distribution, and the prior beliefs we inferred to be
most likely are consistent with a compromise between the two types of /b/
distributions that talkers actually produce. This possibility suggests two
directions for future work. First, large-scale corpus studies of VOT
distributions are needed to determine to what extent the distribution of /b/
mean VOTs is really bimodal across talkers. However, the only study of this type
we are aware of considers only non-negative VOTs @Chodroff2015 and thus excludes
talkers who pre-voice their /b/.  Second, more modeling work is needed to test
whether a multimodal prior is justified given the adaptation data, and if so,
whether it would change the inference about listeners' prior expectations for
/b/.

The other major result of this modeling is that listeners have high confidence
in their prior expectations about the VOT distributions of /b/ and /p/, acting
as if they had already observed around 200--800 samples from each category (for
the category means and variances, respectively) from the unfamiliar talker they
encountered in our experiment.

At first blush, this conflicts with previous work on another phonetic contrast,
/b/-/d/, which found confidence values that were one or two orders of magnitude
smaller than those inferred here [@Kleinschmidt2015].  Interestingly, the /b/-/d/
contrast is cued by spectral cues (formant frequency transitions) which
generally vary substantially across talkers e.g., @Peterson1952. The acoustic
cues to the /b/-/p/ contrast used in the current study do not show as much
variability across talkers [e.g., @Allen2003; @Chodroff2015]. When there is
little variability across talkers, past experience with other talkers' VOT
distributions is highly informative about the distributions that an unfamiliar
talker will produce, requiring less adaptation. Likewise, when there is more
variability across talkers, listeners need to rely more on the current talker's
cue distributions and less on their prior experience. Thus, the apparent
discrepancy between the confidence that listeners place in their prior beliefs
in the current study and in @Kleinschmidt2015 is actually in line with an *ideal
adapter* which combines prior beliefs with current experience weighted according
to confidence. This idea finds further empirical support in @Kraljic2007, who
found that after the same amount of exposure, listeners recalibrate a /d/-/t/
contrast (analogous to the /b/-/p/ contrast used here) much less than an /s/-S
contrast (where the latter exhibits larger variability across talkers; e.g.,
@Newman2001).


## Experiment 2 ##

The results of Experiment 1 show that listeners' adaptation to an unfamiliar
talker is _constrained_. Such constraints are predicted by the ideal adapter
framework, which says that listeners use their experience with how (in this
case) VOT distributions vary _across_ talkers to guide their adaptation to an
unfamiliar talker.  Our modeling results provide further evidence for this
explanation, and show that the constraints are consistent with belief updating
starting from a single set of prior beliefs about cross-talker variability in
VOT distributions.

In Experiment 2, we provide a further, more specific test of the ideal adapter
explanation. There are many ways that cue distributions can vary, and in
Experiment 1 we manipulated just one of them, shifting a single bimodal VOT
distribution up or down but leaving the overall shape the same. In this
experiment, we go one step beyond this, shifting the means of the /b/ and /p/
clusters semi-independently. This results in bimodal distributions that differ
both in the _overall_ mean VOT, and in the _separation_ of the /b/ and /p/
clusters (Figure @fig:exposure-dists-exp2).

The main goal of this experiment is to test the predictive power of the model
and prior beliefs it inferred listeners to have. Belief updating---starting from
these inferred prior beliefs---provides an excellent fit to listeners'
adaptation behavior across conditions in Experiment 1, but the real utility of a
model is its ability to predict behavior in new situations. The qualitatively
different set of input distributions we use in this experiment provide a
stringent test of the predictive utility of this framework.

<!-- MEH. we don't really address this by updating the modeling beliefs, right?
so why bother. --> The particular set of distributions we chose was informed by
the particular way that the model fit the data. In particular the model's
pattern of _uncertainty_ about the joint combination of category means and
variances given the data revealed that our data was more informative about the
category boundary location implied by the prior cue distributions (which is
largely determined by the midpoint between the /b/ and /p/ prior means) than
about the separation between the two clusters. By varying the separation between
the clusters we expect to gain more information that can further constrain the
inferred beliefs, as well as testing how much the uncertainty inherent in the
structure of the first experiment affects the predictive power of the model.

Another, secondary goal is to test how listeners adapt to large, negative
VOTs. The inferred prior beliefs about the /b/ VOT distribution of a typical
talker have higher variance than the short-lag distribution generally considered
to be typical of American English, but are consistent with a comprpomise between
short-lag and prevoiced distributions.  One way to determine whether listeners
consider prevoiced and short-lag VOT distributions are a single cluster or two
different clusters is to directly compare adaptation to short-lag and prevoiced
distributions, which informs the specific choice of distributions we used in
this second experiment. In the short term, this allows us to assess how well the
model's inferred prior beliefs generalize to adaptation to prevoiced
distributions. In the long term, it provides data to directly compare the
current model (that treats /b/ as a single VOT cluster) to a model that treats
/b/ as a mixture of short-lag and prevoiced clusters. Such a model is a
conceptually straightforward---but computationally substantially more
complex---extension of the current model.

```{r separatemeans-data}

sepmeans <- supunsup::separatemeans_clean

sepmeans_conds <-
  sepmeans %>%
  group_by(bvotCond, pvotCond) %>%
  summarise() %>%
  ungroup() %>%
  arrange(bvotCond, pvotCond) %>%
  mutate(vot_cond = paste(bvotCond, pvotCond, sep=', '),
         vot_cond = factor(vot_cond, levels=vot_cond))

sepmeans_stats <-
  sepmeans %>%
  filter(!is_test) %>%
  left_join(sepmeans_conds) %>%
  group_by(bvotCond, pvotCond, vot_cond, trueCat) %>%
  summarise(mean = mean(vot), sd = sd(vot)) %>%
  rename(category = trueCat)

sepmeans_exposure_lhood <-
  sepmeans_stats %>%
  group_by(bvotCond, pvotCond, vot_cond) %>%
  do({stats_to_lhood(.)})

sepmeans_exposure_class <-
  sepmeans_exposure_lhood %>%
  do({lhood_to_classification(.)})

sepmeans_test <-
  sepmeans %>%
  filter(is_test) %>%
  left_join(sepmeans_conds)

```

### Methods

```{r exposure-dists-exp2, fig.width=10, fig.height=2, fig.cap="In Experiment 2, each subject heard a talker that produced one of these five VOT distributions. The variance of each category was constant across conditions, but the means varied semi-independently."}


sepmeans %>%
  left_join(sepmeans_conds) %>%
  filter(!is_test) %>%
  group_by(vot_cond, vot) %>%
  filter(subject == first(subject)) %>%
  tally() %>%
  ggplot(aes(x=vot)) +
  geom_bar(stat='identity', aes(y=n, fill=vot_cond)) +
  geom_line(data=prior_lhood, aes(y=lhood*1600, group=category),
            color="black", linetype=2) +
  facet_grid(.~vot_cond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Frequency') +
  scale_fill_discrete('/b/, /p/\nmean VOT')

```

As in Experiment 1, listeners performed a distributional learning task, hearing
a talker who produced one of five different bimodal VOT distributions. Across
these distributions, /b/ and /p/ mean VOT varied semi-independently (Figure
{@fig:exposure-dists-exp2}). In particular, they covered a wider range of
implied category boundaries (from -15 to 45ms VOT) than Experiment 1 (10 to
50ms), especially on the lower end. They also varied in the distance between
clusters (from 40 to 130ms VOT), unlike Experiment 1 which maintained a constant
separation between /b/ and /p/ mean VOT of 40ms. Note that the condition with
/b/ mean of 10ms and /p/ mean of 50ms VOT is the same as in Experiment 1.

#### Subjects

```{r exp2-subjects}

n_excl2 <- supunsup::separatemeans_excluded %>%
  left_join(sepmeans_conds) %>%
  group_by(vot_cond, subject) %>%
  summarise() %>%
  tally()

n_clean2 <- sepmeans %>%
  left_join(sepmeans_conds) %>%
  group_by(vot_cond, subject) %>%
  summarise() %>%
  tally()

n_total2 <- bind_rows(n_excl2, n_clean2) %$% sum(n)

accept_to_submit_time <-
  supunsup::separatemeans_assignments %>%
  do(daver::boot_ci(., function(d,i) with(d[i, ], mean(submittime-accepttime))))

active_time <- supunsup::separatemeans %>%
  group_by(subject) %>%
  summarise(time = max(tend) - min(tstart)) %>%
  mutate(time_min = time / 60000) %>%
  do(daver::boot_ci(.$time_min, function(d,i) mean(d[i])))

post_test_len <-
  sepmeans_test %>%
  filter(subject==first(subject)) %>%
  nrow()

```

We recruited `r n_total2` subjects via Amazon's Mechanical Turk, who were paid
\$2.50 for participation. The task took subjects about 20 minutes to complete
(`r active_time %$% observed` minutes active time on average, and 
`r accept_to_submit_time %$% observed` 
minutes between accepting and submitting the
HIT). As in Experiment 1, we excluded subjects 
($n=`r n_excl2 %$% sum(n)`$) 
who failed to classify words with unambiguous VOTs reliably during exposure (not
post-test, see below), after which
`r n_clean2 %$% sum(n)` subjects remained for analysis (no subjects had previously
participated in a VOT study). Subjects were evently distributed across conditions (range of `r n_clean2 %$% min(n)` to `r n_clean2 %$% max(n)` per condition).

#### Procedure: assessing category boundaries

The procedure was identical to Experiment 1, with one exception. After
completing the 222 trial exposure phase as in Experiment 1, listeners completed
a test phase in order to assess their category boundaries. This phase consisted
of `r post_test_len` additional trials with VOTs evenly distributed from -10 to
50ms. This additional phase was necessary because of the large separtion between
/b/ and /p/ clusters in the exposure distributions meant that in some conditions
there were no trials with VOTs anywhere near the predicted (or typical)
boundaries.

Critically, listeners were not told about the change from exposure to test
phase: the procedures were identical, and there was no break in between.
Besides the change in VOT distrbutions, there was no way for listeners to tell
that they had entered the test phase. Of course, if listeners are actually (as
we hypothesize) _learning_ these distributions, their behavior may well change
as they proceed through the test phase, gradually erasing any effect of the
differences in the exposure distributions. Thus, when analyzing data from the
test phase, whenever possible we limit ourselves to the early parts of the test
phase, when listeners behavior should be minimally affected by the change in
distributions.

As in Experiment 1, we evaluated listeners adaptation by their classification
functions, which we estimated by fitting a logistic GLM. In addition to VOT, we
included a predictor for trial in order to account for any un-learning effects
that might happen during post-test. In order to visualize the fitted
classification functions and to estimate listeners' category boundaries, we used
the classification function estimates fro the trial halfway through the first
third of the post-test.[^why-not-beginning]

[^why-not-beginning]: The predictions from a regression model are more uncertain
    and more driven by noise at the estreme ranges of continuous predictors,
    like trial number. Thus we use the one-sixth of the post-test to estimate
    classification functions, rather than the very beginning, because it is
    _close_ to the beginning of the post test, but not so close that the
    predictions substantially affected by the extra instability of the
    predictions that come at the edge of a continuous predictors range.

```{r fit-cat-bounds}

bound_at_trial <- post_test_len / 6

## estimate category boundares at 1/6 of post test trials (bound_at_trial)
boundaries_exp2 <- sepmeans %>%
  filter(is_test) %>%
  group_by(bvotCond, pvotCond, subject) %>%
  mutate(trial = trial - min(trial) - bound_at_trial) %>% 
  do({ glm(respP ~ vot + trial, family='binomial', data=.) %>%
         broom::tidy() %>%
         select(term, estimate)
  }) %>%
  ungroup() %>%
  spread(term, estimate) %>%
  mutate(boundary = -`(Intercept)` / vot,
         ideal_boundary = (bvotCond + pvotCond)/2,
         prior_boundary = prior_bound,
         prop_shift = (boundary-prior_boundary)/(ideal_boundary-prior_boundary)) %>%
  left_join(sepmeans_conds) %>%
  filter(boundary < 100, boundary > -100)

```

```{r boundary-exposure-vs-test-exp2, fig.width=5, fig.height=2.5, fig.cap="Category boundaries estimated during post-test are correlated with estimates from exposure (in the 10, 50 condition where such an estimate is possible), but more variable."}

boundaries_exp2_exposure <-
  sepmeans %>%
  filter(!is_test, bvotCond == 10, pvotCond == 50) %>%
  group_by(subject) %>%
  do({ glm(respP ~ vot, family='binomial', data=.) %>%
         broom::tidy() %>%
         select(term,estimate) }) %>%
  spread(term, estimate) %>%
  mutate(boundary_exposure = -`(Intercept)` / vot)

boundaries_exp2_comparison <- 
  boundaries_exp2_exposure %>%
  left_join(boundaries_exp2 %>% select(subject, boundary)) %>%
  ungroup()

boundaries_exp2_exp_v_test_summary <- 
  boundaries_exp2_comparison %>%
  do(daver::boot_ci(., function(d,i) with(d[i, ], mean(boundary_exposure - boundary))))


boundaries_exp2_comparison %>%
  ggplot(aes(x=boundary, y=boundary_exposure)) +
  geom_abline() +
  geom_point() +
  geom_text(data = boundaries_exp2_comparison %>%
              filter(boundary == min(boundary)),
            label = 'One subject', hjust=0, vjust=0,
            nudge_x = .5, nudge_y = .5) +
  stat_smooth(method='lm') +
  coord_equal(ylim=c(20,36)) +
  labs(x='Category boundary estimated from post-test',
       y='Boundary estimated\nfrom exposure')

```

To validate this way of measuring listeners' category boundaries, we included
the 10ms mean /b/ VOT exposure distribution conditions from Experiment 1 as a
condition in Experiment 2 (where we refer to it as the 10ms /b/, 50ms /p/
condition). This allowed us to compare the method of assessing listener's
category boundaries from Experiment 1 (using exposure trials themselves) with
the following post-test trials _within_ the same subjects, as well as
replicating the results of Experiment 1 (and testing out-of-sample, same
condition prediction accuracy for the model results). Figure
@fig:boundary-exposure-vs-test-exp2 shows that the post-test estimates are more
variable, but correlated with the exposure estimates. Critically, they are not
biased, differing by
`r boundaries_exp2_exp_v_test_summary$observed` ms VOT 
on average (95% CI of 
`r boundaries_exp2_exp_v_test_summary %$% sprintf('%.1f--%.1f', ci_lo, ci_high)`
ms VOT). See also Figure @fig:boundary-violin-plots-exp2, which shows the
corresponding distribution of boundaries from Experiment 1.

### Results

```{r fig.width=10, fig.height=2, fig.cap="Listeners' categorization functions in Experiment 2 (during post-test) reflect partial adaptation to the exposure talker, especially for more extreme conditions extreme distributions. Adaptation was even less complete than in Experiment 1. Classification functions are estimated with a logistic GLM including trial; curves show predictions for the beginning of the post-test phase to minimize impact of unlearning during test."}

## generate and plot predictions halfway through first third (1/6 of 70 = 
exp2_predict_at <- data_frame(trial = bound_at_trial,
                              vot = seq(min(sepmeans_test$vot),
                                        max(sepmeans_test$vot)))

sepmeans_test %>%
  group_by(subject, vot_cond) %>%
  mutate(trial = trial - min(trial)) %>%
  nest() %>%
  mutate(mod = map(data, glm, formula=respP ~ vot+trial, family='binomial'),
         pred = map(mod, predict, exp2_predict_at, type='response')) %>%
  unnest(map(pred, ~ mutate(exp2_predict_at, prob_p = .))) %>%
  ggplot(aes(x=vot, y=prob_p, color=vot_cond)) +
  geom_line(aes(group=subject), alpha=0.2) +
  geom_line(data = prior_class,
            linetype=2, color='black') +
  geom_line(data = sepmeans_exposure_class,
            linetype=2, size=1) +
  facet_grid(.~vot_cond) +
  scale_x_continuous('VOT (ms)', limits = vot_limits) +
  scale_y_continuous('Probability /p/ response') + 
  scale_color_discrete('/b/, /p/\nmean VOT')


```


```{r boundary-violin-plots-exp2, fig.width=6.5, fig.height=4.5, fig.cap="The distribution of listeners' individual category boundaries in Experiment 2 reflects partial adaptation to the exposure talker's VOT distributions. Extreme shifts have minimal additional effect on listeners' boundaries, corroborating the strong prior biases observed in Experiment 1. The un-filled distribution shows the boundaries from listeners in the corresponding 10ms VOT /b/ mean condition from Experiment 1. These conditions had the same exposure distributions, but differed in whether category boundaries were measured during exposure (Experiment 1) or during a post-test phase (Experiment 2)."}

ggplot(boundaries_exp2, aes(y = boundary, x = vot_cond, fill = vot_cond)) +
  ## this is an awful hack: plot the violins first to force continuous x
  geom_violin(color=NA,draw_quantiles=c(0.25, 0.5, 0.75), show.legend=FALSE,
              alpha = 0) +
  geom_segment(data = boundaries_exp2 %>%    # prior boundary
                 summarise(xmin = min(as.numeric(vot_cond))-0.5,
                           xmax = max(as.numeric(vot_cond))+0.5,
                           y = unique(prior_bound)),
               aes(x=xmin, xend=xmax, y=y, yend=y, fill=NA),
               color='black', linetype=2) + 
  geom_segment(aes(x=as.numeric(vot_cond)-.5, # exposure boundaries_exp2
                   xend=as.numeric(vot_cond)+.5,
                   y=ideal_boundary,
                   yend=ideal_boundary,
                   color=vot_cond),
               linetype = 2, size = 1,
               data = boundaries_exp2 %>% group_by(vot_cond, ideal_boundary) %>% summarise()) +
  geom_violin(color=NA,
              ## draw_quantiles=c(0.25, 0.5, 0.75),
              alpha = 0.5,
              show.legend=FALSE) +
  coord_flip() +
  geom_pointrange(stat='summary', fun.data='mean_cl_boot', color='white') +
  theme(legend.position='none') +
  labs(x = 'Condition (Mean /b/ and /p/ VOT)',
       y = 'Category boundary (ms VOT)') +
  scale_color_discrete() +
  geom_violin(data = boundaries_exp1 %>%
                filter(bvotCond == 10) %>%
                mutate(vot_cond = '10, 50'),
              fill=NA, aes(color=vot_cond))

```

As in Experiment 1, listeners adapted to the different input distributions, but
only incompletely. On average, listeners' classification functions were
intermediate between the classification functions corresponding to the typical
talker and exposure talker's distributions. Listeners adapted on average just as
much in the one condition that was shared with Experiment 1 (mean VOT of 10ms
for /b/, 50ms for /p/), but their boundaries were much more variable (Figure
@fig:boundary-violin-plots-exp2, empty vs. filled distrbution).

#### Evidence for separate prevoiced cluster?

Listeners showed very little sensitivity to increasingly large negative mean /b/
VOTs: the average category boundaries were very similar in the -20, -50, and
-80ms /b/ mean conditions. On average these were comparable to the 0ms /b/, 40ms
/p/ mean condition from Experiment 1, which had a higher /b/ mean but lower /p/
mean. This suggests that listeners are more sensitive to variation in /p/ than
in /b/. Nevertheless, listeners in the 10ms /b/, 80ms /p/ still did not fully
adapt to the exposure distribution, producing category boundaries that were not
significantly different from those in the 10ms /b/, 50ms /p/ condition.

```{r prop-prevoicing}
prop_pre_by_talker <- 
  votcorpora::vot %>%
  filter(source == 'gva13') %>%
  group_by(subject) %>%
  summarise(prop_pre = mean(prevoiced)) %>%
  arrange(prop_pre)
```


One interpretation of this lack of sensitivity to large negative /b/ mean VOTs
is that listeners treat /b/ as being a mixture of a short-lag cluster (with a
mean around 0ms VOT) and a prevoiced cluster (with a large negative mean
VOT). Most American English talkers produce at least some short-lag VOTs, even
if they produce mostly prevoiced. @Goldrick2013 found that, even the talker who prevoiced the most still produced short-lag VOTs
`r prop_pre_by_talker %$% round(100 - prop_pre*100) %>% min()`% of the time. 
Thus, even for a talker who produces mostly prevoiced /b/s, a listener needs to
be ready for some short-lag /b/s as well. Because the short-lag distribution is
closer to the /p/ distribution, it is the one that largely determines where a
listener's category boundary will fall. In conditions where the exposure
distributions don't provide very much (if any) inforamtion about a talker's
short-lag distribution, we'd thus expect listeners' category boundaries to be
unchanged from a typical talker. This is in fact exactly what we observe in the
Experiment 2 conditions with large negative /b/ mean VOTs.

Of course, there are other possible explanations for the lack of large shifts in
these conditions. For instance, even if listeners treat /b/ as a single unimodal
cluster, the negative VOT distributions we used are still extreme relative to
the range of /b/ mean VOTs across talkers.  In the next section, we take one
approach to addressing this possibility, by evaluating how well the prior
beliefs inferred from Experiment 1---assuming a single /b/ cluster---predict the
pattern of adaptation here.

#### Predicted adaptation from inferred priors

Next, we ask how well do the prior beliefs inferred on the basis of Experiment 1
can predict the pattern of adaptation across exposure distributions we observed
in Experiment 2. Figure @fig:predict-exp2-from-inferred shows the predicted
classification functions (95% posterior predcitive intervals) for the exposure
conditions in Experiment 2, using the same parameters that were fit to
Experiment 1 (and visualized in Figure @fig:model-fit-classification).

```{r predict-expt2-from-inferred, cache=TRUE}

#' Convert from stan parametrization to beliefupdatr::nix2
stan_conj_to_nix2 <- function(stan_p) {
  with(stan_p, list(nu = nu_0,
                    kappa = kappa_0,
                    mu = mu_0,
                    sigma2 = sigma_0 ^ 2))
}

# convert samples of prior params in array form to list of samples in nix2
# parameter list form.
#
# e.g., mod_nix2_samples[[1]][[1]] is the first 
mod_nix2_samples <- 
  mod_samples[c('nu_0', 'kappa_0', 'mu_0', 'sigma_0')] %>%
  ## repeate these since there's just one for both categories
  map_at(c('nu_0', 'kappa_0'), ~ cbind(.x, .x)) %>%
  ## turn arrays into nested lists
  map(array_tree) %>%
  ## zip list of variables into list of samples
  transpose() %>%
  ## zip each sample's list of variables into list of categories
  map(transpose) %>%
  ## ...and zip list of samples into a list of categories
  transpose() %>%
  set_names(c('b', 'p')) %>%
  ## rename and convert expected sd to var
  at_depth(2, stan_conj_to_nix2)

## confirm that we have nix2 params at depth 2
invisible(mod_nix2_samples %>% at_depth(2, ~ assert_that(is_nix2_params(.))))

## get summary statistics for each condition
updated_nix2_samples <- 
  sepmeans %>%
  left_join(sepmeans_conds) %>%
  group_by(bvotCond, pvotCond, trueCat) %>%
  filter(subject == first(subject),
         is_test == FALSE) %>%
  nest() %>%
  mutate(prior_samples = map(trueCat, ~ mod_nix2_samples[[.x]]),
         updated_samples = map2(data, prior_samples,
                                function(d, s) map(s, nix2_update, x=d$vot)))

sample_to_lhood <- function(p)
  data_frame(vot = seq(-10,50),
             lhood = d_nix2_predict(vot, p))

predicted_lhood <- updated_nix2_samples %>%
  mutate(test_lhood = map(updated_samples,
                          . %>%
                            map(sample_to_lhood) %>%
                            do.call(what=rbind))) %>%
  unnest(test_lhood)

lapse_rate_samples <-
  mod_samples[['lapse_rate']] %>%
  as.numeric() %>%
  data_frame(lapse_rate=.) %>%
  mutate(sample = row_number())

predicted_prob_p <-
  predicted_lhood %>%
  group_by(bvotCond, pvotCond, trueCat, vot) %>%
  mutate(sample = row_number()) %>%
  spread(trueCat, lhood) %>%
  left_join(lapse_rate_samples) %>%
  mutate(prob_p = p / (b+p),
         prob_p_lapse = prob_p * (1-lapse_rate) + lapse_rate*0.5) %>%
  group_by(bvotCond, pvotCond, vot) %>%
  summarise_each(funs(mean=mean, lo=quantile(., 0.025), hi=quantile(., 0.975)),
                 prob_p, prob_p_lapse)

```

```{r plot-exp2-predictions-first-third, fig.width=8, fig.height=2, fig.cap="The prior beliefs inferred from Experiment 1 predict how much listeners adapt to each of the input distributions of Experiment 2. Shaded regions show the 95% posterior predictive intervals, and dots and lines show mean and 95% bootstrapped CIs for subject mean classification. Note the additional uncertainty in predictions relative to the posterior predictive distribution given Experiment 1's conditions (Figure {@fig:model-fit-classification})."}

predicted_prob_p %>%
  left_join(sepmeans_conds) %>%
  ggplot(aes(x=vot, y=prob_p_lapse_mean, color=vot_cond, fill=vot_cond)) +
  ## geom_line() +
  geom_ribbon(aes(ymin=prob_p_lapse_lo, ymax=prob_p_lapse_hi),
              alpha=0.2, color=NA) +
  geom_pointrange(data = sepmeans_test %>%
                    filter(ntile(trial,3) == 1) %>%
                    group_by(vot_cond, vot, subject) %>%
                    summarise(resp_p = mean(respP)),
                  aes(y=resp_p),
                  stat='summary', fun.data=mean_cl_boot) +
  facet_grid(.~vot_cond)

```

```{r exp2-mod-goodness-of-fit}

#' @param d_test test data
#' @param pred model predictions (i.e., predicted_prob_p)
#' @param ... additional arguments passed to group_by for output summary
exp2_gof <- function(d_test, pred, ...) {
  d_test %>%
    group_by(subject, vot, bvotCond, pvotCond, vot_cond, respCat, ...) %>%
    tally() %>%
    spread(respCat, n, fill=0) %>%
    left_join(pred) %>%
    group_by(...) %>%
    mutate(LL_mod = dbinom(x = p, size = b+p, prob = prob_p_lapse_mean,
                           log = TRUE),
           LL_null = dbinom(x = p, size = b+p, prob = mean(p/(b+p)),
                            log = TRUE)) %>%
    summarise(LL_mod = sum(LL_mod),
              LL_null = sum(LL_null),
              rho = cor(p/(b+p), prob_p_lapse_mean, method='spearman'),
              n = n()) %>%
    mutate(LL_ratio = LL_mod - LL_null,
           pseudo_R2_mcfadden = 1 - LL_mod/LL_null,
           pseudo_R2_nagelkerke = (1 - exp(2/n * -LL_ratio)) / (1-exp(2/n*LL_null)))
}

mod_pred_gof_exp2 <- sepmeans_test %>% exp2_gof(predicted_prob_p)

## by condition
mod_pred_gof_exp2_by_cond <-
  sepmeans_test %>%
  filter(ntile(trial,3) == 1) %>%
  exp2_gof(predicted_prob_p, vot_cond) %>%
  mutate(LL_mod_each = LL_mod/n, LL_ratio_each = LL_ratio/n)

mod_pred_gof_exp2_by_third <- 
  sepmeans_test %>%
  mutate(third=ntile(trial, 3)) %>%
  exp2_gof(predicted_prob_p, third)

```

Overall, the predicted classification functions line up reasonably well with
listeners' actual category boundaries at test (Figure
@fig:plot-exp2-predictions-first-third; 
Spearman's $\rho=`r mod_pred_gof_exp2_by_third %>% filter(third==1) %$% rho`$,
log-likelihood ratio with intercept-only null model of 
`r mod_pred_gof_exp2_by_third %>% filter(third==1) %$% LL_ratio`).

The biggest discrepancy in predicted category boundary is in the /b/ 10ms, /p/
80ms condition, where the model predicts a bigger category boundary shift than
is actually observed. There are also some inferred prior parameters that are consistent with the generally poor adaptation to large negative /b/ mean VOTs. This suggests one of two things. First, this means we cannot rule out the possibility that listeners really do treat /b/ as a single unimodal cluster on the basis of these results. Second, it could be the case that listeners do, in fact, treat /b/ as a mixture of short-lag and prevoiced VOT clusters, and that the model-inferred beliefs reflect the best approximation of these by a model that incorrectly assumes a single unimodal cluster.

The model predictions are also more slightly accurate earlier in the test phase
(likelihood ratios vs. null model for the first, second, and final third of the
post-test phase of
`r mod_pred_gof_exp2_by_third %$% LL_ratio %>% round() %>% paste_and()`).
This suggests that listeners may be unlearning the distributions encountered in
the test phase. Such unlearning would be consistent with other work that shows
that phonetic adaptation (recalibration) can be undone by prolonged testing
[@Kraljic2005; @Vroomen2004].

Finally, one intresting feature of the model predictions themselves is that the
uncertainty is quite a bit higher for the two extreme negative /b/ VOT (-80ms
and -50ms) conditions. This suggests that there is uncertainty in the posterior
over prior beliefs inferred from Experiment 1, but that this uncertainty is not
relfected in uncertainty about the classification functions for Experiment 1
(Figure @fig:model-fit-classification), which the model was trained on. But this
uncertainty is revealed by input distributions that are more diverse, like those
in Experiment 2 that vary in the distance between clusters as well as the
overall mean. This goes to show the importance of checking model predictions for
conditions that are different from those used to train it.


### Discussion ###

<!-- TODO: this is rough, revise it -->
Relative to Experiment 1, the effect of the distribution manipulation in
Experient 2 is pretty small. In fact, it appears that moving the /b/ mean VOT
from -20 to -50 and even -80 has little additional effect on the category
boundary, even though each of these changes would move the boundary by -15ms VOT
each (if the prior was completely ignored).

<!-- todo: a little casual in tone -->
At first glance, this is surprising. But when you consider the _structure_ of how mean VOT varies across categories, it begins to make some sense. @Chodroff2015, for instance, found that across talkers, the VOTs of different places of articulation and voicing categories were strongly positively correlated. If listeners are sensitive to this correlation structure, the resulting prior on their adaptation would effectively filter out portions of the variability that are not positively correlated across categories.

However, computational modeling reveals that these apparent differences between Experiments 1 and 2 are not only _consistent_ with belief updating starting from a singel set of prior beliefs, but are actually _predicted_ based on the adaptation in Experiment 1.

## General discussion

In two experiments, we have evaluated how well listeners can adapt to different
"accents" by learning distributions of VOTs. Our results confirm previous
findings that listeners can adapt to novel accents like these via distributional
learning [@Clayards2008; @Munson2011]. We go beyond previous work by
systematically evaluating adaptation performance on a range of different
distributions. Our results show that listeners do not adapt as well to extreme
distributions that deviate substantially from what it typical of American
English. This goes to show that while distributional learning is, in principle,
a very powerful general-purpose approach to dealing with the lack of invariance,
it is _constrained_ in actual practice.

### Constraints on distributional learning

Where do these constraints come from? A central tenet of the ideal adapter
framework [@Kleinschmidt2015] is that listeners can---and should---benefit from
structure in how talkers vary in the cue distributions they produce. Even if all
a listener knows is that a talker is speaking English, their prior experience
with other speakers of English provides a lot of information about the cue
distributions that they should expect, which makes the process of inferring that
particular talkers' distributions much more efficient. In this view, listeners
will pay a price when they encounter a talker whos distributions fall outside
the range they expect. Thus, listeners expectations should be matched to the
actual variability in the world, in order to get as much benefit from the head
start provided by prior expectations while avoiding over-confidence.

Our modeling results show that this explanation is at least consistent with the
pattern of constraints that we observed behaviorally. The range of adaptation
behavior across different distributions in Experiment 1 can be fit very well by
a model that assumes that listeners update their beliefs based on the
distributions they experience, starting from a prior expectation about what a
typical talker sounds like and how much talkers vary. Moreover, the beliefs that
we can infer listeners have for an unfamiliar talker match the actual cue
distributions produced by American English speakers reasonably well (with the
caveat that our model makes potentially problematic assumptions about the
structure of categories within and across talkers, which we discuss below).

There are, of course, other possible sources for these constraints. For
instance, the basic nature of the mammalian auditory system may constrain the
distributions that are actually psychoacoustically distinguishable. Failure to
adapt to particular distributions may then simply be a side effect of these
distributions being difficult to perceive because of non-linearities in the
underlying psychoacoustic dimensions (which we model as linear in VOT). This is
a plausible explanation for VOT in particular, since non-human animals are also
more inclined to categorize voiced and voiceless stops along human phonetic
category boundaries [@Kuhl1975]. Moreover, the typical boundary between voiced
and voiceless word initial stops more or less corresponds to the temopral
asynchrony required for neural responses to separately time lock to two events,
[@Steinschneider1994].

Another plausible possibility is that listeners' expectations are driven by
their _own_ productions, and that any regularity between these comes from
acquisition or innate anatomical constraints.
<!-- TODO: citations and expand -->
Other, intermediate possibilities exist, too. For instance, listeners'
expectations may be based on their own productions, but their _confidence_ based
on the level of cross-talker variability they have experienced.

These alternative explanations are plausible in the case of voicing, in large
part because the cue distributions do not vary substantially across
talkers. Contrasts that are distinguished by spectral cues _do_ vary
substantially across talkers, and moreover show substantial stylistic
variability that cannot be normalized away
[@McMurray2011a; @Jongman2000; @Hagiwara1997; @Hillenbrand1995; @Clopper2005; @Newman2001].

Testing for constraints on adaptation for these other, more variable contrasts
would help resolve these different possible explanations. For instance, if
listeners are _more_ contrained than would be expected based on cross-talker
variation, it would suggest that the constraints come from something like their
own productions, since levels of _within_ talker variability are similar across
contrasts [@Newman2001; @Allen2003]. The presence of _structure_ in cross-talker
variation also provides a means to test more specific predictions of the ideal
adapter. In particular, the ideal adapter predicts that when talkers cluster in
groups, that listeners' distributional learning is informed (and hence
constrained) by their experience with variabiltiy across talkers in the relevant
group, not just overall. For instance, if listeners pick up on the fact that
male and female talkers systematically differ in their vowel formant
distributions, then knowing that a talker is male or female should lead to more
constrained adaptation than if they do not know the gender of the talker (all
else being equal, including other cues to gender).

### Measuring listeners' subjective beliefs about unfamiliar talkers

Our other major finding is that it is possible to infer what cue distributions
listeners expect from an unfamiliar talker, based solely on listeners adaptation
behavior (as measured by phonetic classification). The actual beliefs that our
model infers listeners to have provide a good fit to this adaptation behavior,
can predict adaptation for distributions that were not used to train the model,
and align reasonably well with distributions that talkers actually produce. The
combination of presenting listeners with a parametrically manipulated range of
distributions and analyzing the resulting changes in phonetic classification
with a belief updating model provides a potentially powerful tool for measuring
listeners' expectations. This is important because these expectations are an
essential part of how listeners manage to adapt to efficiently to talker
variability, but are difficult to measure, being implicit and subjective. The
subjective nature of these expectations is particularly important, because they
may deviate in substantial ways from the actual nature of cross-talker variation
[for one example, see @Niedzielski1999].

This tool nevertheless still has limitations. Most importantly, the prior
beliefs a model can infer are fundamentally bound by the assumptions the model
makes about that nature of those beliefs and how they are updated. If the
structure of the world does not align with what is assumed by the model, the
models' inferences may be misleading in subtle ways, _even if the predictive
accuracy of the model is good_.  For instance, we assumed that each phonetic
category is a single normal distribution, but in reality the way people actually
produce /b/ appears to be a mixture of two distinct clusters. Even so, our model
effectively predicted adaptation to different distributions, even with negative
VOTs where the structure of the /b/ category seems like it should make a
difference. The beliefs that the model infers listeners to have for /b/, though,
do not look like distributions of VOTs that talkers actually produce for
/b/. Instead, they look like a good _approximation_ of the true distrbution
using only a single cluster, with a mean and variance that is intermediate
between the two clusters talkers actually produce. It would be premature to
conclude that listeners actually believe a typical talker would produce this
distribution. Rather, this discrepancy highlights areas where more data is
needed. That is, a model need not be _true_ for it to be _useful_, as long as
it's understood as a means for interpreting and guiding empirical work and not a
source of ground truth.


## Conclusion {#sec:conclusion}

<!-- from cog sci paper -->

A central prediction of the ideal adapter framework [@Kleinschmidt2015] is that
listeners adapt to unfamiliar talkers by combining their prior beliefs with
observed evidence about that particular talker's cue distributions. In this
paper, we have shown first that for a range of different accents (cue
distributions), listeners' behavior in a distributional learning experiment
reflects a compromise between what would be expected for the cue distributions
produced by a typical talker and the exposure talker. Second, the range of
adaptation behavior observed across the various accents that listeners heard can
be captured by a belief-updating model with a single set of prior expectations
that are updated based on experience with the exposure talker.

These results emphasize the importance of listeners' prior expectations for
robust speech perception in the face of talker variability. Even if all the
listener knows about the talker is that they are speaking English, they can
still benefit from prior experience with other speakers of English to provide an
informative head start for adaptation.  The modeling framework we use has the
additional advantage of allowing us to *infer* what cue distributions listeners
believe an unfamiliar talker will produce. This provides a potentially
powerful---and heretofore missing---tool for probing listeners' prior
expectations, based only on comprehension data. These beliefs reflect what
listeners have learned about the variability they can expect across talkers, and
probing how this internal model is related to the *actual* variability across
talkers (measured via speech production data) is an important next step in
advancing our understanding of robust speech perception.

More generally, prior knowledge is increasingly understood to play in important
role in a number of perceptual and memory domains [e.g,. @Brady2013; @Froyen2015a;
@Orhan2011]. Distributional learning provides an approach to probing prior
expectations about the *statistics* of the sensory world, which, as in speech
perception, are critical to effectively coping with non-stationarity in sensory
statistics.


* big thign: provides evidence supporting a basic prediction of ideal adapter:
  listeners use experience with variability across talkers to infrom starting
  point for adaptation
