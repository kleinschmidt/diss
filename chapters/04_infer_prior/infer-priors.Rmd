---
Title: What do you expect from an unfamiliar talker?
Author: Dave Kleinschmidt
bibliography: /Users/dkleinschmidt/Documents/papers/library-clean.bib
output:
    html_document:
        code_folding: hide
        dev: png
        keep_md: true
        md_extensions: +implicit_figures
        pandoc_args:
        - --filter
        - pandoc-fignos
        - --filter
        - pandoc-tablenos
    pdf_document:
        md_extensions: +implicit_figures
        pandoc_args:
        - --filter
        - pandoc-fignos
        - --filter
        - pandoc-tablenos
---

```{r preamble}

library(assertthat)
library(magrittr)
library(dplyr)
library(tidyr)
library(purrr)
library(stringr)
library(lme4)
library(ggplot2)

## devtools::install_github('kleinschmidt/phonetic-sup-unsup')
library(supunsup)
## devtools::install_github('kleinschmidt/beliefupdatr')
library(beliefupdatr)

library(knitr)
opts_chunk$set(warning = FALSE,
               message = FALSE,
               error = FALSE,
               echo=opts_knit$get("rmarkdown.pandoc.to") != 'latex')

## Produce markdown-formatted figures so that pandoc knows what to do with
## the captions. requires pandoc-fignos to parse the IDs. refer to figures
## in text with {@fig:label} or just @fig:label
## 
## (see https://github.com/tomduck/pandoc-fignos)
knit_hooks$set(plot = function(x, options) {
  paste0('![', options$fig.cap, ']',
         '(', opts_knit$get('base.url'), paste(x, collapse='.'), ')',
         '{#fig:', options$label, '}')
})
## Produce markdown-formatted table captions with anchors for cross-refs.
## Requires pandoc-tablenos to parse the IDs. Refer to tables
## in text with {@tbl:label} or @tbl:label.
## Based partly on http://stackoverflow.com/a/18672268
##
## (see https://github.com/tomduck/pandoc-tablenos)
knit_hooks$set(tbl.cap = function(before, options, envir) {
  if(!before){
    paste0('\n\nTable: ', options$tbl.cap,
           ' {#tbl:', options$label, '}', sep = '')
  }
})

```

# Introduction

A longstanding problem in speech perception is how listeners manage to
cope with substantial differences in how individual talkers produce
speech. Recent evidence suggests that one strategy listeners employ is
to *rapidly adapt* to unfamiliar talkers [@Bertelson2003; @Clarke2004; @Kraljic2007 among others]. Such adaptation can be
understood as a form of statistical inference. This insight is captured
by a recent proposal, the *ideal adapter* framework [@Kleinschmidt2015].
Each talker's particular accent (way of talking) can be formalized as
the distribution of acoustic cues that they produce for each phonetic
category (or other underlying linguistic unit). Listeners are taken to
adapt to an unfamiliar talker via *distributional learning*, inferring
the underlying talker-specific cue distributions from the talker's
productions.

Critically, this statistical inference process draws on implicit beliefs
about *how* talkers tend to differ from each other. As a consequence,
adaptation to an unfamiliar talker should depend on a listener's prior
experience with other talkers, rather than only on the speech produced
by the unfamiliar talker themselves. Specifically, a listener's
experience with other talkers provides the starting point for the
distributional learning required for adaptation, or, in Bayesian terms,
a *prior belief* about the probability of different possible accents
(cue distributions). More informative prior beliefs can substantially
reduce the amount of direct evidence needed to converge on accurate
beliefs about the current talker's cue distributions.

The goals of the present work are two-fold. First, we test a critical
prediction of the ideal adapter framework. To the extent that a
listener's prior beliefs are informative, they must take some
probability *away* from unlikely accents. Confronted by a talker whose
accent falls well outside the range of what they expect based on their
previous experience, the ideal adapter framework predicts that a
listener will require more evidence to adapt, leading to slowed or
incomplete adaptation. There is some evidence that this is the case. For
instance, @Idemaru2011 found that listeners have difficulty adapting to
a talker who produces anti-correlated distributions of two cues that are
typically positively or un-correlated. @Sumner2011 found that listeners
had trouble adapting to a talker who produced a distribution of cues for
the /b/ and /p/ sounds that had substantially lower means than a typical
talker.

However, no studies have systematically probed whether and how a
listener's prior expectations constrain phonetic adaptation, or even
what kind of prior beliefs listeners have. To that end, we expose
listeners to a range of different accents, which differ (only) in the
cue distributions for /b/ and /p/. By parametrically manipulating these
distributions, we create a range of accents that are more or less
similar to what a typical talker of English produces. We then assess the
degree to which listeners adapt their beliefs about the novel talker's
cue distributions, depending on the *a priori* typicality of these
distributions.

To anticipate the results, we find that typicality of the novel talker's
cue distribution predicts the degree to which listeners adapt to the
talker. This suggests that listeners not only have beliefs about the cue
distributions for a *particular* single talker [as suggested by previous
work, @Clayards2008; @Feldman2009; @Kleinschmidt2015; @Kronrod2012]),
but also have implicit beliefs about the ways in which talkers tend to
*differ* from each other, and hence what to expect from an unfamiliar
talker. This leads to the second question we address here: what is the
content of listeners' prior beliefs about inter-talker variability? To
this end, we use a Bayesian belief-updating model to work backwards from
listeners' adaptation behavior across talkers and *infer* listeners'
shared prior beliefs. This approach provides a more direct assessment of
listeners' subjective prior beliefs than production data from many
different talkers, which moreover requires costly and time-consuming
annotation. If successful, this would provide a powerful method for
investigating listeners' prior beliefs that could be applied to other
cues, categories, and even social variables (like gender, native
language background, etc.).


```{r typical-talker}

prior_lhood <- 
  supunsup::prior_stats %>%
  filter(source == 'kronrod2012') %>%
  supunsup::stats_to_lhood()

prior_class <- prior_lhood %>% lhood_to_classification()

```

# Experiment 1

```{r expt1-data}

data_exp1 <- supunsup::supunsup_clean %>%
  filter(supCond == 'unsupervised') %>%
  mutate(trueCat = respCategory,
         subjNum = as.numeric(factor(subject)),
         trueCatNum = as.numeric(trueCat),
         respCatNum = as.numeric(respCat))

```


```{r vot-dists-exp1, fig.width=8, fig.height=2, fig.cap="Each subject heard one of these five synthetic accents, which differ only in the distribution of VOTs of the word-initial stops. Black dashed lines show VOT distributions from a hypothetical typical talker [as estimated by @Kronrod2012]. Note that the 0 and 10ms shifted accents are reasonably close to this typical talker, while the $-10$, 20, and 30ms shifted accents deviate substantially."}

exposure_stats <- data_exp1 %>%
  group_by(bvotCond, category=trueCat) %>%
  summarise(mean=mean(vot), sd=sd(vot))

sd_noise = sqrt(82)

exposure_lhood <- exposure_stats %>%
  group_by(bvotCond) %>%
  do(supunsup::stats_to_lhood(., sd_noise))

data_exp1 %>%
  group_by(bvotCond, vot) %>%
  filter(subject == first(subject)) %>%
  tally() %>%
  ggplot(aes(x=vot)) +
  geom_bar(stat='identity', aes(y=n, fill=bvotCond)) +
  geom_line(data=prior_lhood, aes(y=lhood*1600, group=category),
            color="black", linetype=2) +
  geom_text(data=data.frame(bvotCond=-10), x = 10, y = 60,
            label = 'Typical Talker',
            color='black', hjust=0, vjust=0.3, size=3) +
  geom_text(data=data.frame(bvotCond=-10), x = 40, y = 50,
            label = 'Exposure\nTalker',
            color=hcl(h=15, c=100, l=65), hjust=0, vjust=0.8, size=3,
            lineheight=1) + 
  facet_grid(.~bvotCond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Frequency') +
  scale_fill_discrete('/b/ mean\nVOT') ## +
  ## theme(legend.position='none')

``` 

We tested the role that listeners prior expectations play in adapting to an
unfamiliar talker by exposing them to one of five synthetic "accents" (Figure
@fig:vot-dists-exp1). These accents differed only in the distribution of
voice onset time (VOT), the primary cue to word-initial stop consonant voicing
in English (e.g., "beach" vs. "peach"). Adaptation was assessed based on
listeners' classification function, or how they labeled each VOT as /b/ or /p/.

## Methods {#sec:methods}

### Subjects {#sec:subjects}

```{r participants-exp1}

n_subj <- data_exp1 %>% group_by(subject) %>% summarise() %>% tally()

excluded <- supunsup::supunsup_excluded %>%
  filter(supCond == 'unsupervised') %>%
  group_by(subject) %>% 
  summarise() %>% 
  right_join(supunsup::excludes) %>%
  select(subject, exclude80PercentAcc, rank)

n_excluded <- nrow(excluded)
n_subj_repeat <- sum(!is.na(excluded$rank))
n_subj_bad <- sum(!is.na(excluded$exclude80PercentAcc))
n_both <- n_subj_repeat + n_subj_bad - n_excluded 

n_total <- n_subj + n_excluded

```

We recruited `r n_total` subjects via Amazon's Mechanical Turk, who were paid
\$2.00 for participation, which took about 20 minutes. We excluded subjects who
participated more than once ($n=`r n_subj_repeat`$) or whose accuracy at and
VOT---as extrapolated via a logistic GLM---was less than 80% correct 
($n=`r n_subj_bad`$; $n=`r n_both`$ for both reasons). 
Excluded subjects were roughly
equally distributed across conditions (maximum of 5 in 0ms /b/ VOT condition,
and minimum of 1 in 20ms /b/ VOT condition). After these exclusions, data from
`r n_subj` subjects remained for analysis for analysis.

### Procedure

![Example trial display (beach/peach). Listeners first click on the
    green button to play the word, then click on one picture to indicate what
    they heard.](figure_manual/beach_peach.png){#fig:beach-peach}

Our distributional learning procedure is described in @Kleinschmidt2015a, and is
based on @Clayards2008. On each trial, two response option images appeared,
which corresponded to one of three /b/-/p/ minimal pairs (beach/peach,
bees/peas, or beak/peak). Subjects then clicked on a central button which played
the corresponding minimal pair word, and then clicked on the picture to indicate
whether they heard the /b/ or /p/ member of the minimal pair (Figure
@fig:beach-peach). Subjects performed 222 of these trials.

Each trial's word was synthesized with a voice onset time (VOT) that was
randomly drawn from a bimodal distribution, with low and high VOT clusters
implicitly corresponding to /b/ and /p/, respectively. This distribution defined
the *accent* that the subject heard, and each subject was pseudorandomly
assigned to one of five accent conditions (Figure @fig:vot-dists-exp1). Each of these
accents implies a different /b/-/p/ category boundary, and listeners' adaptation
was evaluated by comparing their /b/-/p/ classification function (fitted by a
logistic GLM) to the classification function derived from the typical talker's
cue distributions (corresponding to no adaptation) and the exposure distribution
(corresponding to maximal---but not necessarily optimal---adaptation).

## Results and Discussion

```{r class-curves, fig.height=2, fig.width=8, fig.cap="Listeners' responses, smoothed with logistic functions (thin lines), compared with the classification functions expected based on a typical talker (no learning; dashed black lines) and complete adaptation to the exposure distributions (thick dashed colored lines). Listeners' actual category boundaries lie between the typical talker and exposure talker boundaries (see Table {@tbl:boundary-shift})."}

## generate predicted classification functions assuming Bayes-optimal classifier
## + noise

perfect_learning <- exposure_stats %>%
  group_by(bvotCond) %>%
  do(stats_to_lhood(.)) %>%
  lhood_to_classification

no_learning <- prior_lhood %>%
  lhood_to_classification

prior_bound <- no_learning %>%
  arrange(abs(prob_p - 0.5)) %>%
  filter(row_number() ==1) %$%
  vot


boundaries <- data_exp1 %>%
  group_by(bvotCond, subject) %>%
  do({ glm(respP ~ vot, family='binomial', data=.) %>%
         broom::tidy() %>%
         select(term, estimate)
  }) %>%
  ungroup() %>%
  spread(term, estimate) %>%
  mutate(boundary = -`(Intercept)` / vot,
         ideal_boundary = as.numeric(as.character(bvotCond)) + 20,
         prior_boundary = prior_bound,
         prop_shift = (boundary-prior_boundary)/(ideal_boundary-prior_boundary))

boundary_summary <- boundaries %>%
  group_by(bvotCond) %>%
  summarise(median_shift_perc = round(100*median(prop_shift)),
            shift_text = paste(median_shift_perc, '%', sep='')) %>%
  filter(bvotCond != 0)                 # basically no shift possible

ggplot(data_exp1, aes(x=vot, y=respP, color=bvotCond)) +
  geom_line(aes(group=subject), stat='smooth', method='glm', 
            method.args=list(family='binomial'), alpha=0.2) +
  facet_grid(.~bvotCond) +
  geom_line(data=perfect_learning, aes(y=prob_p), group=1, linetype=2, size=1) +
  geom_line(data=no_learning, aes(y=prob_p), group=1, linetype=2, color='black') +
  geom_text(data=data.frame(bvotCond=-10),
            x = 30, y = 0, label = 'Typical\ntalker',
            size = 3.5, hjust=0, vjust = 0, color='black',
            lineheight=1) + 
  geom_text(data=data.frame(bvotCond=-10),
            x = 12, y = 1, label = 'Expo-\nsure',
            size = 3.5, hjust=1, vjust=1, color=hcl(15, c=100, l=65),
            lineheight=1, fontface='bold') + 
  geom_text(data=data.frame(bvotCond=-10),
            x = 90, y = 0.75, label = 'Actual\nlisteners',
            size = 3.5, hjust=1, vjust=1, color=hcl(15, c=100, l=65),
            lineheight=1) + 
  ## geom_text(data=boundary_summary, aes(x=75, y=0.1, label=shift_text), color='black') + 
  ## theme(legend.position='none') +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Probability /p/ response') + 
  scale_color_discrete('/b/ mean\nVOT')

```

Figure {@fig:class-curves} shows the classification functions for each
individual listener. In each accent, these classification functions tend to fall
in between the boundaries predicted by the typical talker distributions and the
boundaries implied by the exposure distributions.  We can quantify this by the
percentage of the predicted shift in category boundary from the classification
function for the typical talker to the boundary implied by the input
distribution (Table {@tbl:boundary-shift}). A 0% shift corresponds to no
adaptation at all, while a shift of 100% corresponds to complete adaptation to
the exposure distributions, with no (remaining) influence of any prior beliefs.

In all conditions, the average shift percentage was between 0% and 100%
(except the 0ms shift condition, which is so close to the typical talker that
estimating the percentage is numerically unstable). More interestingly, the more
extreme conditions show less complete adaptation than the less extreme conditions.
Together, these results suggest that listeners' adaptation was constrained by
their prior expectations (given the finite amount of evidence they received
about the unfamiliar talker).  This provides qualitative evidence that listeners
combine their prior expectations with observed cue distributions in order to
rapidly adapt to unfamiliar talkers, as predicted by the ideal adapter framework
[@Kleinschmidt2015].


```{r boundary-shift, results='asis', tbl.cap="Percentage of boundary shift from typical talker to each exposure talker (see Figure {@fig:class-curves}), averaged over subjects with 95% bootstrapped confidence intervals.  0% shift corresponds to no adaptation at all, while 100% corresponds to perfect adaptation, ignoring any prior  beliefs. Typical and exposure talker boundaries were too close together to reliably determine boundary shift percentage in the 0ms condition."}

## bootstrapped boundary summary
boundaries %>% 
  group_by(bvotCond) %>% 
  do(daver::boot_ci(., function(d,i) {mean(d$prop_shift[i])})) %>%
  mutate(observed = round(observed * 100),
         ci_lo = round(ci_lo * 100),
         ci_high = round(ci_high * 100)) %>%
  ungroup() %>%
  transmute(`/b/ mean VOT` = bvotCond,
            `Mean shift` = ifelse(bvotCond == 0,
                                  '---',
                                  sprintf('%d%%', observed)),
            `95% CI` = ifelse(bvotCond == 0,
                                '---',
                                sprintf('%d--%d%%', ci_lo, ci_high))) %>%
  knitr::kable(escape = FALSE)

```


## Modeling


```{r run-model, eval=FALSE}

## run the belief-updating model for inferring prior. the model source is in
## the beliefupdatr package, or will be soon :)
##
## devtools::install_github('kleinschmidt/beliefupdatr')

data_exp1_stan_conj <- data_exp1 %>%
  supunsup::supunsup_to_stan_conj()
  
library(rstan)
mod_lapsing <- beliefupdatr::compile_stan('conj_id_lapsing_fit.stan')

fit_lapsing <- stan(fit = mod_lapsing,
                    data = data_exp1_stan_conj,
                    chains = 4,
                    iter = 1000)

head(summary(fit_lapsing)$summary, n=10)

mod_summary <- summary(fit_lapsing)$summary
mod_samples <- rstan::extract(fit_lapsing)

saveRDS(mod_samples, file='models/samples_lapsing.rds')
saveRDS(mod_summary, file='models/summary_lapsing.rds')

```

```{r load-samples}

mod_samples <- readRDS('models/samples_lapsing.rds')

```


# Experiment 2

```{r separatemeans-data}

sepmeans <- supunsup::separatemeans_clean

sepmeans_conds <-
  sepmeans %>%
  group_by(bvotCond, pvotCond) %>%
  summarise() %>%
  ungroup() %>%
  arrange(bvotCond, pvotCond) %>%
  mutate(vot_cond = paste(bvotCond, pvotCond),
         vot_cond = factor(vot_cond, levels=vot_cond))

sepmeans_stats <-
  sepmeans %>%
  filter(!is_test) %>%
  left_join(sepmeans_conds) %>%
  group_by(bvotCond, pvotCond, vot_cond, trueCat) %>%
  summarise(mean = mean(vot), sd = sd(vot)) %>%
  rename(category = trueCat)

sepmeans_exposure_lhood <-
  sepmeans_stats %>%
  group_by(bvotCond, pvotCond, vot_cond) %>%
  do({stats_to_lhood(.)})

sepmeans_exposure_class <-
  sepmeans_exposure_lhood %>%
  do({lhood_to_classification(.)})

```

```{r fig.width=10, fig.height=2}

sepmeans %>%
  filter(is_test) %>%
  left_join(sepmeans_conds) %>%
  ggplot(aes(x=vot, y=respP)) +
  stat_smooth(method='glm', method.args=list(family='binomial'), geom='line',
              aes(group=subject, color=vot_cond), alpha=0.2) +
  stat_smooth(method='glm', method.args=list(family='binomial'), geom='line',
              aes(color=vot_cond), size=2) +
  geom_line(data = prior_class %>% filter(vot <= 50, vot >= -10),
            linetype=2, color='black', aes(x=vot, y=prob_p)) + 
  geom_line(data = sepmeans_exposure_class %>% filter(vot <= 50, vot >= -10),
            linetype=2, size=2, aes(x=vot, y=prob_p, color=vot_cond)) + 
  facet_grid(.~vot_cond)


```

```{r }

sepmeans %>%
  filter(is_test) %>%
  left_join(sepmeans_conds) %>%
  ggplot(aes(x=vot, y=respP)) +
  stat_smooth(method='glm', method.args=list(family='binomial'), geom='line',
              aes(color=vot_cond), size=2) +
  geom_line(data = prior_class %>% filter(vot <= 50, vot >= -10),
            linetype=2, color='black', aes(x=vot, y=prob_p)) + 
  geom_line(data = sepmeans_exposure_class %>% filter(vot <= 50, vot >= -10),
            linetype=2, size=2, aes(x=vot, y=prob_p, color=vot_cond))

```

## Mixed-effects model

```{r sepmeans-prepare-lmer}

sepmeans_conds <-
  sepmeans %>%
  group_by(bvotCond, pvotCond) %>%
  summarise() %>%
  ungroup() %>%
  arrange(bvotCond, pvotCond) %>%
  mutate(vot_cond = paste(bvotCond, pvotCond),
         vot_cond = factor(vot_cond, levels=vot_cond))

cs <- contr.helmert(sepmeans_conds$vot_cond) %*% diag(1/(1:4))
colnames(cs) <- levels(sepmeans_conds$vot_cond)[2:5]
contrasts(sepmeans_conds$vot_cond) <- cs
  

# process data for modeling:
sepmeans_test <- sepmeans %>%
  # only modeling test trials:
  filter(is_test) %>%
  # convert b/p means into center + distance
  mutate(vot_cond_center = bvotCond + pvotCond,
         vot_cond_dist = pvotCond - bvotCond) %>%
  left_join(sepmeans_conds, by=c('bvotCond', 'pvotCond')) %>%
  # center/10 and scaled versions of continuous predictors
  mutate_each(funs(c10 = (. - mean(.))/10,
                   scale = (. - mean(.))/sd(.)),
              vot, vot_cond_center, vot_cond_dist, trial)

summary(sepmeans_test)

```

Lots of convergence issues with LMER.

```{r sepmeans-run-lmer, cache=TRUE}



m0 <- glmer(respP ~ 1 + vot_c10 + vot_cond + trial_c10 +
              (1 | subject),
            data = sepmeans_test, family='binomial')

summary(m0)

m0.1 <- glmer(respP ~ 1 + vot_c10 + vot_cond + trial_c10 +
                (1 + vot_c10 | subject),
              data = sepmeans_test, family='binomial')

summary(m0.1)

m0.2 <- glmer(respP ~ 1 + vot_c10 + vot_cond + trial_c10 +
                (1 + vot_c10 || subject),
              data = sepmeans_test, family='binomial')

summary(m0.2)

## m1 <- glmer(respP ~ 1 + vot_c10 * vot_cond * trial_c10 +
##               (1 + vot_c10 * trial_c10 | subject),
##             data = sepmeans_test, family='binomial')

## summary(m1)

```

```{r sepmeans-lmer-table, results='asis'}
stargazer::stargazer(m0, m0.1, m0.2, star.cutoffs=c(0.05, 0.01, 0.001), type='html')
```

## Category boundary analysis

```{r fit-cat-bounds}

boundaries <- sepmeans %>%
  filter(is_test) %>%
  group_by(bvotCond, pvotCond, subject) %>%
  mutate(trial = trial - min(trial)) %>% 
  do({ glm(respP ~ vot * trial, family='binomial', data=.) %>%
         broom::tidy() %>%
         select(term, estimate)
  }) %>%
  ungroup() %>%
  spread(term, estimate) %>%
  mutate(boundary = -`(Intercept)` / vot) %>%
  left_join(sepmeans_conds)

```

```{r}

ggplot(boundaries, aes(x=vot_cond, y=boundary)) +
  geom_violin() +
  stat_summary(fun.data=mean_cl_boot, geom='pointrange') +
  lims(y = c(-20, 60))

```

## Predicted adaptation from inferred priors

How well do the prior beliefs inferred on the basis of Experiment 1 predict the adaptation we observed in Experiment 2?

```{r predict-expt2-from-inferred, cache=TRUE}

#' Convert from stan parametrization to beliefupdatr::nix2
stan_conj_to_nix2 <- function(stan_p) {
  with(stan_p, list(nu = nu_0,
                    kappa = kappa_0,
                    mu = mu_0,
                    sigma2 = sigma_0 ^ 2))
}

# convert samples of prior params in array form to list of samples in nix2
# parameter list form.
#
# e.g., mod_nix2_samples[[1]][[1]] is the first 
mod_nix2_samples <- 
  mod_samples[c('nu_0', 'kappa_0', 'mu_0', 'sigma_0')] %>%
  ## repeate these since there's just one for both categories
  map_at(c('nu_0', 'kappa_0'), ~ cbind(.x, .x)) %>%
  ## turn arrays into nested lists
  map(array_tree) %>%
  ## zip list of variables into list of samples
  transpose() %>%
  ## zip each sample's list of variables into list of categories
  map(transpose) %>%
  ## ...and zip list of samples into a list of categories
  transpose() %>%
  set_names(c('b', 'p')) %>%
  ## rename and convert expected sd to var
  at_depth(2, stan_conj_to_nix2)

## confirm that we have nix2 params at depth 2
invisible(mod_nix2_samples %>% at_depth(2, ~ assert_that(is_nix2_params(.))))

## get summary statistics for each condition
updated_nix2_samples <- 
  sepmeans %>%
  left_join(sepmeans_conds) %>%
  group_by(bvotCond, pvotCond, trueCat) %>%
  filter(subject == first(subject),
         is_test == FALSE) %>%
  nest() %>%
  mutate(prior_samples = map(trueCat, ~ mod_nix2_samples[[.x]]),
         updated_samples = map2(data, prior_samples,
                                function(d, s) map(s, nix2_update, x=d$vot)))

sample_to_lhood <- function(p)
  data_frame(vot = seq(-10,50),
             lhood = d_nix2_predict(vot, p))

predicted_lhood <- updated_nix2_samples %>%
  mutate(test_lhood = map(updated_samples,
                          . %>%
                            map(sample_to_lhood) %>%
                            do.call(what=rbind))) %>%
  unnest(test_lhood)

lapse_rate_samples <- mod_samples['lapse_rate'] %>%
  as_data_frame() %>%
  mutate(sample = row_number())

predicted_prob_p <-
  predicted_lhood %>%
  group_by(bvotCond, pvotCond, trueCat, vot) %>%
  mutate(sample = row_number()) %>%
  spread(trueCat, lhood) %>%
  left_join(lapse_rate_samples) %>%
  mutate(prob_p = p / (b+p),
         prob_p_lapse = prob_p * (1-lapse_rate) + lapse_rate*0.5) %>%
  group_by(bvotCond, pvotCond, vot) %>%
  summarise_each(funs(mean=mean, lo=quantile(., 0.025), hi=quantile(., 0.975)),
                 prob_p, prob_p_lapse)

```

```{r plot-exp2-predictions-by-thirds, fig.width=10, fig.height=6}

sepmeans_test_by_subject_thirds <-
  sepmeans_test %>%
  group_by(vot_cond, vot, subject, third=ntile(trial,3)) %>%
  summarise(resp_p = mean(respP))

predicted_prob_p %>%
  left_join(sepmeans_conds) %>%
  ggplot(aes(x=vot, y=prob_p_lapse_mean, color=vot_cond, fill=vot_cond)) +
  geom_line() +
  geom_ribbon(aes(ymin=prob_p_lapse_lo, ymax=prob_p_lapse_hi),
              alpha=0.2, color=NA) +
  geom_pointrange(data = sepmeans_test_by_subject_thirds,
                  aes(y=resp_p),
                  stat='summary', fun.data=mean_cl_boot) +
  facet_grid(third~vot_cond)

```

```{r plot-exp2-predictions-first-third, fig.width=10, fig.height=2}

predicted_prob_p %>%
  left_join(sepmeans_conds) %>%
  ggplot(aes(x=vot, y=prob_p_lapse_mean, color=vot_cond, fill=vot_cond)) +
  geom_line() +
  geom_ribbon(aes(ymin=prob_p_lapse_lo, ymax=prob_p_lapse_hi),
              alpha=0.2, color=NA) +
  geom_pointrange(data = filter(sepmeans_test_by_subject_thirds, third==1),
                  aes(y=resp_p),
                  stat='summary', fun.data=mean_cl_boot) +
  facet_grid(.~vot_cond)

```

Overall, the predicted classification curves line up reasonably well with listeners' actual category boundaries at test. The biggest discrepancy is in the /b/ 10ms, /p/ 80ms condition, where the model predicts a bigger category boundary shift than is actually observed. The model also predicts a clearer assymptote for some of the lower conditions, although this may be a side-effect of more variability in individual listeners' category boundaries (which has the effect of smoothing out the average boundary plotted here).

The model accuracy appears to be slightly better earlier in the test phase, which is consistent with other work that shows that phonetic adaptation (recalibration) can be undone by prolonged testing [@Kraljic2005; @Vroomen2004].

One intresting feature of the model predictions themselves is that the uncertainty is quite a bit higher for the two extreme negative /b/ VOT (-80ms and -50ms) conditions. This suggests that there is uncertainty in the posterior over prior beliefs inferred from Experiment 1, but that this uncertainty is orthogonal to the predicted classification when the distance between the two categories' means is held constant (as in Experiment 1) and hence not detectable from the posterior predictive distributions for Experiment 1 (e.g. Figure __NNN__). But when the distance between the categories varies, the uncertainty in the inferred beliefs _is_ reflected in the predicted category boundaries. This goes to show the importance of checking model predictions for conditions that are different from those used to train it, since this can reveal additional hypotheses that are worth testing.

We can quantify the effects of the additional information provided by Experiment 2 by updating the inferred prior beliefs, re-weighting the samples from the inferred beliefs from Experiment 1 using the likelihood of the data from Experiment 2. This weighted sample can then be used to approximate functions of the posterior (e.g., the expected prior beliefs given our data).

```{r importance-weights-expt2}



```

## Discussion

The effect of this manipulation is pretty small. In fact, it appears that moving the /b/ mean VOT from -20 to -50 and even -80 has little additional effect on the category boundary, even though each of these changes would move the boundary by -15ms VOT each (if the prior was completely ignored).

At first glance, this is surprising. But when you consider the _structure_ of how mean VOT varies across categories, it begins to make some sense. @Chodroff2015, for instance, found that across talkers, the VOTs of different places of articulation and voicing categories were strongly positively correlated. If listeners are sensitive to this correlation structure, the resulting prior on their adaptation would effectively filter out portions of the variability that are not positively correlated across categories.

Another possibility is that listeners are sensitive to the possibility of talkers producing a distinct cluster of pre-voiced /b/s that have large negative VOTs. When the /b/ mean enters this very-low-VOT range, it no longer affects the category boundary any more, since that is primarily determined by the short-lag category that most talkers who prevoice still produce occasionally.
