# Framing

* In other domains: sensitivity to empirical prior in perceptual inferences. The same logic applies at the level of inferring generative model, too. But lacking evidence for this.
    * TODO: lit on sensitivity to prior in inferences. Knill, Orhan/Jacobs, Shafto?
    * (Maybe better in discussion)
* Relevant acquisition work: need to learn clusters of cues (phonetic categories). Analogously, learn cluster(s) of talkers/cue distributions.

## Experiment 2

* distributions that predict more extreme divergence from typical classification.
* extreme negative VOTs: prevoicing

# Analyses

* Compare fits with buckeye data.
* Fit same prior to buckeye _production_ data (maybe goldrick data, since that's isolated word reading?)

# Discussion

* WHere do constraints come from? other sources: innate/self-productions.
* Points to make
    * people adapt by distributional learning
    * but it is _constrained_
    * you can infer prior beliefs, which are similar to actual patterns of cross-talker variability and which predict adaptation to other distributions
    * nevertheless, the prior beliefs you infer are fundamentally bound by the model you use. you can still have predictive accuracy with a model that does not even match the structure of the data, to say nothing of what people are actually doing.
