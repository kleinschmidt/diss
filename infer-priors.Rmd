---
Title: What do you expect from an unfamiliar talker?
Author: Dave Kleinschmidt
---

```{r preamble}

library(assertthat)
library(magrittr)
library(dplyr)
library(tidyr)
library(purrr)
library(lme4)
library(ggplot2)

## devtools::install_github('kleinschmidt/phonetic-sup-unsup')
library(supunsup)
## devtools::install_github('kleinschmidt/beliefupdatr')
library(beliefupdatr)

knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      error = FALSE)

```

# Introduction

```{r typical-talker}

prior_lhood <- 
  supunsup::prior_stats %>%
  filter(source == 'kronrod2012') %>%
  supunsup::stats_to_lhood()

prior_class <- prior_lhood %>% lhood_to_classification()

```

# Experiment 1

```{r expt1-data}

data_exp1 <- supunsup::supunsup_clean %>%
  filter(supCond == 'unsupervised') %>%
  mutate(trueCat = respCategory,
         subjNum = as.numeric(factor(subject)),
         trueCatNum = as.numeric(trueCat),
         respCatNum = as.numeric(respCat))

```

## Modeling


```{r run-model, eval=FALSE}

## run the belief-updating model for inferring prior. the model source is in
## the beliefupdatr package, or will be soon :)
##
## devtools::install_github('kleinschmidt/beliefupdatr')

data_exp1_stan_conj <- data_exp1 %>%
  supunsup::supunsup_to_stan_conj()
  
library(rstan)
mod_lapsing <- beliefupdatr::compile_stan('conj_id_lapsing_fit.stan')

fit_lapsing <- stan(fit = mod_lapsing,
                    data = data_exp1_stan_conj,
                    chains = 4,
                    iter = 1000)

head(summary(fit_lapsing)$summary, n=10)

mod_summary <- summary(fit_lapsing)$summary
mod_samples <- rstan::extract(fit_lapsing)

saveRDS(mod_samples, file='models/samples_lapsing.rds')
saveRDS(mod_summary, file='models/summary_lapsing.rds')

```

```{r load-samples}

mod_samples <- readRDS('models/samples_lapsing.rds')

```


# Experiment 2

```{r separatemeans-data}

sepmeans <- supunsup::separatemeans_clean

sepmeans_conds <-
  sepmeans %>%
  group_by(bvotCond, pvotCond) %>%
  summarise() %>%
  ungroup() %>%
  arrange(bvotCond, pvotCond) %>%
  mutate(vot_cond = paste(bvotCond, pvotCond),
         vot_cond = factor(vot_cond, levels=vot_cond))

sepmeans_stats <-
  sepmeans %>%
  filter(!is_test) %>%
  left_join(sepmeans_conds) %>%
  group_by(bvotCond, pvotCond, vot_cond, trueCat) %>%
  summarise(mean = mean(vot), sd = sd(vot)) %>%
  rename(category = trueCat)

sepmeans_exposure_lhood <-
  sepmeans_stats %>%
  group_by(bvotCond, pvotCond, vot_cond) %>%
  do({stats_to_lhood(.)})

sepmeans_exposure_class <-
  sepmeans_exposure_lhood %>%
  do({lhood_to_classification(.)})

```

```{r fig.width=10, fig.height=2}

sepmeans %>%
  filter(is_test) %>%
  left_join(sepmeans_conds) %>%
  ggplot(aes(x=vot, y=respP)) +
  stat_smooth(method='glm', method.args=list(family='binomial'), geom='line',
              aes(group=subject, color=vot_cond), alpha=0.2) +
  stat_smooth(method='glm', method.args=list(family='binomial'), geom='line',
              aes(color=vot_cond), size=2) +
  geom_line(data = prior_class %>% filter(vot <= 50, vot >= -10),
            linetype=2, color='black', aes(x=vot, y=prob_p)) + 
  geom_line(data = sepmeans_exposure_class %>% filter(vot <= 50, vot >= -10),
            linetype=2, size=2, aes(x=vot, y=prob_p, color=vot_cond)) + 
  facet_grid(.~vot_cond)


```

```{r }

sepmeans %>%
  filter(is_test) %>%
  left_join(sepmeans_conds) %>%
  ggplot(aes(x=vot, y=respP)) +
  stat_smooth(method='glm', method.args=list(family='binomial'), geom='line',
              aes(color=vot_cond), size=2) +
  geom_line(data = prior_class %>% filter(vot <= 50, vot >= -10),
            linetype=2, color='black', aes(x=vot, y=prob_p)) + 
  geom_line(data = sepmeans_exposure_class %>% filter(vot <= 50, vot >= -10),
            linetype=2, size=2, aes(x=vot, y=prob_p, color=vot_cond))

```

## Mixed-effects model

```{r sepmeans-prepare-lmer}

sepmeans_conds <-
  sepmeans %>%
  group_by(bvotCond, pvotCond) %>%
  summarise() %>%
  ungroup() %>%
  arrange(bvotCond, pvotCond) %>%
  mutate(vot_cond = paste(bvotCond, pvotCond),
         vot_cond = factor(vot_cond, levels=vot_cond))

cs <- contr.helmert(sepmeans_conds$vot_cond) %*% diag(1/(1:4))
colnames(cs) <- levels(sepmeans_conds$vot_cond)[2:5]
contrasts(sepmeans_conds$vot_cond) <- cs
  

# process data for modeling:
sepmeans_test <- sepmeans %>%
  # only modeling test trials:
  filter(is_test) %>%
  # convert b/p means into center + distance
  mutate(vot_cond_center = bvotCond + pvotCond,
         vot_cond_dist = pvotCond - bvotCond) %>%
  left_join(sepmeans_conds, by=c('bvotCond', 'pvotCond')) %>%
  # center/10 and scaled versions of continuous predictors
  mutate_each(funs(c10 = (. - mean(.))/10,
                   scale = (. - mean(.))/sd(.)),
              vot, vot_cond_center, vot_cond_dist, trial)

summary(sepmeans_test)

```

Lots of convergence issues with LMER.

```{r sepmeans-run-lmer, cache=TRUE}



m0 <- glmer(respP ~ 1 + vot_c10 + vot_cond + trial_c10 +
              (1 | subject),
            data = sepmeans_test, family='binomial')

summary(m0)

m0.1 <- glmer(respP ~ 1 + vot_c10 + vot_cond + trial_c10 +
                (1 + vot_c10 | subject),
              data = sepmeans_test, family='binomial')

summary(m0.1)

m0.2 <- glmer(respP ~ 1 + vot_c10 + vot_cond + trial_c10 +
                (1 + vot_c10 || subject),
              data = sepmeans_test, family='binomial')

summary(m0.2)

## m1 <- glmer(respP ~ 1 + vot_c10 * vot_cond * trial_c10 +
##               (1 + vot_c10 * trial_c10 | subject),
##             data = sepmeans_test, family='binomial')

## summary(m1)

```

```{r sepmeans-lmer-table, results='asis'}
stargazer::stargazer(m0, m0.1, m0.2, star.cutoffs=c(0.05, 0.01, 0.001), type='html')
```

## Category boundary analysis

```{r fit-cat-bounds}

boundaries <- sepmeans %>%
  filter(is_test) %>%
  group_by(bvotCond, pvotCond, subject) %>%
  mutate(trial = trial - min(trial)) %>% 
  do({ glm(respP ~ vot * trial, family='binomial', data=.) %>%
         broom::tidy() %>%
         select(term, estimate)
  }) %>%
  ungroup() %>%
  spread(term, estimate) %>%
  mutate(boundary = -`(Intercept)` / vot) %>%
  left_join(sepmeans_conds)

```

```{r}

ggplot(boundaries, aes(x=vot_cond, y=boundary)) +
  geom_violin() +
  stat_summary(fun.data=mean_cl_boot, geom='pointrange') +
  lims(y = c(-20, 60))

```

## Predicted adaptation from inferred priors

How well do the prior beliefs inferred on the basis of Experiment 1 predict the adaptation we observed in Experiment 2?

```{r predict-expt2-from-inferred}

#' Convert from stan parametrization to beliefupdatr::nix2
stan_conj_to_nix2 <- function(stan_p) {
  with(stan_p, list(nu = nu_0,
                    kappa = kappa_0,
                    mu = mu_0,
                    sigma2 = sigma_0 ^ 2))
}

# convert samples of prior params in array form to list of samples in nix2
# parameter list form.
#
# e.g., mod_nix2_samples[[1]][[1]] is the first 
mod_nix2_samples <- 
  mod_samples[c('nu_0', 'kappa_0', 'mu_0', 'sigma_0')] %>%
  ## repeate these since there's just one for both categories
  map_at(c('nu_0', 'kappa_0'), ~ cbind(.x, .x)) %>%
  ## turn arrays into nested lists
  map(array_tree) %>%
  ## zip list of variables into list of samples
  transpose() %>%
  ## zip each sample's list of variables into list of categories
  map(transpose) %>%
  ## ...and zip list of samples into a list of categories
  transpose() %>%
  set_names(c('b', 'p')) %>%
  ## rename and convert expected sd to var
  at_depth(2, stan_conj_to_nix2)

## confirm that we have nix2 params at depth 2
invisible(mod_nix2_samples %>% at_depth(2, ~ assert_that(is_nix2_params(.))))

## get summary statistics for each condition
updated_nix2_samples <- 
  sepmeans %>%
  left_join(sepmeans_conds) %>%
  group_by(bvotCond, pvotCond, trueCat) %>%
  filter(subject == first(subject),
         is_test == FALSE) %>%
  nest() %>%
  mutate(prior_samples = map(trueCat, ~ mod_nix2_samples[[.x]]),
         updated_samples = map2(data, prior_samples,
                                function(d, s) map(s, nix2_update, x=d$vot)))

sample_to_lhood <- function(p)
  data_frame(vot = seq(-10,50),
             lhood = d_nix2_predict(vot, p))

predicted_lhood <- updated_nix2_samples %>%
  mutate(test_lhood = map(updated_samples,
                          . %>%
                            map(sample_to_lhood) %>%
                            do.call(what=rbind))) %>%
  unnest(test_lhood)

lapse_rate_samples <- mod_samples['lapse_rate'] %>%
  as_data_frame() %>%
  mutate(sample = row_number())

predicted_prob_p <-
  predicted_lhood %>%
  group_by(bvotCond, pvotCond, trueCat, vot) %>%
  mutate(sample = row_number()) %>%
  spread(trueCat, lhood) %>%
  left_join(lapse_rate_samples) %>%
  mutate(prob_p = p / (b+p),
         prob_p_lapse = prob_p * (1-lapse_rate) + lapse_rate*0.5) %>%
  group_by(bvotCond, pvotCond, vot) %>%
  summarise_each(funs(mean=mean, lo=quantile(., 0.025), hi=quantile(., 0.975)),
                 prob_p, prob_p_lapse)

```

```{r plot-exp2-predictions-by-thirds, fig.width=10, fig.height=6}

sepmeans_test_by_subject_thirds <-
  sepmeans_test %>%
  group_by(vot_cond, vot, subject, third=ntile(trial,3)) %>%
  summarise(resp_p = mean(respP))

predicted_prob_p %>%
  left_join(sepmeans_conds) %>%
  ggplot(aes(x=vot, y=prob_p_lapse_mean, color=vot_cond, fill=vot_cond)) +
  geom_line() +
  geom_ribbon(aes(ymin=prob_p_lapse_lo, ymax=prob_p_lapse_hi),
              alpha=0.2, color=NA) +
  geom_pointrange(data = sepmeans_test_by_subject_thirds,
                  aes(y=resp_p),
                  stat='summary', fun.data=mean_cl_boot) +
  facet_grid(third~vot_cond)

```

```{r plot-exp2-predictions-first-third, fig.width=10, fig.height=2}

predicted_prob_p %>%
  left_join(sepmeans_conds) %>%
  ggplot(aes(x=vot, y=mean, color=vot_cond, fill=vot_cond)) +
  geom_line() +
  geom_ribbon(aes(ymin=lo, ymax=hi), alpha=0.2, color=NA) +
  geom_pointrange(data = filter(sepmeans_test_by_subject_thirds, third==1),
                  stat='summary', fun.data=mean_cl_boot) +
  facet_grid(third~vot_cond)

```


## Discussion

The effect of this manipulation is pretty small. In fact, it appears that moving the /b/ mean VOT from -20 to -50 and even -80 has little additional effect on the category boundary, even though each of these changes would move the boundary by -15ms VOT each (if the prior was completely ignored).

At first glance, this is surprising. But when you consider the _structure_ of how mean VOT varies across categories, it begins to make some sense. @Chodroff2015, for instance, found that across talkers, the VOTs of different places of articulation and voicing categories were strongly positively correlated. If listeners are sensitive to this correlation structure, the resulting prior on their adaptation would effectively filter out portions of the variability that are not positively correlated across categories.

Another possibility is that listeners are sensitive to the possibility of talkers producing a distinct cluster of pre-voiced /b/s that have large negative VOTs. When the /b/ mean enters this very-low-VOT range, it no longer affects the category boundary any more, since that is primarily determined by the short-lag category that most talkers who prevoice still produce occasionally.
